{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os # For file/directory interaction\n",
    "import time, sys\n",
    "from datetime import datetime, date # For log data\n",
    "import re # For text replacement\n",
    "import spacy # Pipeline processes (stopword and punctuation removal, lemmatization)\n",
    "from nltk.stem.snowball import SnowballStemmer # Pipeline process for stemming\n",
    "import json\n",
    "\n",
    "workingDir = os.getcwd()\n",
    "commonDir = os.path.join(workingDir,'Files')\n",
    "pdfDir = os.path.join(commonDir,'PDF')\n",
    "txtFilesDir = os.path.join(commonDir,'Text Files')\n",
    "rtnFilesDir = os.path.join(commonDir,'n Removed')\n",
    "spaceFilesDir = os.path.join(commonDir,'No Spaces')\n",
    "swFilesDir = os.path.join(commonDir,'Stop Words')\n",
    "engFilesDir = os.path.join(commonDir,'English Words')\n",
    "stemFilesDir = os.path.join(commonDir,'Stemmed')\n",
    "jsonDocIndex = os.path.join(commonDir,'doc_dictionary.json')\n",
    "jsonInvIndex = os.path.join(commonDir,'inverted_index.json')\n",
    "jsonGramIndex = os.path.join(commonDir,'gram_index.json')\n",
    "jsonTFMatrix = os.path.join(commonDir,'tf_matrix.json')\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter, process_pdf\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "\n",
    "# Uses PDFminer3k to extract text from PDF documents\n",
    "def getText(pdfPath):\n",
    "    # Sets up necessary objects\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    sio = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, sio, laparams=laparams)\n",
    "    # Reads file to the text converter\n",
    "    with open(pdfPath, 'rb') as pdfFile:\n",
    "        process_pdf(rsrcmgr, device, pdfFile)\n",
    "    # Retrieves text result\n",
    "    text = sio.getvalue()\n",
    "    device.close()\n",
    "    sio.close()\n",
    "    return text\n",
    "\n",
    "# Pre-condition: All PDF files to be processed are in the sub-directory\n",
    "#     pdfDir, and pdfDir is in workingDir. workingDir is by default the \n",
    "#     directory in which the program is executed\n",
    "# Post-condition: All PDF files processed without error are converted to\n",
    "#     text files which are placed in a sub-directory 'Text Files'\n",
    "# NOTE: This will process all documents by default. Change the value of\n",
    "#     'limit' to limit the number of documents processed at once\n",
    "def pdfToText(pdfPath = pdfDir, txtPath = txtFilesDir, workingDir = commonDir, limit = -1):\n",
    "    if not os.path.exists(pdfPath):\n",
    "        print('The specified directory \"' + pdfPath + '\" does not exist')\n",
    "        return\n",
    "    # Creates 'Text Files' directory for converted PDFs\n",
    "    if not os.path.exists(txtPath):\n",
    "        os.mkdir(txtPath)\n",
    "    \n",
    "    docNum = 0\n",
    "    docErr = 1\n",
    "    totalNum = len([file for file in os.scandir(pdfPath) if file.name.endswith('.pdf')])\n",
    "    with open(os.path.join(workingDir,'log.txt'), 'a+', encoding=\"utf-8\") as log:    \n",
    "        log.write(\"PDF to Text\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")    \n",
    "        for entity in os.scandir(pdfPath):\n",
    "            # Moves on to next entity if the current entity is not a PDF\n",
    "            if not entity.name.endswith('.pdf'):\n",
    "                continue\n",
    "            index = -4 # Remove '.pdf' from file name when creating '.txt' file\n",
    "            fileName = entity.name[:index]+'.txt'\n",
    "            \n",
    "            # Attempt to read the PDF file, extract text from each page,\n",
    "            #     and write the text to a text file with the same name\n",
    "            # Some documents are protected, corrupted, etc. and text cannot be extracted\n",
    "            # Exceptions are recorded in log.txt\n",
    "            # hasError remains true until each step in the try block is complete\n",
    "            if fileName not in os.listdir(txtPath): \n",
    "                print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "                # Extracts text via getText method and writes to a text file. \n",
    "                # Errors are reported in log.txt\n",
    "                hasError = True\n",
    "                try:\n",
    "                    text = getText(os.path.join(pdfPath,entity.name))\n",
    "                    txtFile = open(os.path.join(txtPath,fileName), 'w+', encoding=\"utf-8\")\n",
    "                    txtFile.write(text)\n",
    "                    docNum += 1\n",
    "                    print(\"done ({}/{})\".format(docNum, limit))\n",
    "                    hasError = False\n",
    "                except Exception as e:\n",
    "                    log.write(str(docErr)+\": \" + entity.name + \": \\n\\t\" + str(e)+\"\\n\")\n",
    "                if hasError:\n",
    "                    print(\"there was an error reading this document. See log for details. Reference number \"+str(docErr)+\".\\n\")\n",
    "                    docErr += 1\n",
    "            else:\n",
    "                continue\n",
    "            if docNum >= limit and limit > 0:\n",
    "                print(\"PDF to Text was stopped after \"+str(docNum)+\" documents.\")\n",
    "                break\n",
    "        log.write(\"\\n\\n\")\n",
    "pdfToText(limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS MUST BE AFTER TEXT CONVERSION BEFORE ANY OTHER FUNCTIONS\n",
    "# Function to remove \\n\n",
    "def rmvN(txtPath = txtFilesDir, rtnPath = rtnFilesDir):\n",
    "    # Checks that text file directory exists/is correct\n",
    "    if not os.path.exists(txtPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call pdfToText first and the directory will be created')\n",
    "        return\n",
    "    if not os.path.exists(rtnPath):\n",
    "        os.mkdir(rtnPath)\n",
    "\n",
    "    # Substitutes returns and hyphens at the end of each line with empty strings\n",
    "    for entity in os.scandir(txtPath):\n",
    "        txtFilePath = os.path.join(txtPath,entity.name)\n",
    "        rtnFilePath = os.path.join(rtnPath,entity.name)\n",
    "        if not os.path.exists(rtnFilePath):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            with open(txtFilePath, 'r+', encoding='utf-8') as txtFile:\n",
    "                with open(rtnFilePath, 'w+', encoding='utf-8') as rtnFile:\n",
    "                    text = txtFile.read()\n",
    "                    text = re.sub('-\\n', '', text)\n",
    "                    text = re.sub('\\n', ' ', text)\n",
    "                    rtnFile.write(text)\n",
    "                    rtnFile.truncate()\n",
    "            print(\"done\")\n",
    "rmvN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Funtion to move files without spaces to new 'Without Spaces' directory         \n",
    "def checkSpaces(txtPath = rtnFilesDir, spacesPath = spaceFilesDir, workingDir = commonDir):\n",
    "    # Checks that text file directory exists\n",
    "    if not os.path.exists(txtPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call rmvN first and the directory will be created')\n",
    "        return\n",
    "    if not os.path.exists(spacesPath):\n",
    "        os.mkdir(spacesPath)\n",
    "    \n",
    "    with open(os.path.join(workingDir,'Spaces.txt'), 'a+', encoding='utf-8') as spaces: \n",
    "        spaces.write(\"Check Spaces\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")\n",
    "        for entity in os.scandir(txtPath):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            txtFile = open(os.path.join(txtPath,entity.name), 'r', encoding='utf-8')\n",
    "            text = txtFile.read()\n",
    "            split = text.split(' ')\n",
    "            if len(split) < len(text)/10 or len(text) < 100 or text == '':\n",
    "                txtFile.close()\n",
    "                spaces.write(entity.name+'\\n')\n",
    "                if entity.name not in os.listdir(spacesPath):\n",
    "                    os.rename(os.path.join(txtPath,entity.name), os.path.join(spacesPath,entity.name))\n",
    "                else:\n",
    "                    os.remove(os.path.join(txtPath,entity.name))\n",
    "            print(\"done\")\n",
    "        spaces.write('\\n\\n')\n",
    "checkSpaces() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "def rmvStopWords(txtPath = rtnFilesDir, swPath = swFilesDir):\n",
    "    if not os.path.exists(txtPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call rmvN first and the directory will be created')\n",
    "        return\n",
    "    if not os.path.exists(swPath):\n",
    "        os.mkdir(swPath)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for entity in os.scandir(txtPath):\n",
    "        if not os.path.exists(os.path.join(swPath,entity.name)):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            with open(os.path.join(txtPath,entity.name), 'r+', encoding='utf-8') as txtFile:\n",
    "                with open(os.path.join(swPath,entity.name), 'w+', encoding='utf-8') as swFile:\n",
    "                    doc = nlp(txtFile.read())\n",
    "                    noStopWords = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.text.isnumeric()]\n",
    "                    swFile.write(\" \".join(noStopWords))\n",
    "                    swFile.truncate()\n",
    "            print(\"done\")\n",
    "\n",
    "rmvStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove non-english words\n",
    "def rmvNonEng(txtPath = swFilesDir, engPath = engFilesDir, workingDir = commonDir):\n",
    "    if not os.path.exists(txtPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call rmvStopWords first and the directory will be created')\n",
    "        return\n",
    "    if not os.path.exists(engPath):\n",
    "        os.mkdir(engPath)\n",
    "    with open(os.path.join(workingDir,'words_dictionary.json')) as json_file:\n",
    "        words = json.load(json_file)\n",
    "        \n",
    "    lets = []\n",
    "    alph = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for let in alph:\n",
    "        lets.append(let)\n",
    "        for char in alph:\n",
    "            lets.append(let+char)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        if not os.path.exists(os.path.join(engPath,entity.name)):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            with open(os.path.join(txtPath,entity.name), 'r+', encoding='utf-8') as txtFile:\n",
    "                with open(os.path.join(engPath,entity.name), 'w+', encoding='utf-8') as engFile:\n",
    "                    text = txtFile.read().split(' ')\n",
    "                    engChars = [word for word in text if word in words and word not in lets]\n",
    "                    engFile.write(\" \".join(engChars))\n",
    "                    engFile.truncate()\n",
    "            print(\"done\")\n",
    "rmvNonEng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stem words in all documents\n",
    "def stem(txtPath = engFilesDir, stemPath = stemFilesDir, workingDir = commonDir):\n",
    "    if not os.path.exists(txtPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call rmvNonEng first and the directory will be created')\n",
    "        return\n",
    "    if not os.path.exists(stemPath):\n",
    "        os.mkdir(stemPath)\n",
    "        \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    for entity in os.scandir(txtPath):\n",
    "        if not os.path.exists(os.path.join(stemPath, entity.name)):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            with open(os.path.join(txtPath,entity.name), 'r+', encoding='utf-8') as txtFile:\n",
    "                with open(os.path.join(stemPath,entity.name), 'w+', encoding='utf-8') as stemFile:\n",
    "                    text = txtFile.read().split(' ')\n",
    "                    stemmed = [stemmer.stem(word) for word in text]\n",
    "                    stemFile.write(\" \".join(stemmed))\n",
    "                    stemFile.truncate()\n",
    "            print(\"done\")\n",
    "stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create JSON with file_name : doc_id\n",
    "def update_doc_index(jsonPath = jsonDocIndex, txtPath = stemFilesDir):\n",
    "    if os.path.exists(jsonPath):\n",
    "        with open(jsonPath, 'r') as jsonFile:\n",
    "            docIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        docIndex = {}\n",
    "    for fileName in os.listdir(txtPath):\n",
    "        if fileName not in docIndex.values():\n",
    "            docIndex[len(docIndex)+1] = fileName\n",
    "    with open(jsonPath, 'w') as jsonFile:\n",
    "        json.dump(docIndex, jsonFile, indent=4)\n",
    "\n",
    "update_doc_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_inv_index(ngrams, jsonDocPath = jsonDocIndex, jsonInvPath = jsonInvIndex, txtPath = stemFilesDir, workingDir = commonDir):\n",
    "    if not os.path.exists(txtPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call stem first and the directory will be created')\n",
    "        return\n",
    "    if not os.path.exists(jsonDocPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call update_doc_index first and the file will be created')\n",
    "        return\n",
    "    if os.path.exists(jsonInvPath):\n",
    "        with open(jsonInvPath, 'r') as jsonFile:\n",
    "            invIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        invIndex = {}\n",
    "    # Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "        \n",
    "    for ngram in ngrams:\n",
    "        for docID in docIndex:\n",
    "            if docID not in invIndex:\n",
    "                invIndex[docID] = {}\n",
    "            if ngram not in invIndex[docID]:\n",
    "                invIndex[docID][ngram] = {}\n",
    "                with open(os.path.join(txtPath,docIndex[docID]), 'r', encoding='utf-8') as txtFile:\n",
    "                    text = txtFile.read().split(' ')\n",
    "                    while len(text) > ngram-1:\n",
    "                        term = \" \".join(text[:ngram])\n",
    "                        if term in invIndex[docID][ngram]:\n",
    "                            invIndex[docID][ngram][term] += 1\n",
    "                        else:\n",
    "                            invIndex[docID][ngram][term] = 1\n",
    "                        text.pop(0)\n",
    "    with open(jsonInvPath, 'w') as jsonFile:\n",
    "        json.dump(invIndex, jsonFile, indent=4)\n",
    "\n",
    "update_inv_index(list(range(2,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_gram_index(minFreq = 1, jsonInvPath = jsonInvIndex, jsonGramPath = jsonGramIndex, txtPath = stemFilesDir):\n",
    "    # Makes sure all files and directories exist (jsonInv, txtDir)\n",
    "    if not os.path.exists(jsonInvPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonInvPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call update_inv_index first and the file will be created')\n",
    "        return\n",
    "    # jsonGram is created if it does not exist\n",
    "    if os.path.exists(jsonGramPath):\n",
    "        with open(jsonGramPath, 'r') as jsonFile:\n",
    "            gramIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        gramIndex = {}\n",
    "    # Loads Inverted File index\n",
    "    with open(jsonInvPath, 'r') as jsonFile:\n",
    "        invIndex = json.load(jsonFile)\n",
    "        \n",
    "    for docID, ngrams in invIndex.items():\n",
    "        for terms in ngrams.values():\n",
    "            for term, freq in terms.items():\n",
    "                if freq >= minFreq:\n",
    "                    if term not in gramIndex:\n",
    "                        gramIndex[term] = {}\n",
    "                    gramIndex[term][docID] = freq\n",
    "    # Writes gramIndex to gram JSON file\n",
    "    with open(jsonGramPath, 'w') as jsonFile:\n",
    "        json.dump(gramIndex, jsonFile, indent=4)\n",
    "\n",
    "update_gram_index()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print grams found in 3 or more documents\n",
    "def getGrams(numDocs = 3, jsonDocPath = jsonDocIndex, jsonGramPath = jsonGramIndex, txtPath = stemFilesDir, workingDir = commonDir):\n",
    "    # Makes sure all files and directories exist (textDir, jsonDoc, txtDir)\n",
    "    if not os.path.exists(txtPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call stem first and the directory will be created')\n",
    "        return\n",
    "    if not os.path.exists(jsonDocPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call update_doc_index first and the file will be created')\n",
    "        return\n",
    "    if not os.path.exists(jsonGramPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonGramPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call update_gram_index first and the file will be created')\n",
    "        return\n",
    "    # Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "    # Loads gram index \n",
    "    with open(jsonGramPath, 'r') as jsonFile:\n",
    "        gramIndex = json.load(jsonFile)\n",
    "    \n",
    "    for gram, docs in gramIndex.items():\n",
    "        if len(docs) >= numDocs:\n",
    "            print(\"\\\"\"+gram+\"\\\" found in \"+str(len(docs))+\" documents\")\n",
    "            for docID in docs:\n",
    "                print(\"\\t\"+ str(docIndex[docID]))\n",
    "                \n",
    "getGrams(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create vectors for all n-grams (with freq>1?) between two docs, multiply them,\n",
    "#     then divide by the product Euclidian norms\n",
    "# Print out similar phrases\n",
    "def freqMatrices(ngrams, jsonDocPath = jsonDocIndex, jsonInvPath = jsonInvIndex, jsonTFMPath = jsonTFMatrix, limit = -1):\n",
    "    if not os.path.exists(jsonDocPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call update_doc_index first and the file will be created')\n",
    "        return\n",
    "    if not os.path.exists(jsonInvPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonInvPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call update_inv_index first and the file will be created')\n",
    "        return\n",
    "    \n",
    "    with open(jsonDocPath, 'r') as jsonDoc:\n",
    "        docIndex = json.load(jsonDoc)\n",
    "    with open(jsonInvPath, 'r') as jsonInv:\n",
    "        invIndex = json.load(jsonInv)\n",
    "        \n",
    "    # jsonTFM is created if it does not exist\n",
    "    if os.path.exists(jsonTFMPath):\n",
    "        with open(jsonTFMPath, 'r') as jsonFile:\n",
    "            tfMatrix = json.load(jsonFile)\n",
    "    else:\n",
    "        tfMatrix = {}     \n",
    "    \n",
    "    lastDoc = -1\n",
    "    docsProcessed = 0\n",
    "    for doc1 in invIndex:\n",
    "        docNum = 1 + docsProcessed\n",
    "        docsProcessed += 1\n",
    "        doc1Terms = []\n",
    "        if doc1 not in tfMatrix:\n",
    "            tfMatrix[doc1] = {}\n",
    "        for ngram, terms in invIndex[doc1].items():\n",
    "            if int(ngram) in ngrams:\n",
    "                for term in terms:\n",
    "                    doc1Terms.append(term)\n",
    "        if int(doc1) == lastDoc:\n",
    "            break\n",
    "        for doc2 in invIndex:\n",
    "            if int(doc2) == lastDoc and lastDoc > 0:\n",
    "                break\n",
    "            if  docsProcessed == 1 and docNum >= limit and limit > 0:\n",
    "                lastDoc = int(doc2)\n",
    "                break\n",
    "            if int(doc2) > int(doc1) and doc2 not in tfMatrix[doc1]:\n",
    "                docNum += 1\n",
    "                print(docIndex[doc1]+\" and \"+docIndex[doc2])\n",
    "                if doc2 not in tfMatrix[doc1]:\n",
    "                    tfMatrix[doc1][doc2] = {}\n",
    "                    tfMatrix[doc1][doc2][\"all\"] = {}\n",
    "                    tfMatrix[doc1][doc2][\"like\"] = {}\n",
    "                allTerms = doc1Terms.copy()\n",
    "                likeTerms = []\n",
    "                for ngram, terms in invIndex[doc2].items():\n",
    "                    if int(ngram) in ngrams:\n",
    "                        for term in terms:\n",
    "                            if term not in allTerms:\n",
    "                                allTerms.append(term)\n",
    "                            else:\n",
    "                                likeTerms.append(term)\n",
    "                allTerms.sort()\n",
    "                likeTerms.sort()\n",
    "                doc1all = {}\n",
    "                doc2all = {}\n",
    "                doc1like = {}\n",
    "                doc2like = {}\n",
    "                for gramLen in ngrams:\n",
    "                    doc1all[gramLen] = []\n",
    "                    doc2all[gramLen] = []\n",
    "                    doc1like[gramLen] = []\n",
    "                    doc2like[gramLen] = []\n",
    "                for term in allTerms:\n",
    "                    length = len(term.split())\n",
    "                    ngram = str(len(term.split()))\n",
    "                    doc1all[length].append(invIndex[doc1][ngram].get(term, 0))\n",
    "                    doc2all[length].append(invIndex[doc2][ngram].get(term, 0))\n",
    "                for term in likeTerms:\n",
    "                    length = len(term.split())\n",
    "                    ngram = str(len(term.split()))\n",
    "                    doc1like[length].append(invIndex[doc1][ngram].get(term))\n",
    "                    doc2like[length].append(invIndex[doc2][ngram].get(term))\n",
    "                tfMatrix[doc1][doc2][\"all\"][doc1] = doc1all\n",
    "                tfMatrix[doc1][doc2][\"all\"][doc2] = doc2all\n",
    "                tfMatrix[doc1][doc2][\"like\"][doc1] = doc1like\n",
    "                tfMatrix[doc1][doc2][\"like\"][doc2] = doc2like\n",
    "                numLike = len(likeTerms)\n",
    "                print(\"Doc1 has \"+str(len(doc1Terms)-numLike)+\" unique terms\")\n",
    "                print(\"Doc2 has \"+str(len(allTerms)-len(doc1Terms))+\" unique terms\")\n",
    "                for gramLength in ngrams:\n",
    "                    print(\"{}-grams: {}\".format(gramLength, len(doc1like[gramLength])))\n",
    "                print(\"Total like terms: \"+str(numLike))\n",
    "                print(\"Total unlike terms: \"+str(len(allTerms)-numLike), end=\"\\n\\n\")\n",
    "                with open(jsonTFMPath, 'w') as jsonFile:\n",
    "                    json.dump(tfMatrix, jsonFile, indent=4)\n",
    "grams = [2, 3]\n",
    "freqMatrices(grams, limit=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def calcCosSim(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        print(\"Lists do not have the same number of elements\")\n",
    "        return -1\n",
    "    sumProd = sum(n1*n2 for n1, n2 in zip(list1, list2))\n",
    "    sumL1 = sum(n**2 for n in list1)\n",
    "    sumL2 = sum(n**2 for n in list2)\n",
    "    try:\n",
    "        cossim = sumProd/(math.sqrt(sumL1)*math.sqrt(sumL2))\n",
    "    except:\n",
    "        cossim = 0\n",
    "    return cossim\n",
    "\n",
    "def cossim(gramWeights, jsonDocPath = jsonDocIndex, jsonTFMPath = jsonTFMatrix, workingDir = commonDir):\n",
    "    if not os.path.exists(jsonDocPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call update_doc_index first and the file will be created')\n",
    "    if not os.path.exists(jsonTFMPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonTFMPath + '\" does not exist')\n",
    "        print('Check that the path is correct. You must call freqMatrices first and the file will be created')\n",
    "        return\n",
    "    \n",
    "    with open(jsonDocPath, 'r') as jsonDoc:\n",
    "        docIndex = json.load(jsonDoc)\n",
    "    with open(jsonTFMPath, 'r') as jsonInv:\n",
    "        tfMatrix = json.load(jsonInv)\n",
    "    \n",
    "    for doc1 in tfMatrix:\n",
    "        for doc2, values in tfMatrix[doc1].items():\n",
    "            print(docIndex[doc1]+\"({}) and \".format(doc1)+docIndex[doc2]+\"({})\".format(doc2))\n",
    "            doc1freq = []\n",
    "            doc2freq = []\n",
    "            doc1weighted = []\n",
    "            doc2weighted = []\n",
    "            for gramLen, weight in gramWeights.items():\n",
    "                gramLen = str(gramLen)\n",
    "                doc1freq += values[\"like\"][doc1][gramLen]\n",
    "                doc2freq += values[\"like\"][doc2][gramLen]\n",
    "                doc1weighted += [ val*weight for val in values[\"like\"][doc1][gramLen] ]\n",
    "                doc2weighted += [ val*weight for val in values[\"like\"][doc2][gramLen] ]   \n",
    "            print(\"Cosine Similarity: \", calcCosSim(doc1freq, doc2freq))\n",
    "            print(\"Weighted: \", calcCosSim(doc1weighted, doc2weighted), end=\"\\n\\n\")        \n",
    "            \n",
    "gramWeight = {2:1, 3:5}\n",
    "cossim(gramWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
