{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future improvements\n",
    "# Separate dir for each step\n",
    "# Create JSON array to assign IDs, \n",
    "#     keep track of PDF files process (each step?) etc.\n",
    "# Remove numbers, urls\n",
    "# Change variable names of jsonDoc, jsonInv, jsonGram to avoid redefiining argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os # For file/directory interaction\n",
    "import time, sys\n",
    "from datetime import datetime, date # For log data\n",
    "import re # For text replacement\n",
    "import spacy # Pipeline processes (stopword and punctuation removal, lemmatization)\n",
    "from nltk.stem.snowball import SnowballStemmer # Pipeline process for stemming\n",
    "import json\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txtFilesDir = 'Text Files'\n",
    "rtnFilesDir = 'n Removed'\n",
    "spaceFilesDir = 'No Spaces'\n",
    "swFilesDir = 'Stop Words'\n",
    "engFilesDir = 'English Words'\n",
    "stemFilesDir = 'Stemmed'\n",
    "jsonDocIndex = 'doc_dictionary.json'\n",
    "jsonInvIndex = 'inverted_index.json'\n",
    "jsonGramIndex = 'gram_index.json'\n",
    "\n",
    "absolute = 'C:/Users/micah/Documents/IWU/CIS Practicum/Files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on 'Lafferty_pcfg-notes.pdf'. . . \"Lafferty_p...\" already exists\n",
      "\n",
      "Now on 'LearningExecutableSemanticParsers.pdf'. . . \"LearningEx...\" already exists\n",
      "\n",
      "Now on 'p123-zhang.pdf'. . . \"p123-zhang...\" already exists\n",
      "\n",
      "Now on 'Partial Parsing Finite-State Cascades.pdf'. . . \"Partial Pa...\" already exists\n",
      "\n",
      "Now on 'QueryEffectiveness.pdf'. . . \"QueryEffec...\" already exists\n",
      "\n",
      "Now on 'Text Clustering Algorithms.pdf'. . . \"Text Clust...\" already exists\n",
      "\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.pdf'. . . \"Text-Analy...\" already exists\n",
      "\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.pdf'. . . \"Workshop o...\" already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-condition: All PDF files to be processed are in the sub-directory\n",
    "#     pdfDir, and pdfDir is in absPath. absPath is by default the \n",
    "#     directory in which the program is executed\n",
    "# Post-condition: All PDF files processed without error are converted to\n",
    "#     text files which are placed in a new sub-directory 'Text Files'\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter, process_pdf#process_pdf\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "def getText(pdfPath):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    sio = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, sio, laparams=laparams)\n",
    "#     interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    \n",
    "    with open(pdfPath, 'rb') as pdfFile:\n",
    "        process_pdf(rsrcmgr, device, pdfFile)\n",
    "#         parser = PDFParser(pdfFile)\n",
    "#         doc.set_parser(parser)\n",
    "#         interpreter.process_pdf(doc)\n",
    "#             layout = device.get_result()\n",
    "#             for element in layout:\n",
    "#                 if instanceof(element, LTTextBoxHorizontal):\n",
    "#                     print(element.get_text)\n",
    "                \n",
    "    \n",
    "    text = sio.getvalue()\n",
    "    \n",
    "    device.close()\n",
    "    sio.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def pdfToText(pdfDir, absPath = os.getcwd(), txtDir = txtFilesDir):\n",
    "    \n",
    "    pdfPath = absPath+'/'+pdfDir\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if pdfDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + directory + '\" does not exist')\n",
    "        return\n",
    "# Creates 'Text Files' directory for converted PDFs\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        os.mkdir(txtPath)\n",
    "    \n",
    "    docNum = 0\n",
    "    stopAt = 25\n",
    "    totalNum = len([file for file in os.scandir(pdfPath) if file.name.endswith('.pdf')])\n",
    "    with open(absPath+'/'+'log.txt', 'a+', encoding=\"utf-8\") as log:\n",
    "        log.write(\"PDF to Text\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")        \n",
    "# Moves on to next entity if the current entity is not a PDF\n",
    "        for entity in os.scandir(pdfPath):\n",
    "            if not entity.name.endswith('.pdf'):\n",
    "                continue\n",
    "            docNum += 1\n",
    "            index = -4 # Remove '.pdf' from file name when creating '.txt' file\n",
    "            fileName = entity.name[:index]+'.txt'\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            \n",
    "# This block attempts to read the PDF file, extract text from each page,\n",
    "#     and write the text to a text file with the same name\n",
    "# Some documents are protected, corrupted, etc. and text cannot be extracted\n",
    "# Exceptions are recorded in log.txt\n",
    "# hasError remains true until each step in the try block is complete\n",
    "            if fileName not in os.listdir(txtPath): \n",
    "                hasError = True\n",
    "                try:\n",
    "                    text = getText(pdfPath+'/'+entity.name)\n",
    "                    txtFile = open(txtPath+'/'+fileName, 'w+', encoding=\"utf-8\")\n",
    "                    txtFile.write(text)\n",
    "                    print(\"done\\n\")\n",
    "                    hasError = False\n",
    "#                     except TypeError as e:\n",
    "#                         log.write(\"TypeError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except PyPDF2.utils.PdfReadError as e:\n",
    "#                         log.write(\"PdfReadError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except OSError as e:\n",
    "#                         log.write(\"OSError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except decimal.InvalidOperation as e:\n",
    "#                         log.write(\"InvalidOperation: \" + entity.name + \":\\n\" + str(e))\n",
    "                except Exception as e:\n",
    "                    log.write(str(docNum)+\": \" + entity.name + \": \\n\\t\" + str(e)+\"\\n\")\n",
    "\n",
    "                if hasError:\n",
    "                    print(\"there was an error reading this document. See log for details. Reference number \"+str(docNum)+\".\\n\")\n",
    "            else:\n",
    "                max = 10\n",
    "                if len(fileName) > max:\n",
    "                    print('\"'+fileName[:max]+'...\"', end='')\n",
    "                else:\n",
    "                    print('\"'+fileName+'\"', end='')\n",
    "                print(' already exists\\n')\n",
    "            if docNum == stopAt:\n",
    "                print(\"PDF to Text was stopped after \"+str(docNum)+\" documents.\")\n",
    "                break\n",
    "        log.write(\"\\n\\n\")\n",
    "pdfToText('Working', absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on 10 (good) files at a time until pipeline works\n",
    "#   then incrementally add files and clean up errors\n",
    "\n",
    "# Function to remove \\n\n",
    "def rmvN(txtDir = txtFilesDir, rtnDir = rtnFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    rtnPath = absPath+'/'+rtnDir\n",
    "    if rtnDir not in os.listdir(absPath):\n",
    "        os.mkdir(rtnPath)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(rtnPath+'/'+entity.name, 'w+', encoding='utf-8') as rtnFile:\n",
    "                text = txtFile.read()\n",
    "                text = re.sub('-\\n', '', text)\n",
    "                text = re.sub('\\n', '', text)\n",
    "                rtnFile.write(text)\n",
    "                rtnFile.truncate()\n",
    "        print(\"done\")\n",
    "rmvN(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Funtion to move files without spaces to new 'Without Spaces' directory         \n",
    "def checkSpaces(txtDir = rtnFilesDir, spacesDir = spaceFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    spacesPath = absPath+'/'+spacesDir\n",
    "    if spacesDir not in os.listdir(absPath):\n",
    "        os.mkdir(spacesPath)\n",
    "        \n",
    "    with open(absPath+'/'+'Spaces.txt', 'a+', encoding='utf-8') as spaces: \n",
    "        spaces.write(\"Check Spaces\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")\n",
    "        for entity in os.scandir(txtPath):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            txtFile = open(txtPath+'/'+entity.name, 'r', encoding='utf-8')\n",
    "            text = txtFile.read()\n",
    "            split = text.split(' ')\n",
    "            if len(split) < len(text)/10 or len(text) < 100 or text == '':\n",
    "                txtFile.close()\n",
    "                spaces.write(entity.name+'\\n')\n",
    "                if entity.name not in os.listdir(spacesPath):\n",
    "                    os.rename(txtPath+'/'+entity.name, spacesPath+'/'+entity.name)\n",
    "                else:\n",
    "                    os.remove(txtPath+'/'+entity.name)\n",
    "            print(\"done\")\n",
    "        spaces.write('\\n\\n')\n",
    "checkSpaces(absPath = absolute) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "# NLTK or SpaCy\n",
    "# Inverted File: gram:[doc1, doc3] or gram:[[doc1,freq], [doc3,freq]]\n",
    "def rmvStopWords(nlp, txtDir = rtnFilesDir, swDir = swFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    swPath = absPath+'/'+swDir\n",
    "    if swDir not in os.listdir(absPath):\n",
    "        os.mkdir(swPath)\n",
    "\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(swPath+'/'+entity.name, 'w+', encoding='utf-8') as swFile:\n",
    "                doc = nlp(txtFile.read())\n",
    "                noStopWords = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.text.isnumeric()]\n",
    "                swFile.write(\" \".join(noStopWords))\n",
    "                swFile.truncate()\n",
    "        print(\"done\")\n",
    "\n",
    "rmvStopWords(nlp, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-english words\n",
    "def rmvNonEng(txtDir = swFilesDir, engDir = engFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    engPath = absPath+'/'+engDir\n",
    "    if engDir not in os.listdir(absPath):\n",
    "        os.mkdir(engPath)\n",
    "    with open(absPath+'/'+'words_dictionary.json') as json_file:\n",
    "        words = json.load(json_file)\n",
    "        \n",
    "    lets = []\n",
    "    alph = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for let in alph:\n",
    "        lets.append(let)\n",
    "        for char in alph:\n",
    "            lets.append(let+char)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(engPath+'/'+entity.name, 'w+', encoding='utf-8') as engFile:\n",
    "                text = txtFile.read().split(' ')\n",
    "                engChars = [word for word in text if word in words and word not in lets]\n",
    "                engFile.write(\" \".join(engChars))\n",
    "                engFile.truncate()\n",
    "        print(\"done\")\n",
    "rmvNonEng(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words in all documents\n",
    "def stem(txtDir = engFilesDir, stemDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    stemPath = absPath+'/'+stemDir\n",
    "    if stemDir not in os.listdir(absPath):\n",
    "        os.mkdir(stemPath)\n",
    "        \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(stemPath+'/'+entity.name, 'w+', encoding='utf-8') as stemFile:\n",
    "                text = txtFile.read().split(' ')\n",
    "                stemmed = [stemmer.stem(word) for word in text]\n",
    "                stemFile.write(\" \".join(stemmed))\n",
    "                stemFile.truncate()\n",
    "        print(\"done\")\n",
    "stem(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create JSON with file_name : doc_id\n",
    "def update_doc_index(jsonDoc = jsonDocIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    jsonPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc in os.listdir(absPath):\n",
    "        with open(jsonPath, 'r') as jsonFile:\n",
    "            docIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        docIndex = {}\n",
    "    for fileName in os.listdir(txtPath):\n",
    "        if fileName not in docIndex.values():\n",
    "            docIndex[len(docIndex)+1] = fileName\n",
    "    with open(jsonPath, 'w') as jsonFile:\n",
    "        json.dump(docIndex, jsonFile, indent=4)\n",
    "\n",
    "update_doc_index(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_inv_index(ngrams, jsonDoc = jsonDocIndex, jsonInv = jsonInvIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv in os.listdir(absPath):\n",
    "        with open(jsonInvPath, 'r') as jsonFile:\n",
    "            invIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        invIndex = {}\n",
    "#     Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "        \n",
    "    for ngram in ngrams:\n",
    "        for docID in docIndex:\n",
    "            if docID not in invIndex:\n",
    "                invIndex[docID] = {}\n",
    "            if ngram not in invIndex[docID]:\n",
    "                invIndex[docID][ngram] = {}\n",
    "                with open(txtPath+'/'+docIndex[docID], 'r', encoding='utf-8') as txtFile:\n",
    "                    text = txtFile.read().split(' ')\n",
    "                    while len(text) > ngram-1:\n",
    "                        term = \" \".join(text[:ngram])\n",
    "                        if term in invIndex[docID][ngram]:\n",
    "                            invIndex[docID][ngram][term] += 1\n",
    "                        else:\n",
    "                            invIndex[docID][ngram][term] = 1\n",
    "                        text.pop(0)\n",
    "    with open(jsonInvPath, 'w') as jsonFile:\n",
    "        json.dump(invIndex, jsonFile, indent=4)\n",
    "\n",
    "update_inv_index(list(range(2,4)), absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_gram_index(minFreq = 1, jsonInv = jsonInvIndex, jsonGram = jsonGramIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "#     Makes sure all files and directories exist (jsonInv, txtDir)\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonInvPath + '\" does not exist')\n",
    "        return\n",
    "#     jsonGram is created if it does not exist\n",
    "    jsonGramPath = absPath+'/'+jsonGram\n",
    "    if jsonGram in os.listdir(absPath):\n",
    "        with open(jsonGramPath, 'r') as jsonFile:\n",
    "            gramIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        gramIndex = {}\n",
    "#     Loads Inverted File index\n",
    "    with open(jsonInvPath, 'r') as jsonFile:\n",
    "        invIndex = json.load(jsonFile)\n",
    "        \n",
    "    for docID, ngrams in invIndex.items():\n",
    "        for terms in ngrams.values():\n",
    "            for term, freq in terms.items():\n",
    "                if freq >= minFreq:\n",
    "                    if term not in gramIndex:\n",
    "                        gramIndex[term] = {}\n",
    "                    gramIndex[term][docID] = freq\n",
    "#     Writes gramIndex to gram JSON file\n",
    "    with open(jsonGramPath, 'w') as jsonFile:\n",
    "        json.dump(gramIndex, jsonFile, indent=4)\n",
    "\n",
    "update_gram_index(absPath = absolute)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"natur languag process\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\tp123-zhang.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\"natur languag\" found in 8 documents\n",
      "\tLafferty_pcfg-notes.txt\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tp123-zhang.txt\n",
      "\tPartial Parsing Finite-State Cascades.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"comput scienc\" found in 6 documents\n",
      "\tLafferty_pcfg-notes.txt\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"languag process\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tp123-zhang.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"machin learn\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tp123-zhang.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"comput linguist\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tPartial Parsing Finite-State Cascades.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"intern confer\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tp123-zhang.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"inform retriev\" found in 5 documents\n",
      "\tp123-zhang.txt\n",
      "\tPartial Parsing Finite-State Cascades.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n"
     ]
    }
   ],
   "source": [
    "# Print grams found in 3 or more documents\n",
    "def getGrams(numDocs = 3, jsonDoc = jsonDocIndex, jsonGram = jsonGramIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "#     Makes sure all files and directories exist (textDir, jsonDoc, txtDir)\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonGramPath = absPath+'/'+jsonGram\n",
    "    if jsonGram not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonGramPath + '\" does not exist')\n",
    "        return\n",
    "#    Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "#    Loads gram index \n",
    "    with open(jsonGramPath, 'r') as jsonFile:\n",
    "        gramIndex = json.load(jsonFile)\n",
    "    \n",
    "    for gram, docs in gramIndex.items():\n",
    "        if len(docs) >= numDocs:\n",
    "            print(\"\\\"\"+gram+\"\\\" found in \"+str(len(docs))+\" documents\")\n",
    "            for docID in docs:\n",
    "                print(\"\\t\"+ str(docIndex[docID]))\n",
    "                \n",
    "getGrams(5, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lafferty_pcfg-notes.txt and Lafferty_pcfg-notes.txt\n",
      "Doc1 has 0 unique terms\n",
      "Doc2 has 0 unique terms\n",
      "Total like terms: \t2051\n",
      "Total unlike terms: \t0\n",
      "Cosine Similarity: 1.0\n",
      "Weighted Cosine Similarity: 1.0\n",
      "\n",
      "Lafferty_pcfg-notes.txt and LearningExecutableSemanticParsers.txt\n",
      "Doc1 has 2006 unique terms\n",
      "Doc2 has 5334 unique terms\n",
      "Total like terms: \t45\n",
      "Total unlike terms: \t7340\n",
      "Cosine Similarity: 0.2030132572821122\n",
      "Weighted Cosine Similarity: 0.23272391323219224\n",
      "\n",
      "Lafferty_pcfg-notes.txt and p123-zhang.txt\n",
      "Doc1 has 2043 unique terms\n",
      "Doc2 has 3254 unique terms\n",
      "Total like terms: \t8\n",
      "Total unlike terms: \t5297\n",
      "Cosine Similarity: 0.8356716504931925\n",
      "Weighted Cosine Similarity: 0.8356716504931925\n",
      "\n",
      "Lafferty_pcfg-notes.txt and Partial Parsing Finite-State Cascades.txt\n",
      "Doc1 has 2040 unique terms\n",
      "Doc2 has 2290 unique terms\n",
      "Total like terms: \t11\n",
      "Total unlike terms: \t4330\n",
      "Cosine Similarity: 0.5849962754207856\n",
      "Weighted Cosine Similarity: 0.5849962754207856\n",
      "\n",
      "Lafferty_pcfg-notes.txt and QueryEffectiveness.txt\n",
      "Doc1 has 2046 unique terms\n",
      "Doc2 has 1846 unique terms\n",
      "Total like terms: \t5\n",
      "Total unlike terms: \t3892\n",
      "Cosine Similarity: 0.9561828874675149\n",
      "Weighted Cosine Similarity: 0.9561828874675149\n",
      "\n",
      "Lafferty_pcfg-notes.txt and Text Clustering Algorithms.txt\n",
      "Doc1 has 2007 unique terms\n",
      "Doc2 has 13311 unique terms\n",
      "Total like terms: \t44\n",
      "Total unlike terms: \t15318\n",
      "Cosine Similarity: 0.6375436308731434\n",
      "Weighted Cosine Similarity: 0.6375436308731434\n",
      "\n",
      "Lafferty_pcfg-notes.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Doc1 has 2047 unique terms\n",
      "Doc2 has 4462 unique terms\n",
      "Total like terms: \t4\n",
      "Total unlike terms: \t6509\n",
      "Cosine Similarity: 0.5636214801906779\n",
      "Weighted Cosine Similarity: 0.5636214801906779\n",
      "\n",
      "Lafferty_pcfg-notes.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Doc1 has 1959 unique terms\n",
      "Doc2 has 37901 unique terms\n",
      "Total like terms: \t92\n",
      "Total unlike terms: \t39860\n",
      "Cosine Similarity: 0.31763349480560743\n",
      "Weighted Cosine Similarity: 0.324081469057268\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and LearningExecutableSemanticParsers.txt\n",
      "Doc1 has 0 unique terms\n",
      "Doc2 has 0 unique terms\n",
      "Total like terms: \t5379\n",
      "Total unlike terms: \t0\n",
      "Cosine Similarity: 1.0\n",
      "Weighted Cosine Similarity: 1.0000000000000002\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and p123-zhang.txt\n",
      "Doc1 has 5362 unique terms\n",
      "Doc2 has 3245 unique terms\n",
      "Total like terms: \t17\n",
      "Total unlike terms: \t8607\n",
      "Cosine Similarity: 0.4006534496076638\n",
      "Weighted Cosine Similarity: 0.6902193207873889\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and Partial Parsing Finite-State Cascades.txt\n",
      "Doc1 has 5357 unique terms\n",
      "Doc2 has 2279 unique terms\n",
      "Total like terms: \t22\n",
      "Total unlike terms: \t7636\n",
      "Cosine Similarity: 0.4945251939530321\n",
      "Weighted Cosine Similarity: 0.4945251939530321\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and QueryEffectiveness.txt\n",
      "Doc1 has 5369 unique terms\n",
      "Doc2 has 1841 unique terms\n",
      "Total like terms: \t10\n",
      "Total unlike terms: \t7210\n",
      "Cosine Similarity: 0.5327510929053121\n",
      "Weighted Cosine Similarity: 0.750756053322751\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and Text Clustering Algorithms.txt\n",
      "Doc1 has 5299 unique terms\n",
      "Doc2 has 13275 unique terms\n",
      "Total like terms: \t80\n",
      "Total unlike terms: \t18574\n",
      "Cosine Similarity: 0.38755413477798223\n",
      "Weighted Cosine Similarity: 0.4833686128413733\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Doc1 has 5347 unique terms\n",
      "Doc2 has 4434 unique terms\n",
      "Total like terms: \t32\n",
      "Total unlike terms: \t9781\n",
      "Cosine Similarity: 0.818964399380372\n",
      "Weighted Cosine Similarity: 0.8405883236754242\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Doc1 has 5098 unique terms\n",
      "Doc2 has 37712 unique terms\n",
      "Total like terms: \t281\n",
      "Total unlike terms: \t42810\n",
      "Cosine Similarity: 0.6979107861189516\n",
      "Weighted Cosine Similarity: 0.7899506584892592\n",
      "\n",
      "p123-zhang.txt and p123-zhang.txt\n",
      "Doc1 has 0 unique terms\n",
      "Doc2 has 0 unique terms\n",
      "Total like terms: \t3262\n",
      "Total unlike terms: \t0\n",
      "Cosine Similarity: 1.0000000000000002\n",
      "Weighted Cosine Similarity: 1.0\n",
      "\n",
      "p123-zhang.txt and Partial Parsing Finite-State Cascades.txt\n",
      "Doc1 has 3254 unique terms\n",
      "Doc2 has 2293 unique terms\n",
      "Total like terms: \t8\n",
      "Total unlike terms: \t5547\n",
      "Cosine Similarity: 0.8571428571428571\n",
      "Weighted Cosine Similarity: 0.8571428571428571\n",
      "\n",
      "p123-zhang.txt and QueryEffectiveness.txt\n",
      "Doc1 has 3242 unique terms\n",
      "Doc2 has 1831 unique terms\n",
      "Total like terms: \t20\n",
      "Total unlike terms: \t5073\n",
      "Cosine Similarity: 0.7448583696609948\n",
      "Weighted Cosine Similarity: 0.8216342914824332\n",
      "\n",
      "p123-zhang.txt and Text Clustering Algorithms.txt\n",
      "Doc1 has 3204 unique terms\n",
      "Doc2 has 13297 unique terms\n",
      "Total like terms: \t58\n",
      "Total unlike terms: \t16501\n",
      "Cosine Similarity: 0.5309050602494183\n",
      "Weighted Cosine Similarity: 0.5609352336250385\n",
      "\n",
      "p123-zhang.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Doc1 has 3243 unique terms\n",
      "Doc2 has 4447 unique terms\n",
      "Total like terms: \t19\n",
      "Total unlike terms: \t7690\n",
      "Cosine Similarity: 0.566448171703207\n",
      "Weighted Cosine Similarity: 0.8029584603179978\n",
      "\n",
      "p123-zhang.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Doc1 has 3170 unique terms\n",
      "Doc2 has 37901 unique terms\n",
      "Total like terms: \t92\n",
      "Total unlike terms: \t41071\n",
      "Cosine Similarity: 0.3785889025077362\n",
      "Weighted Cosine Similarity: 0.49794177991493355\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and Partial Parsing Finite-State Cascades.txt\n",
      "Doc1 has 0 unique terms\n",
      "Doc2 has 0 unique terms\n",
      "Total like terms: \t2301\n",
      "Total unlike terms: \t0\n",
      "Cosine Similarity: 1.0\n",
      "Weighted Cosine Similarity: 1.0\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and QueryEffectiveness.txt\n",
      "Doc1 has 2296 unique terms\n",
      "Doc2 has 1846 unique terms\n",
      "Total like terms: \t5\n",
      "Total unlike terms: \t4142\n",
      "Cosine Similarity: 0.5211062503347383\n",
      "Weighted Cosine Similarity: 0.5211062503347383\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and Text Clustering Algorithms.txt\n",
      "Doc1 has 2278 unique terms\n",
      "Doc2 has 13332 unique terms\n",
      "Total like terms: \t23\n",
      "Total unlike terms: \t15610\n",
      "Cosine Similarity: 0.619504000498241\n",
      "Weighted Cosine Similarity: 0.6335394326267051\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Doc1 has 2295 unique terms\n",
      "Doc2 has 4460 unique terms\n",
      "Total like terms: \t6\n",
      "Total unlike terms: \t6755\n",
      "Cosine Similarity: 0.6726915834767423\n",
      "Weighted Cosine Similarity: 0.6726915834767423\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Doc1 has 2178 unique terms\n",
      "Doc2 has 37870 unique terms\n",
      "Total like terms: \t123\n",
      "Total unlike terms: \t40048\n",
      "Cosine Similarity: 0.4460045671515864\n",
      "Weighted Cosine Similarity: 0.5141967305136445\n",
      "\n",
      "QueryEffectiveness.txt and QueryEffectiveness.txt\n",
      "Doc1 has 0 unique terms\n",
      "Doc2 has 0 unique terms\n",
      "Total like terms: \t1851\n",
      "Total unlike terms: \t0\n",
      "Cosine Similarity: 1.0000000000000002\n",
      "Weighted Cosine Similarity: 1.0\n",
      "\n",
      "QueryEffectiveness.txt and Text Clustering Algorithms.txt\n",
      "Doc1 has 1824 unique terms\n",
      "Doc2 has 13328 unique terms\n",
      "Total like terms: \t27\n",
      "Total unlike terms: \t15152\n",
      "Cosine Similarity: 0.8977066925657322\n",
      "Weighted Cosine Similarity: 0.8977066925657322\n",
      "\n",
      "QueryEffectiveness.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Doc1 has 1837 unique terms\n",
      "Doc2 has 4452 unique terms\n",
      "Total like terms: \t14\n",
      "Total unlike terms: \t6289\n",
      "Cosine Similarity: 0.6260850727121083\n",
      "Weighted Cosine Similarity: 0.7035493740548076\n",
      "\n",
      "QueryEffectiveness.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Doc1 has 1778 unique terms\n",
      "Doc2 has 37920 unique terms\n",
      "Total like terms: \t73\n",
      "Total unlike terms: \t39698\n",
      "Cosine Similarity: 0.3411821729192323\n",
      "Weighted Cosine Similarity: 0.3670346817654985\n",
      "\n",
      "Text Clustering Algorithms.txt and Text Clustering Algorithms.txt\n",
      "Doc1 has 0 unique terms\n",
      "Doc2 has 0 unique terms\n",
      "Total like terms: \t13355\n",
      "Total unlike terms: \t0\n",
      "Cosine Similarity: 0.9999999999999998\n",
      "Weighted Cosine Similarity: 1.0000000000000002\n",
      "\n",
      "Text Clustering Algorithms.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Doc1 has 13313 unique terms\n",
      "Doc2 has 4424 unique terms\n",
      "Total like terms: \t42\n",
      "Total unlike terms: \t17737\n",
      "Cosine Similarity: 0.3576915808263528\n",
      "Weighted Cosine Similarity: 0.3576915808263528\n",
      "\n",
      "Text Clustering Algorithms.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc1 has 12811 unique terms\n",
      "Doc2 has 37449 unique terms\n",
      "Total like terms: \t544\n",
      "Total unlike terms: \t50260\n",
      "Cosine Similarity: 0.37107072003627756\n",
      "Weighted Cosine Similarity: 0.4099192079028303\n",
      "\n",
      "Text-Analytics-and-Natural-Language-Processing--.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Doc1 has 0 unique terms\n",
      "Doc2 has 0 unique terms\n",
      "Total like terms: \t4466\n",
      "Total unlike terms: \t0\n",
      "Cosine Similarity: 1.0\n",
      "Weighted Cosine Similarity: 1.0\n",
      "\n",
      "Text-Analytics-and-Natural-Language-Processing--.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Doc1 has 4363 unique terms\n",
      "Doc2 has 37890 unique terms\n",
      "Total like terms: \t103\n",
      "Total unlike terms: \t42253\n",
      "Cosine Similarity: 0.8012747881651855\n",
      "Weighted Cosine Similarity: 0.8913253004499337\n",
      "\n",
      "Workshop on Robust Methods in Analysis of Natural Language Data.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Doc1 has 0 unique terms\n",
      "Doc2 has 0 unique terms\n",
      "Total like terms: \t37993\n",
      "Total unlike terms: \t0\n",
      "Cosine Similarity: 1.0\n",
      "Weighted Cosine Similarity: 1.0000000000000002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def calcCosSim(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        print(\"Lists do not have the same number of elements\")\n",
    "        return -1\n",
    "    sumProd = sum(n1*n2 for n1, n2 in zip(list1, list2))\n",
    "    sumL1 = sum(n**2 for n in list1)\n",
    "    sumL2 = sum(n**2 for n in list2)\n",
    "    \n",
    "    return sumProd/(math.sqrt(sumL1)*math.sqrt(sumL2))\n",
    "\n",
    "# Create vectors for all n-grams (with freq>1?) between two docs, multiply them,\n",
    "#     then divide by the product Euclidian norms\n",
    "# Print out similar phrases\n",
    "def cossim(ngrams, jsonDoc = jsonDocIndex, jsonInv = jsonInvIndex, absPath = os.getcwd()):\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonInv not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    \n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    with open(jsonDocPath, 'r') as jsonDoc:\n",
    "        docIndex = json.load(jsonDoc)\n",
    "    with open(jsonInvPath, 'r') as jsonInv:\n",
    "        invIndex = json.load(jsonInv)\n",
    "            \n",
    "        \n",
    "    for doc1 in invIndex:\n",
    "        doc1Terms = []\n",
    "        for ngram, terms in invIndex[doc1].items():\n",
    "            if int(ngram) in ngrams:\n",
    "                for term in terms:\n",
    "                    doc1Terms.append(term)\n",
    "        for doc2 in invIndex:\n",
    "            if doc2 >= doc1:\n",
    "                print(docIndex[doc1]+\" and \"+docIndex[doc2])\n",
    "                allTerms = doc1Terms.copy()\n",
    "                likeTerms = []\n",
    "                for ngram, terms in invIndex[doc2].items():\n",
    "                    if int(ngram) in ngrams:\n",
    "                        for term in terms:\n",
    "                            if term not in allTerms:\n",
    "                                allTerms.append(term)\n",
    "                            else:\n",
    "                                likeTerms.append(term)\n",
    "                allTerms.sort()\n",
    "                doc1freq = []\n",
    "                doc2freq = []\n",
    "                doc1weighted = []\n",
    "                doc2weighted = []\n",
    "                for term in likeTerms:\n",
    "                    weight = ngrams[len(term.split())]\n",
    "                    ngram = str(len(term.split()))\n",
    "                    doc1freq.append(invIndex[doc1][ngram].get(term))\n",
    "                    doc2freq.append(invIndex[doc2][ngram].get(term))\n",
    "                    doc1weighted.append(invIndex[doc1][ngram].get(term)*weight)\n",
    "                    doc2weighted.append(invIndex[doc2][ngram].get(term)*weight)\n",
    "                cosSimilarity = calcCosSim(doc1freq, doc2freq)\n",
    "                weightedCosSim = calcCosSim(doc1weighted, doc2weighted)\n",
    "#                 print(\"Like terms (doc1, doc2):\")\n",
    "#                 for term in likeTerms:\n",
    "#                     print(\"\\t\"+term+\" (\"+str(invIndex[doc1][str(len(term.split()))][term])+\", \"+str(invIndex[doc2][str(len(term.split()))][term])+\")\")\n",
    "                numLike = len(likeTerms)\n",
    "                print(\"Doc1 has \"+str(len(doc1Terms)-numLike)+\" unique terms\")\n",
    "                print(\"Doc2 has \"+str(len(allTerms)-len(doc1Terms))+\" unique terms\")\n",
    "                print(\"Total like terms: \\t\"+str(numLike))\n",
    "                print(\"Total unlike terms: \\t\"+str(len(allTerms)-numLike))\n",
    "                print(\"Cosine Similarity: \"+ str(cosSimilarity))\n",
    "                print(\"Weighted Cosine Similarity: \"+ str(weightedCosSim), end='\\n\\n')\n",
    "gramWeight = {2:1, 3:5}\n",
    "cossim(gramWeight, absPath = absolute)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram parser-based inverted file \n",
    "# (TF-DIF to remove trigrams common to most or all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithm based on trigram inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigram parser-based info to inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement clustering on bigram inverted file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
