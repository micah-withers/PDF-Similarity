{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future improvements\n",
    "# Separate dir for each step\n",
    "# Create JSON array to assign IDs, \n",
    "#     keep track of PDF files process (each step?) etc.\n",
    "# Remove numbers, urls\n",
    "# Change variable names of jsonDoc, jsonInv, jsonGram to avoid redefiining argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os # For file/directory interaction\n",
    "import time, sys\n",
    "from datetime import datetime, date # For log data\n",
    "import re # For text replacement\n",
    "import spacy # Pipeline processes (stopword and punctuation removal, lemmatization)\n",
    "from nltk.stem.snowball import SnowballStemmer # Pipeline process for stemming\n",
    "import json\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txtFilesDir = 'Text Files'\n",
    "rtnFilesDir = 'n Removed'\n",
    "spaceFilesDir = 'No Spaces'\n",
    "swFilesDir = 'Stop Words'\n",
    "engFilesDir = 'English Words'\n",
    "stemFilesDir = 'Stemmed'\n",
    "jsonDocIndex = 'doc_dictionary.json'\n",
    "jsonInvIndex = 'inverted_index.json'\n",
    "jsonGramIndex = 'gram_index.json'\n",
    "\n",
    "absolute = 'C:/Users/micah/Documents/IWU/CIS Practicum/Files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on 'Lafferty_pcfg-notes.pdf'. . . \"Lafferty_p...\" already exists\n",
      "\n",
      "Now on 'LearningExecutableSemanticParsers.pdf'. . . \"LearningEx...\" already exists\n",
      "\n",
      "Now on 'p123-zhang.pdf'. . . \"p123-zhang...\" already exists\n",
      "\n",
      "Now on 'Partial Parsing Finite-State Cascades.pdf'. . . \"Partial Pa...\" already exists\n",
      "\n",
      "Now on 'QueryEffectiveness.pdf'. . . \"QueryEffec...\" already exists\n",
      "\n",
      "Now on 'Text Clustering Algorithms.pdf'. . . \"Text Clust...\" already exists\n",
      "\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.pdf'. . . \"Text-Analy...\" already exists\n",
      "\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.pdf'. . . \"Workshop o...\" already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-condition: All PDF files to be processed are in the sub-directory\n",
    "#     pdfDir, and pdfDir is in absPath. absPath is by default the \n",
    "#     directory in which the program is executed\n",
    "# Post-condition: All PDF files processed without error are converted to\n",
    "#     text files which are placed in a new sub-directory 'Text Files'\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter, process_pdf#process_pdf\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "def getText(pdfPath):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    sio = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, sio, laparams=laparams)\n",
    "#     interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    \n",
    "    with open(pdfPath, 'rb') as pdfFile:\n",
    "        process_pdf(rsrcmgr, device, pdfFile)\n",
    "#         parser = PDFParser(pdfFile)\n",
    "#         doc.set_parser(parser)\n",
    "#         interpreter.process_pdf(doc)\n",
    "#             layout = device.get_result()\n",
    "#             for element in layout:\n",
    "#                 if instanceof(element, LTTextBoxHorizontal):\n",
    "#                     print(element.get_text)\n",
    "                \n",
    "    \n",
    "    text = sio.getvalue()\n",
    "    \n",
    "    device.close()\n",
    "    sio.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def pdfToText(pdfDir, absPath = os.getcwd(), txtDir = txtFilesDir):\n",
    "    \n",
    "    pdfPath = absPath+'/'+pdfDir\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if pdfDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + directory + '\" does not exist')\n",
    "        return\n",
    "# Creates 'Text Files' directory for converted PDFs\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        os.mkdir(txtPath)\n",
    "    \n",
    "    docNum = 0\n",
    "    stopAt = 25\n",
    "    totalNum = len([file for file in os.scandir(pdfPath) if file.name.endswith('.pdf')])\n",
    "    with open(absPath+'/'+'log.txt', 'a+', encoding=\"utf-8\") as log:\n",
    "        log.write(\"PDF to Text\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")        \n",
    "# Moves on to next entity if the current entity is not a PDF\n",
    "        for entity in os.scandir(pdfPath):\n",
    "            if not entity.name.endswith('.pdf'):\n",
    "                continue\n",
    "            docNum += 1\n",
    "            index = -4 # Remove '.pdf' from file name when creating '.txt' file\n",
    "            fileName = entity.name[:index]+'.txt'\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            \n",
    "# This block attempts to read the PDF file, extract text from each page,\n",
    "#     and write the text to a text file with the same name\n",
    "# Some documents are protected, corrupted, etc. and text cannot be extracted\n",
    "# Exceptions are recorded in log.txt\n",
    "# hasError remains true until each step in the try block is complete\n",
    "            if fileName not in os.listdir(txtPath): \n",
    "                hasError = True\n",
    "                try:\n",
    "                    text = getText(pdfPath+'/'+entity.name)\n",
    "                    txtFile = open(txtPath+'/'+fileName, 'w+', encoding=\"utf-8\")\n",
    "                    txtFile.write(text)\n",
    "                    print(\"done\\n\")\n",
    "                    hasError = False\n",
    "#                     except TypeError as e:\n",
    "#                         log.write(\"TypeError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except PyPDF2.utils.PdfReadError as e:\n",
    "#                         log.write(\"PdfReadError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except OSError as e:\n",
    "#                         log.write(\"OSError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except decimal.InvalidOperation as e:\n",
    "#                         log.write(\"InvalidOperation: \" + entity.name + \":\\n\" + str(e))\n",
    "                except Exception as e:\n",
    "                    log.write(str(docNum)+\": \" + entity.name + \": \\n\\t\" + str(e)+\"\\n\")\n",
    "\n",
    "                if hasError:\n",
    "                    print(\"there was an error reading this document. See log for details. Reference number \"+str(docNum)+\".\\n\")\n",
    "            else:\n",
    "                max = 10\n",
    "                if len(fileName) > max:\n",
    "                    print('\"'+fileName[:max]+'...\"', end='')\n",
    "                else:\n",
    "                    print('\"'+fileName+'\"', end='')\n",
    "                print(' already exists\\n')\n",
    "            if docNum == stopAt:\n",
    "                print(\"PDF to Text was stopped after \"+str(docNum)+\" documents.\")\n",
    "                break\n",
    "        log.write(\"\\n\\n\")\n",
    "pdfToText('Working', absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on 10 (good) files at a time until pipeline works\n",
    "#   then incrementally add files and clean up errors\n",
    "\n",
    "# Function to remove \\n\n",
    "def rmvN(txtDir = txtFilesDir, rtnDir = rtnFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    rtnPath = absPath+'/'+rtnDir\n",
    "    if rtnDir not in os.listdir(absPath):\n",
    "        os.mkdir(rtnPath)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(rtnPath+'/'+entity.name, 'w+', encoding='utf-8') as rtnFile:\n",
    "                text = txtFile.read()\n",
    "                text = re.sub('-\\n', '', text)\n",
    "                text = re.sub('\\n', '', text)\n",
    "                rtnFile.write(text)\n",
    "                rtnFile.truncate()\n",
    "        print(\"done\")\n",
    "rmvN(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Funtion to move files without spaces to new 'Without Spaces' directory         \n",
    "def checkSpaces(txtDir = rtnFilesDir, spacesDir = spaceFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    spacesPath = absPath+'/'+spacesDir\n",
    "    if spacesDir not in os.listdir(absPath):\n",
    "        os.mkdir(spacesPath)\n",
    "        \n",
    "    with open(absPath+'/'+'Spaces.txt', 'a+', encoding='utf-8') as spaces: \n",
    "        spaces.write(\"Check Spaces\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")\n",
    "        for entity in os.scandir(txtPath):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            txtFile = open(txtPath+'/'+entity.name, 'r', encoding='utf-8')\n",
    "            text = txtFile.read()\n",
    "            split = text.split(' ')\n",
    "            if len(split) < len(text)/10 or len(text) < 100 or text == '':\n",
    "                txtFile.close()\n",
    "                spaces.write(entity.name+'\\n')\n",
    "                if entity.name not in os.listdir(spacesPath):\n",
    "                    os.rename(txtPath+'/'+entity.name, spacesPath+'/'+entity.name)\n",
    "                else:\n",
    "                    os.remove(txtPath+'/'+entity.name)\n",
    "            print(\"done\")\n",
    "        spaces.write('\\n\\n')\n",
    "checkSpaces(absPath = absolute) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "# NLTK or SpaCy\n",
    "# Inverted File: gram:[doc1, doc3] or gram:[[doc1,freq], [doc3,freq]]\n",
    "def rmvStopWords(nlp, txtDir = rtnFilesDir, swDir = swFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    swPath = absPath+'/'+swDir\n",
    "    if swDir not in os.listdir(absPath):\n",
    "        os.mkdir(swPath)\n",
    "\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(swPath+'/'+entity.name, 'w+', encoding='utf-8') as swFile:\n",
    "                doc = nlp(txtFile.read())\n",
    "                noStopWords = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.text.isnumeric()]\n",
    "                swFile.write(\" \".join(noStopWords))\n",
    "                swFile.truncate()\n",
    "        print(\"done\")\n",
    "\n",
    "rmvStopWords(nlp, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-english words\n",
    "def rmvNonEng(txtDir = swFilesDir, engDir = engFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    engPath = absPath+'/'+engDir\n",
    "    if engDir not in os.listdir(absPath):\n",
    "        os.mkdir(engPath)\n",
    "    with open(absPath+'/'+'words_dictionary.json') as json_file:\n",
    "        words = json.load(json_file)\n",
    "        \n",
    "    lets = []\n",
    "    alph = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for let in alph:\n",
    "        lets.append(let)\n",
    "        for char in alph:\n",
    "            lets.append(let+char)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(engPath+'/'+entity.name, 'w+', encoding='utf-8') as engFile:\n",
    "                text = txtFile.read().split(' ')\n",
    "                engChars = [word for word in text if word in words and word not in lets]\n",
    "                engFile.write(\" \".join(engChars))\n",
    "                engFile.truncate()\n",
    "        print(\"done\")\n",
    "rmvNonEng(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words in all documents\n",
    "def stem(txtDir = engFilesDir, stemDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    stemPath = absPath+'/'+stemDir\n",
    "    if stemDir not in os.listdir(absPath):\n",
    "        os.mkdir(stemPath)\n",
    "        \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(stemPath+'/'+entity.name, 'w+', encoding='utf-8') as stemFile:\n",
    "                text = txtFile.read().split(' ')\n",
    "                stemmed = [stemmer.stem(word) for word in text]\n",
    "                stemFile.write(\" \".join(stemmed))\n",
    "                stemFile.truncate()\n",
    "        print(\"done\")\n",
    "stem(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create JSON with file_name : doc_id\n",
    "def update_doc_index(jsonDoc = jsonDocIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    jsonPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc in os.listdir(absPath):\n",
    "        with open(jsonPath, 'r') as jsonFile:\n",
    "            docIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        docIndex = {}\n",
    "    for fileName in os.listdir(txtPath):\n",
    "        if fileName not in docIndex.values():\n",
    "            docIndex[len(docIndex)+1] = fileName\n",
    "    with open(jsonPath, 'w') as jsonFile:\n",
    "        json.dump(docIndex, jsonFile, indent=4)\n",
    "\n",
    "update_doc_index(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_inv_index(ngrams, jsonDoc = jsonDocIndex, jsonInv = jsonInvIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv in os.listdir(absPath):\n",
    "        with open(jsonInvPath, 'r') as jsonFile:\n",
    "            invIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        invIndex = {}\n",
    "#     Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "        \n",
    "    for ngram in ngrams:\n",
    "        for docID in docIndex:\n",
    "            if docID not in invIndex:\n",
    "                invIndex[docID] = {}\n",
    "            if ngram not in invIndex[docID]:\n",
    "                invIndex[docID][ngram] = {}\n",
    "                with open(txtPath+'/'+docIndex[docID], 'r', encoding='utf-8') as txtFile:\n",
    "                    text = txtFile.read().split(' ')\n",
    "                    while len(text) > ngram-1:\n",
    "                        term = \" \".join(text[:ngram])\n",
    "                        if term in invIndex[docID][ngram]:\n",
    "                            invIndex[docID][ngram][term] += 1\n",
    "                        else:\n",
    "                            invIndex[docID][ngram][term] = 1\n",
    "                        text.pop(0)\n",
    "    with open(jsonInvPath, 'w') as jsonFile:\n",
    "        json.dump(invIndex, jsonFile, indent=4)\n",
    "\n",
    "update_inv_index(list(range(2,4)), absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_gram_index(minFreq = 1, jsonInv = jsonInvIndex, jsonGram = jsonGramIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "#     Makes sure all files and directories exist (jsonInv, txtDir)\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonInvPath + '\" does not exist')\n",
    "        return\n",
    "#     jsonGram is created if it does not exist\n",
    "    jsonGramPath = absPath+'/'+jsonGram\n",
    "    if jsonGram in os.listdir(absPath):\n",
    "        with open(jsonGramPath, 'r') as jsonFile:\n",
    "            gramIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        gramIndex = {}\n",
    "#     Loads Inverted File index\n",
    "    with open(jsonInvPath, 'r') as jsonFile:\n",
    "        invIndex = json.load(jsonFile)\n",
    "        \n",
    "    for docID, ngrams in invIndex.items():\n",
    "        for terms in ngrams.values():\n",
    "            for term, freq in terms.items():\n",
    "                if freq >= minFreq:\n",
    "                    if term not in gramIndex:\n",
    "                        gramIndex[term] = {}\n",
    "                    gramIndex[term][docID] = freq\n",
    "#     Writes gramIndex to gram JSON file\n",
    "    with open(jsonGramPath, 'w') as jsonFile:\n",
    "        json.dump(gramIndex, jsonFile, indent=4)\n",
    "\n",
    "update_gram_index(absPath = absolute)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"natur languag process\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\tp123-zhang.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\"natur languag\" found in 8 documents\n",
      "\tLafferty_pcfg-notes.txt\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tp123-zhang.txt\n",
      "\tPartial Parsing Finite-State Cascades.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"comput scienc\" found in 6 documents\n",
      "\tLafferty_pcfg-notes.txt\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"languag process\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tp123-zhang.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"machin learn\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tp123-zhang.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"comput linguist\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tPartial Parsing Finite-State Cascades.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tText-Analytics-and-Natural-Language-Processing--.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"intern confer\" found in 5 documents\n",
      "\tLearningExecutableSemanticParsers.txt\n",
      "\tp123-zhang.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "\"inform retriev\" found in 5 documents\n",
      "\tp123-zhang.txt\n",
      "\tPartial Parsing Finite-State Cascades.txt\n",
      "\tQueryEffectiveness.txt\n",
      "\tText Clustering Algorithms.txt\n",
      "\tWorkshop on Robust Methods in Analysis of Natural Language Data.txt\n"
     ]
    }
   ],
   "source": [
    "# Print grams found in 3 or more documents\n",
    "def getGrams(numDocs = 3, jsonDoc = jsonDocIndex, jsonGram = jsonGramIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "#     Makes sure all files and directories exist (textDir, jsonDoc, txtDir)\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonGramPath = absPath+'/'+jsonGram\n",
    "    if jsonGram not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonGramPath + '\" does not exist')\n",
    "        return\n",
    "#    Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "#    Loads gram index \n",
    "    with open(jsonGramPath, 'r') as jsonFile:\n",
    "        gramIndex = json.load(jsonFile)\n",
    "    \n",
    "    for gram, docs in gramIndex.items():\n",
    "        if len(docs) >= numDocs:\n",
    "            print(\"\\\"\"+gram+\"\\\" found in \"+str(len(docs))+\" documents\")\n",
    "            for docID in docs:\n",
    "                print(\"\\t\"+ str(docIndex[docID]))\n",
    "                \n",
    "getGrams(5, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lafferty_pcfg-notes.txt and LearningExecutableSemanticParsers.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 48)\n",
      "\tgrammar model (1, 2)\n",
      "\tactual comput (1, 1)\n",
      "\tdenot set (1, 3)\n",
      "\tspeech recognit (4, 1)\n",
      "\tstatist model (1, 3)\n",
      "\tsyntact pars (1, 1)\n",
      "\tconsist follow (1, 1)\n",
      "\tprobabl deriv (1, 3)\n",
      "\trule form (2, 2)\n",
      "\tderiv general (1, 1)\n",
      "\tcontext free (9, 1)\n",
      "\tfree grammar (4, 1)\n",
      "\trule build (1, 1)\n",
      "\tlanguag model (5, 1)\n",
      "\tnumber time (5, 2)\n",
      "\tassign probabl (4, 3)\n",
      "\tprobabl give (2, 1)\n",
      "\trule grammar (1, 1)\n",
      "\tparamet respect (1, 1)\n",
      "\tparamet deriv (1, 1)\n",
      "\tmaxim likelihood (2, 1)\n",
      "\tlikelihood train (2, 1)\n",
      "\ttrain datum (1, 2)\n",
      "\tlog likelihood (3, 2)\n",
      "\tsimpl approach (1, 1)\n",
      "\tupdat paramet (2, 1)\n",
      "\tlanguag grammar (3, 2)\n",
      "\tform basic (1, 1)\n",
      "\tpoint view (1, 2)\n",
      "\tvalid sentenc (1, 1)\n",
      "\tmodel deriv (1, 1)\n",
      "\tfeatur base (1, 2)\n",
      "\tpars algorithm (1, 1)\n",
      "\tpars chart (1, 1)\n",
      "\testim model (1, 1)\n",
      "\tlexic rule (1, 3)\n",
      "\tparamet estim (1, 1)\n",
      "\tintroduc new (1, 1)\n",
      "\tgive new (1, 1)\n",
      "\tprobabilist grammar (1, 1)\n",
      "\tcomput scienc (1, 1)\n",
      "\tcontext free grammar (4, 1)\n",
      "\tmaxim likelihood train (1, 1)\n",
      "\tnatur languag grammar (1, 1)\n",
      "Doc1 has 2006 unique terms\n",
      "Doc2 has 5334 unique terms\n",
      "Total like terms: \t45\n",
      "Total unlike terms: \t7340\n",
      "Cosine Similarity: 0.004961105719664268\n",
      "\n",
      "Lafferty_pcfg-notes.txt and p123-zhang.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 1)\n",
      "\tfollow section (2, 1)\n",
      "\tsection describ (1, 2)\n",
      "\tforward backward (3, 1)\n",
      "\tfollow exampl (1, 1)\n",
      "\tspecial case (2, 1)\n",
      "\tactual implement (1, 1)\n",
      "\tbegin start (1, 1)\n",
      "Doc1 has 2043 unique terms\n",
      "Doc2 has 3254 unique terms\n",
      "Total like terms: \t8\n",
      "Total unlike terms: \t5297\n",
      "Cosine Similarity: 0.0003040329819611653\n",
      "\n",
      "Lafferty_pcfg-notes.txt and Partial Parsing Finite-State Cascades.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 2)\n",
      "\tconstruct pars (1, 1)\n",
      "\tpars tree (4, 9)\n",
      "\tcontext free (9, 2)\n",
      "\tsurround context (1, 1)\n",
      "\tspeech recognit (4, 1)\n",
      "\ttrain corpus (4, 1)\n",
      "\tmaximum likelihood (2, 1)\n",
      "\trule probabl (6, 1)\n",
      "\tpars sentenc (1, 1)\n",
      "\tpars pars (1, 1)\n",
      "Doc1 has 2040 unique terms\n",
      "Doc2 has 2290 unique terms\n",
      "Total like terms: \t11\n",
      "Total unlike terms: \t4330\n",
      "Cosine Similarity: 0.002211510221930375\n",
      "\n",
      "Lafferty_pcfg-notes.txt and QueryEffectiveness.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 2)\n",
      "\tlarg small (1, 2)\n",
      "\tactual comput (1, 1)\n",
      "\tword associ (1, 1)\n",
      "\tcomput scienc (1, 2)\n",
      "Doc1 has 2046 unique terms\n",
      "Doc2 has 1846 unique terms\n",
      "Total like terms: \t5\n",
      "Total unlike terms: \t3892\n",
      "Cosine Similarity: 0.00020217547615202983\n",
      "\n",
      "Lafferty_pcfg-notes.txt and Text Clustering Algorithms.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tproblem appli (1, 1)\n",
      "\trecent advanc (1, 2)\n",
      "\talgorithm appli (1, 3)\n",
      "\tfollow section (2, 1)\n",
      "\tsection describ (1, 1)\n",
      "\tmethod probabilist (2, 3)\n",
      "\tnon negat (1, 13)\n",
      "\ttrain datum (1, 3)\n",
      "\tprobabilist model (1, 3)\n",
      "\tpoint view (1, 1)\n",
      "\tlet assum (1, 2)\n",
      "\tlet matrix (1, 1)\n",
      "\tpartial deriv (2, 1)\n",
      "\tlanguag model (5, 4)\n",
      "\talgorithm context (1, 2)\n",
      "\torder show (1, 1)\n",
      "\tcorpus second (1, 1)\n",
      "\tword new (1, 1)\n",
      "\tiniti set (2, 3)\n",
      "\tprocess repeat (1, 1)\n",
      "\tneed comput (1, 1)\n",
      "\tconsid similar (1, 1)\n",
      "\tneed method (1, 1)\n",
      "\talgorithm algorithm (2, 1)\n",
      "\tlead larg (1, 1)\n",
      "\talgorithm direct (1, 1)\n",
      "\tword associ (1, 1)\n",
      "\tnatur languag (1, 2)\n",
      "\tstring word (3, 1)\n",
      "\tparamet estim (1, 3)\n",
      "\tmaximum likelihood (2, 4)\n",
      "\tbasic method (2, 1)\n",
      "\talgorithm maxim (1, 1)\n",
      "\tprobabl respect (1, 2)\n",
      "\tfollow step (2, 1)\n",
      "\tnumber paramet (1, 1)\n",
      "\tcase algorithm (2, 1)\n",
      "\talgorithm step (1, 1)\n",
      "\tgenerat word (1, 1)\n",
      "\tprobabl assign (1, 1)\n",
      "\talgorithm maximum (1, 1)\n",
      "\tdetail algorithm (1, 1)\n",
      "\tlanguag comput (1, 1)\n",
      "\tcomput scienc (1, 1)\n",
      "Doc1 has 2007 unique terms\n",
      "Doc2 has 13311 unique terms\n",
      "Total like terms: \t44\n",
      "Total unlike terms: \t15318\n",
      "Cosine Similarity: 0.0011603314681913144\n",
      "\n",
      "Lafferty_pcfg-notes.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 23)\n",
      "\ttrain datum (1, 1)\n",
      "\tcomput scienc (1, 1)\n",
      "\tsimpl approach (1, 1)\n",
      "Doc1 has 2047 unique terms\n",
      "Doc2 has 4462 unique terms\n",
      "Total like terms: \t4\n",
      "Total unlike terms: \t6509\n",
      "Cosine Similarity: 0.0004611057496322616\n",
      "\n",
      "Lafferty_pcfg-notes.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 45)\n",
      "\tsyntact pars (1, 4)\n",
      "\tpars pars (1, 3)\n",
      "\trecent advanc (1, 3)\n",
      "\tstatist base (1, 2)\n",
      "\tmaximum likelihood (2, 2)\n",
      "\texampl simpl (1, 1)\n",
      "\tprobabilist context (3, 1)\n",
      "\tcontext free (9, 4)\n",
      "\trule probabl (6, 1)\n",
      "\tassign probabl (4, 1)\n",
      "\tfree grammar (4, 2)\n",
      "\testim model (1, 1)\n",
      "\tsentenc probabl (1, 2)\n",
      "\tlanguag model (5, 2)\n",
      "\tmodel speech (1, 1)\n",
      "\tprobabilist model (1, 2)\n",
      "\texampl probabilist (1, 1)\n",
      "\tgrammar write (1, 2)\n",
      "\tnumber non (1, 1)\n",
      "\tpars algorithm (1, 6)\n",
      "\tuse rule (1, 3)\n",
      "\ttrain corpus (4, 3)\n",
      "\tprobabl depend (1, 1)\n",
      "\ttoy grammar (1, 1)\n",
      "\tnumber rule (1, 10)\n",
      "\tstatist model (1, 3)\n",
      "\tsentenc pars (3, 2)\n",
      "\tcertain probabl (1, 1)\n",
      "\tprobabl pars (2, 1)\n",
      "\tillustr general (1, 1)\n",
      "\talgorithm extract (1, 1)\n",
      "\tcomput scienc (1, 1)\n",
      "\tgrammar probabilist (1, 1)\n",
      "\tappli natur (1, 2)\n",
      "\tlanguag grammar (3, 1)\n",
      "\tword sentenc (1, 2)\n",
      "\tprocess set (1, 1)\n",
      "\tcorpus sentenc (1, 1)\n",
      "\tpoint view (1, 2)\n",
      "\tnoun verb (2, 4)\n",
      "\tfeatur base (1, 2)\n",
      "\tfollow exampl (1, 3)\n",
      "\trule grammar (1, 2)\n",
      "\tappli algorithm (1, 1)\n",
      "\tlet assum (1, 1)\n",
      "\trule consid (1, 1)\n",
      "\tsocieti comput (1, 1)\n",
      "\tdescrib algorithm (2, 2)\n",
      "\tsentenc similar (1, 1)\n",
      "\tpars valid (1, 1)\n",
      "\tpars tree (4, 3)\n",
      "\tsentenc syntact (2, 3)\n",
      "\tinvolv pars (1, 1)\n",
      "\tstring word (3, 1)\n",
      "\tsection describ (1, 9)\n",
      "\tconsid exampl (1, 3)\n",
      "\texampl word (1, 2)\n",
      "\tnormal form (2, 1)\n",
      "\trule deriv (2, 1)\n",
      "\timplement algorithm (1, 1)\n",
      "\tpars enabl (1, 1)\n",
      "\tproblem associ (1, 1)\n",
      "\tfollow section (2, 4)\n",
      "\tgrammar requir (1, 1)\n",
      "\tpossibl section (1, 1)\n",
      "\tdetail descript (1, 1)\n",
      "\treader comput (1, 1)\n",
      "\talgorithm algorithm (2, 2)\n",
      "\tsimpl approach (1, 1)\n",
      "\tderiv tree (1, 1)\n",
      "\tform word (1, 1)\n",
      "\tresult section (1, 1)\n",
      "\tlike rule (1, 1)\n",
      "\tnumber time (5, 1)\n",
      "\trule number (1, 1)\n",
      "\trule generat (1, 1)\n",
      "\trule build (1, 1)\n",
      "\tfollow step (2, 1)\n",
      "\ttheori languag (1, 1)\n",
      "\tnew mexico (1, 1)\n",
      "\tforward backward (3, 2)\n",
      "\tdenot set (1, 2)\n",
      "\tbasic method (2, 1)\n",
      "\tsimilar method (1, 1)\n",
      "\thistor reason (1, 1)\n",
      "\tproblem approach (1, 1)\n",
      "\tlanguag comput (1, 1)\n",
      "\tsyntact pars pars (1, 1)\n",
      "\tprobabilist context free (3, 1)\n",
      "\tcontext free grammar (4, 1)\n",
      "\tnumber rule number (1, 1)\n",
      "Doc1 has 1959 unique terms\n",
      "Doc2 has 37901 unique terms\n",
      "Total like terms: \t92\n",
      "Total unlike terms: \t39860\n",
      "Cosine Similarity: 0.003490163058007821\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and p123-zhang.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tuse semant (1, 1)\n",
      "\tnatur languag (48, 1)\n",
      "\tlanguag process (9, 1)\n",
      "\tsystem capabl (1, 1)\n",
      "\twrite rule (1, 1)\n",
      "\tlist requir (1, 1)\n",
      "\tcontext semant (1, 1)\n",
      "\tmodel distribut (1, 1)\n",
      "\tknowledg base (1, 1)\n",
      "\tsystem requir (1, 1)\n",
      "\tlanguag linguist (1, 1)\n",
      "\tfigur show (1, 1)\n",
      "\tshow exampl (1, 1)\n",
      "\tpresent semant (1, 1)\n",
      "\tmachin learn (6, 1)\n",
      "\tintern confer (3, 2)\n",
      "\tnatur languag process (9, 1)\n",
      "Doc1 has 5362 unique terms\n",
      "Doc2 has 3245 unique terms\n",
      "Total like terms: \t17\n",
      "Total unlike terms: \t8607\n",
      "Cosine Similarity: 0.004119740004872438\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and Partial Parsing Finite-State Cascades.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (48, 2)\n",
      "\tcontext free (1, 2)\n",
      "\tcomput linguist (13, 1)\n",
      "\tbroad coverag (3, 1)\n",
      "\trepres sequenc (1, 1)\n",
      "\tregular express (3, 2)\n",
      "\tgrammar grammar (1, 1)\n",
      "\tlarg larg (1, 1)\n",
      "\targument structur (1, 1)\n",
      "\taction exampl (1, 1)\n",
      "\texampl case (1, 1)\n",
      "\tleav right (1, 1)\n",
      "\tparser literatur (1, 1)\n",
      "\tcategori grammar (2, 1)\n",
      "\tchart parser (1, 1)\n",
      "\torder magnitud (1, 1)\n",
      "\tspeech recognit (1, 1)\n",
      "\tdatum import (1, 1)\n",
      "\tbase pars (1, 1)\n",
      "\tmodel languag (1, 1)\n",
      "\tnoun phrase (1, 1)\n",
      "\tlarg corpora (1, 1)\n",
      "Doc1 has 5357 unique terms\n",
      "Doc2 has 2279 unique terms\n",
      "Total like terms: \t22\n",
      "Total unlike terms: \t7636\n",
      "Cosine Similarity: 0.0023116681038114046\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and QueryEffectiveness.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tremain open (1, 1)\n",
      "\tnatur languag (48, 2)\n",
      "\torder magnitud (1, 1)\n",
      "\tquestion perform (1, 1)\n",
      "\tsystem base (1, 1)\n",
      "\tactual comput (1, 1)\n",
      "\tlanguag process (9, 1)\n",
      "\tintern confer (3, 3)\n",
      "\tcomput scienc (1, 2)\n",
      "\tnatur languag process (9, 1)\n",
      "Doc1 has 5369 unique terms\n",
      "Doc2 has 1841 unique terms\n",
      "Total like terms: \t10\n",
      "Total unlike terms: \t7210\n",
      "Cosine Similarity: 0.005033680216463265\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and Text Clustering Algorithms.txt\n",
      "Like terms (doc1, doc2):\n",
      "\ttext problem (1, 2)\n",
      "\tinclud base (1, 1)\n",
      "\tnumber word (1, 1)\n",
      "\tbase represent (1, 2)\n",
      "\tbase method (1, 3)\n",
      "\tpresent featur (1, 1)\n",
      "\tnumber featur (1, 1)\n",
      "\tfeatur use (1, 1)\n",
      "\tsupervis learn (1, 3)\n",
      "\tlarg collect (1, 2)\n",
      "\trequir initi (1, 1)\n",
      "\tsupervis train (1, 1)\n",
      "\ttrain datum (2, 3)\n",
      "\texampl case (1, 1)\n",
      "\tmethod requir (1, 2)\n",
      "\tbase featur (1, 2)\n",
      "\tset featur (1, 1)\n",
      "\tcommon approach (1, 1)\n",
      "\tuse semant (1, 1)\n",
      "\tmodel latent (1, 1)\n",
      "\tpoint view (2, 1)\n",
      "\taddit typic (1, 1)\n",
      "\tderiv respect (1, 1)\n",
      "\tmodel general (3, 3)\n",
      "\ttext exploit (1, 1)\n",
      "\tlanguag model (1, 4)\n",
      "\tparamet model (2, 1)\n",
      "\tobject function (2, 1)\n",
      "\tnumber applic (1, 1)\n",
      "\titer algorithm (1, 1)\n",
      "\tjoin oper (1, 2)\n",
      "\tconstant repres (1, 1)\n",
      "\twork featur (1, 1)\n",
      "\tword leverag (1, 1)\n",
      "\texampl work (1, 3)\n",
      "\tword order (1, 1)\n",
      "\tunderstand problem (4, 1)\n",
      "\tgeneral framework (1, 1)\n",
      "\tnatur languag (48, 2)\n",
      "\tcontain multipl (1, 1)\n",
      "\tparamet estim (1, 3)\n",
      "\tmodel paramet (1, 2)\n",
      "\testim paramet (1, 1)\n",
      "\tset exampl (1, 1)\n",
      "\texampl use (1, 1)\n",
      "\tmodel use (1, 1)\n",
      "\tapproach survey (1, 1)\n",
      "\tresult new (1, 1)\n",
      "\timport rememb (1, 1)\n",
      "\thandl approach (1, 1)\n",
      "\tapproach use (1, 2)\n",
      "\tweight level (1, 1)\n",
      "\thigh probabl (3, 1)\n",
      "\tmultipl pass (1, 1)\n",
      "\tintroduc key (1, 1)\n",
      "\tdiscuss earli (1, 1)\n",
      "\thuman knowledg (1, 1)\n",
      "\tshow possibl (1, 1)\n",
      "\tmain challeng (1, 1)\n",
      "\tlearn model (1, 1)\n",
      "\tjoint model (1, 1)\n",
      "\tproblem natur (1, 1)\n",
      "\tgood general (1, 1)\n",
      "\ttask hand (1, 1)\n",
      "\tmodel natur (1, 1)\n",
      "\tcomput linguist (13, 2)\n",
      "\tassoci comput (13, 1)\n",
      "\tmachin learn (6, 2)\n",
      "\tintern confer (3, 1)\n",
      "\talgorithm larg (1, 1)\n",
      "\tcomput scienc (1, 1)\n",
      "\tstate art (3, 1)\n",
      "\ttechnic report (1, 1)\n",
      "\tstatist approach (2, 1)\n",
      "\thuman languag (1, 1)\n",
      "\tlanguag technolog (1, 1)\n",
      "\tneural network (2, 1)\n",
      "\tmodel natur languag (1, 1)\n",
      "\tassoci comput linguist (13, 1)\n",
      "\thuman languag technolog (1, 1)\n",
      "Doc1 has 5299 unique terms\n",
      "Doc2 has 13275 unique terms\n",
      "Total like terms: \t80\n",
      "Total unlike terms: \t18574\n",
      "Cosine Similarity: 0.0037890705531480793\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (48, 23)\n",
      "\tartifici intellig (5, 5)\n",
      "\tlanguag process (9, 17)\n",
      "\tmodel uncertainti (1, 2)\n",
      "\tmachin learn (6, 10)\n",
      "\tword appear (1, 1)\n",
      "\thard code (1, 2)\n",
      "\tmodel build (1, 1)\n",
      "\tchalleng scale (1, 1)\n",
      "\tlevel accuraci (1, 1)\n",
      "\tsystem train (1, 1)\n",
      "\ttrain datum (2, 1)\n",
      "\tdatum resourc (1, 1)\n",
      "\tappropri action (1, 1)\n",
      "\tcomput scienc (1, 1)\n",
      "\tcomput linguist (13, 1)\n",
      "\thuman languag (1, 1)\n",
      "\tunstructur text (1, 6)\n",
      "\ttask machin (1, 1)\n",
      "\tlarg collect (1, 1)\n",
      "\tmodel model (2, 1)\n",
      "\tallow improv (1, 1)\n",
      "\tneural network (2, 1)\n",
      "\tresult semant (1, 1)\n",
      "\tdeep understand (3, 1)\n",
      "\tresult base (1, 2)\n",
      "\texampl includ (1, 1)\n",
      "\tname entiti (2, 1)\n",
      "\tsimpl approach (1, 1)\n",
      "\tinterfac natur (1, 1)\n",
      "\tnatur languag process (9, 17)\n",
      "\tinterfac natur languag (1, 1)\n",
      "Doc1 has 5347 unique terms\n",
      "Doc2 has 4434 unique terms\n",
      "Total like terms: \t32\n",
      "Total unlike terms: \t9781\n",
      "Cosine Similarity: 0.05353784772126628\n",
      "\n",
      "LearningExecutableSemanticParsers.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (48, 45)\n",
      "\tcomput linguist (13, 26)\n",
      "\tlarg scale (2, 6)\n",
      "\tgenerat logic (2, 5)\n",
      "\tlogic form (55, 5)\n",
      "\tgenerat semant (1, 2)\n",
      "\tsemant pars (43, 10)\n",
      "\trule applic (1, 4)\n",
      "\tmethod natur (6, 7)\n",
      "\tinform system (1, 1)\n",
      "\tassum perfect (1, 1)\n",
      "\tinterpret natur (1, 1)\n",
      "\tmean represent (2, 2)\n",
      "\tshallow analysi (1, 1)\n",
      "\tanaphora resolut (1, 2)\n",
      "\tartifici intellig (5, 1)\n",
      "\tlanguag process (9, 20)\n",
      "\tsyntact pars (1, 4)\n",
      "\tquestion answer (13, 20)\n",
      "\tlinguist syntact (1, 1)\n",
      "\tstate art (3, 5)\n",
      "\tbase pars (1, 3)\n",
      "\tplay import (1, 1)\n",
      "\timport role (1, 1)\n",
      "\trepresent languag (2, 2)\n",
      "\tcomplex semant (1, 1)\n",
      "\tpresent semant (1, 2)\n",
      "\tsemant parser (20, 17)\n",
      "\tanswer system (2, 4)\n",
      "\tsyntax semant (1, 9)\n",
      "\texampl larg (1, 1)\n",
      "\tbroad coverag (3, 8)\n",
      "\tpars framework (4, 1)\n",
      "\tcontext free (1, 4)\n",
      "\tformal grammar (1, 9)\n",
      "\tassign probabl (3, 1)\n",
      "\tfree grammar (1, 2)\n",
      "\tgrammar common (1, 1)\n",
      "\tpars compon (1, 1)\n",
      "\ttext problem (1, 1)\n",
      "\tset exampl (1, 1)\n",
      "\testim model (1, 1)\n",
      "\tlanguag model (1, 2)\n",
      "\thuman languag (1, 1)\n",
      "\tempir method (6, 4)\n",
      "\tprocess mit (1, 3)\n",
      "\tmit press (1, 5)\n",
      "\tdataset obtain (1, 1)\n",
      "\tintern confer (3, 7)\n",
      "\tnorth american (2, 1)\n",
      "\tassoci comput (13, 9)\n",
      "\tstatist approach (2, 4)\n",
      "\trule base (1, 13)\n",
      "\tmodel exampl (1, 1)\n",
      "\tsystem base (1, 2)\n",
      "\tlarg corpora (1, 4)\n",
      "\tparser complex (1, 1)\n",
      "\tgrammar pars (1, 2)\n",
      "\tpars algorithm (1, 6)\n",
      "\tbeam search (3, 1)\n",
      "\tsemant represent (6, 2)\n",
      "\tgrammar grammar (1, 3)\n",
      "\tpredic argument (1, 4)\n",
      "\targument structur (1, 5)\n",
      "\tpars approach (1, 2)\n",
      "\tapproach use (1, 1)\n",
      "\tmodel use (1, 2)\n",
      "\tworld knowledg (1, 2)\n",
      "\trule use (1, 1)\n",
      "\tincreas span (1, 1)\n",
      "\tgrammar contain (1, 1)\n",
      "\talgorithm employ (1, 1)\n",
      "\trule allow (1, 3)\n",
      "\ttask hand (1, 1)\n",
      "\tstatist model (3, 3)\n",
      "\tmodel parser (3, 1)\n",
      "\tsemant natur (2, 2)\n",
      "\tchoic grammar (1, 1)\n",
      "\tlanguag understand (11, 6)\n",
      "\tparser appli (1, 1)\n",
      "\tpossibl improv (1, 2)\n",
      "\timprov model (1, 1)\n",
      "\tpars system (2, 2)\n",
      "\tlanguag inform (1, 2)\n",
      "\tintern joint (2, 1)\n",
      "\tjoint confer (2, 1)\n",
      "\tconfer natur (2, 1)\n",
      "\ttechnic report (1, 3)\n",
      "\treport technic (1, 1)\n",
      "\tcomput scienc (1, 1)\n",
      "\tmodel natur (1, 1)\n",
      "\tpars combinatori (1, 1)\n",
      "\tcombinatori categori (1, 1)\n",
      "\tcategori grammar (2, 1)\n",
      "\tanswer question (2, 2)\n",
      "\treduc pars (1, 1)\n",
      "\tlanguag logic (3, 1)\n",
      "\tdepend base (1, 1)\n",
      "\tbase method (1, 3)\n",
      "\tarticl present (1, 1)\n",
      "\till form (1, 2)\n",
      "\tlanguag grammar (2, 1)\n",
      "\tproduc set (1, 1)\n",
      "\tset depend (1, 1)\n",
      "\tdepend construct (1, 1)\n",
      "\tbuild new (1, 1)\n",
      "\tparser weak (1, 1)\n",
      "\tpars natur (1, 1)\n",
      "\tapplic exampl (1, 2)\n",
      "\tpoint view (2, 2)\n",
      "\tmain difficulti (1, 1)\n",
      "\trule rule (1, 3)\n",
      "\tlike use (1, 1)\n",
      "\trule give (1, 2)\n",
      "\tcomput semant (1, 2)\n",
      "\tfeatur base (2, 2)\n",
      "\tbase unif (1, 1)\n",
      "\thigh score (1, 2)\n",
      "\trule grammar (1, 2)\n",
      "\taccord produc (1, 1)\n",
      "\tgraph generat (1, 1)\n",
      "\trepres semant (3, 1)\n",
      "\tsemant altern (1, 1)\n",
      "\tinterfac databas (1, 1)\n",
      "\tacadem press (1, 4)\n",
      "\tordinari english (1, 1)\n",
      "\tbase semant (1, 5)\n",
      "\tlanguag utter (1, 4)\n",
      "\tutter semant (1, 1)\n",
      "\tmachin translat (2, 2)\n",
      "\tlarg collect (1, 2)\n",
      "\tfigur show (1, 6)\n",
      "\tutter repres (1, 1)\n",
      "\tsystem produc (1, 1)\n",
      "\tgrammar appli (1, 2)\n",
      "\tgive exampl (1, 1)\n",
      "\tgrammar handl (1, 1)\n",
      "\tfeatur encod (1, 3)\n",
      "\texampl lexic (1, 1)\n",
      "\tset rule (2, 5)\n",
      "\tword appear (1, 2)\n",
      "\tform semant (2, 1)\n",
      "\tlanguag technolog (1, 1)\n",
      "\twant express (1, 1)\n",
      "\tunderstand natur (4, 1)\n",
      "\tstructur natur (1, 1)\n",
      "\tstructur sentenc (1, 5)\n",
      "\tpars semant (1, 2)\n",
      "\tpars introduc (1, 1)\n",
      "\tpars exampl (1, 1)\n",
      "\tunderstand languag (1, 1)\n",
      "\tnoun phrase (1, 2)\n",
      "\tknowledg semant (1, 3)\n",
      "\tknowledg base (1, 2)\n",
      "\tdiffer semant (1, 7)\n",
      "\tshare common (1, 1)\n",
      "\trule map (1, 1)\n",
      "\tsyntact categori (1, 1)\n",
      "\tclass featur (1, 1)\n",
      "\tuse semant (1, 2)\n",
      "\talgorithm base (2, 1)\n",
      "\tbase featur (1, 3)\n",
      "\ttag syntact (1, 1)\n",
      "\tcategori exampl (1, 1)\n",
      "\tmatch exact (1, 1)\n",
      "\tunderstand system (1, 4)\n",
      "\tcontext user (1, 1)\n",
      "\tcontext semant (1, 1)\n",
      "\tbase statist (1, 1)\n",
      "\tlanguag construct (1, 1)\n",
      "\tconfer machin (1, 1)\n",
      "\tmachin learn (6, 2)\n",
      "\tlanguag learn (4, 3)\n",
      "\tlearn model (1, 2)\n",
      "\tmodel learn (1, 1)\n",
      "\tresult semant (1, 2)\n",
      "\tform loop (1, 1)\n",
      "\tlearner provid (1, 1)\n",
      "\tpossibl learn (1, 1)\n",
      "\tgenerat set (1, 1)\n",
      "\tspan utter (2, 1)\n",
      "\tgrammar rule (4, 1)\n",
      "\tform pair (3, 1)\n",
      "\tshow exampl (1, 1)\n",
      "\tmodel simpl (1, 1)\n",
      "\tgeneral languag (1, 1)\n",
      "\tform construct (1, 1)\n",
      "\tuse machin (1, 1)\n",
      "\tparser extend (1, 1)\n",
      "\tinput utter (4, 2)\n",
      "\ttype constraint (3, 1)\n",
      "\tconsist utter (1, 1)\n",
      "\tsystem assum (1, 1)\n",
      "\tset result (1, 3)\n",
      "\tkey insight (1, 1)\n",
      "\texampl semant (1, 1)\n",
      "\tgive utter (1, 1)\n",
      "\tdescrib logic (1, 1)\n",
      "\texampl use (1, 1)\n",
      "\tlearn set (1, 1)\n",
      "\tsimpl approach (1, 1)\n",
      "\tlanguag semant (2, 1)\n",
      "\tsemant linguist (1, 1)\n",
      "\tscore function (2, 1)\n",
      "\tderiv exampl (2, 1)\n",
      "\tstatist learn (1, 1)\n",
      "\tmodel build (1, 1)\n",
      "\tapproach learn (1, 1)\n",
      "\tlearn algorithm (1, 2)\n",
      "\tend end (3, 1)\n",
      "\tpossibl rule (1, 1)\n",
      "\trule exampl (1, 1)\n",
      "\tnumber time (2, 1)\n",
      "\ttest time (1, 1)\n",
      "\trule produc (1, 2)\n",
      "\trule increas (1, 1)\n",
      "\tnumber word (1, 3)\n",
      "\torder magnitud (1, 2)\n",
      "\trule build (1, 1)\n",
      "\tlinguist annot (2, 1)\n",
      "\tdefin featur (4, 1)\n",
      "\tproperti allow (1, 1)\n",
      "\ttask requir (2, 1)\n",
      "\tvector defin (1, 1)\n",
      "\tformal semant (2, 1)\n",
      "\torder logic (4, 1)\n",
      "\thuman knowledg (1, 1)\n",
      "\trequir lot (1, 2)\n",
      "\tcomput complex (1, 1)\n",
      "\tstructur defin (1, 1)\n",
      "\thigh order (2, 1)\n",
      "\tcreat new (1, 1)\n",
      "\tform simpl (3, 1)\n",
      "\tway context (1, 1)\n",
      "\tgenerat question (1, 1)\n",
      "\tname entiti (2, 1)\n",
      "\tdeep understand (3, 2)\n",
      "\tquestion method (1, 1)\n",
      "\tlet denot (1, 3)\n",
      "\tdenot set (3, 2)\n",
      "\ttabl answer (1, 1)\n",
      "\tsecond interest (1, 1)\n",
      "\tprocess brill (1, 1)\n",
      "\tcurrent state (1, 2)\n",
      "\tset allow (1, 1)\n",
      "\tform captur (1, 1)\n",
      "\tweight prune (1, 1)\n",
      "\treport result (1, 1)\n",
      "\tsynonym number (1, 1)\n",
      "\tgenerat logic form (2, 4)\n",
      "\tmethod natur languag (6, 5)\n",
      "\tinterpret natur languag (1, 1)\n",
      "\tnatur languag process (9, 13)\n",
      "\tplay import role (1, 1)\n",
      "\tquestion answer system (1, 3)\n",
      "\tcontext free grammar (1, 1)\n",
      "\tempir method natur (6, 4)\n",
      "\tprocess mit press (1, 3)\n",
      "\tassoci comput linguist (13, 8)\n",
      "\tpredic argument structur (1, 1)\n",
      "\tnatur languag understand (7, 2)\n",
      "\tintern joint confer (2, 1)\n",
      "\tjoint confer natur (2, 1)\n",
      "\tconfer natur languag (2, 1)\n",
      "\treport technic report (1, 1)\n",
      "\tmodel natur languag (1, 1)\n",
      "\tpars combinatori categori (1, 1)\n",
      "\tcombinatori categori grammar (1, 1)\n",
      "\tpars natur languag (1, 1)\n",
      "\tsemant natur languag (2, 1)\n",
      "\tnatur languag utter (1, 3)\n",
      "\trule grammar appli (1, 1)\n",
      "\tsemant pars algorithm (1, 2)\n",
      "\tunderstand natur languag (4, 1)\n",
      "\tstructur natur languag (1, 1)\n",
      "\tsemant pars introduc (1, 1)\n",
      "\tconfer machin learn (1, 1)\n",
      "\tlanguag understand system (1, 2)\n",
      "\tnatur languag semant (1, 1)\n",
      "\tnatur languag learn (4, 1)\n",
      "\tcurrent state art (1, 2)\n",
      "Doc1 has 5098 unique terms\n",
      "Doc2 has 37712 unique terms\n",
      "Total like terms: \t281\n",
      "Total unlike terms: \t42810\n",
      "Cosine Similarity: 0.049945961123061856\n",
      "\n",
      "p123-zhang.txt and Partial Parsing Finite-State Cascades.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 2)\n",
      "\texhaust search (1, 2)\n",
      "\trepres number (1, 1)\n",
      "\tunlik tradit (2, 1)\n",
      "\tparticular case (1, 1)\n",
      "\tinform retriev (2, 1)\n",
      "\tparticular applic (1, 1)\n",
      "\tproceed confer (1, 1)\n",
      "Doc1 has 3254 unique terms\n",
      "Doc2 has 2293 unique terms\n",
      "Total like terms: \t8\n",
      "Total unlike terms: \t5547\n",
      "Cosine Similarity: 0.0002899665702926184\n",
      "\n",
      "p123-zhang.txt and QueryEffectiveness.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tinform retriev (2, 14)\n",
      "\tnatur languag (1, 2)\n",
      "\tmeasur tabl (1, 4)\n",
      "\tshow tabl (1, 2)\n",
      "\tresult show (1, 1)\n",
      "\tlanguag process (1, 1)\n",
      "\tuser interfac (2, 1)\n",
      "\tsearch result (1, 1)\n",
      "\tproceed intern (1, 3)\n",
      "\tintern confer (2, 3)\n",
      "\tconfer develop (1, 1)\n",
      "\tdevelop inform (1, 3)\n",
      "\tevalu system (1, 1)\n",
      "\tconfer research (1, 2)\n",
      "\tresult show tabl (1, 1)\n",
      "\tnatur languag process (1, 1)\n",
      "\tproceed intern confer (1, 3)\n",
      "\tintern confer develop (1, 1)\n",
      "\tconfer develop inform (1, 1)\n",
      "\tdevelop inform retriev (1, 3)\n",
      "Doc1 has 3242 unique terms\n",
      "Doc2 has 1831 unique terms\n",
      "Total like terms: \t20\n",
      "Total unlike terms: \t5073\n",
      "Cosine Similarity: 0.006442704166193242\n",
      "\n",
      "p123-zhang.txt and Text Clustering Algorithms.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like terms (doc1, doc2):\n",
      "\tsocial network (5, 8)\n",
      "\ttext measur (1, 1)\n",
      "\tuse text (1, 1)\n",
      "\ttopic model (1, 26)\n",
      "\tdiscuss text (1, 2)\n",
      "\tnetwork base (1, 3)\n",
      "\tfollow section (1, 1)\n",
      "\tdescrib section (1, 1)\n",
      "\tsection describ (2, 1)\n",
      "\tbase network (1, 1)\n",
      "\tsection discuss (1, 3)\n",
      "\tcase term (1, 2)\n",
      "\tdocument text (1, 3)\n",
      "\tprevious case (2, 1)\n",
      "\tuse semant (1, 1)\n",
      "\tinform retriev (2, 12)\n",
      "\tcase vector (1, 1)\n",
      "\tdocument corpus (1, 6)\n",
      "\tfunction measur (1, 2)\n",
      "\tweight term (1, 4)\n",
      "\tterm vector (1, 1)\n",
      "\tsimilar measur (1, 2)\n",
      "\tsearch engin (2, 1)\n",
      "\tmeasur similar (1, 2)\n",
      "\tsearch process (1, 1)\n",
      "\thigh similar (1, 1)\n",
      "\tcase document (4, 4)\n",
      "\trun time (2, 5)\n",
      "\tdocument contain (1, 5)\n",
      "\texampl method (1, 1)\n",
      "\tparticular applic (1, 1)\n",
      "\tcase start (1, 1)\n",
      "\tdocument choos (1, 1)\n",
      "\tdiscuss use (1, 4)\n",
      "\tnatur languag (1, 2)\n",
      "\tterm occur (1, 2)\n",
      "\tpoint time (1, 1)\n",
      "\twork model (1, 1)\n",
      "\tsimilar term (1, 1)\n",
      "\tnode link (1, 1)\n",
      "\tkind link (1, 1)\n",
      "\tlink base (1, 3)\n",
      "\ttext associ (1, 1)\n",
      "\tmulti dimension (3, 1)\n",
      "\tlink propos (1, 1)\n",
      "\tform natur (1, 1)\n",
      "\tprocess partial (1, 1)\n",
      "\tdiscuss idea (1, 1)\n",
      "\tproceed tenth (4, 1)\n",
      "\tmachin learn (1, 2)\n",
      "\tintern confer (2, 1)\n",
      "\tapproach retriev (1, 1)\n",
      "\tjournal inform (1, 1)\n",
      "\tinform scienc (1, 1)\n",
      "\tmethod larg (1, 1)\n",
      "\tweight term vector (1, 1)\n",
      "\tfunction measur similar (1, 1)\n",
      "\tjournal inform scienc (1, 1)\n",
      "Doc1 has 3204 unique terms\n",
      "Doc2 has 13297 unique terms\n",
      "Total like terms: \t58\n",
      "Total unlike terms: \t16501\n",
      "Cosine Similarity: 0.0023960797776378688\n",
      "\n",
      "p123-zhang.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 23)\n",
      "\tlanguag process (1, 17)\n",
      "\tmachin learn (1, 10)\n",
      "\ttime data (1, 1)\n",
      "\thigh relev (1, 2)\n",
      "\tsearch result (1, 4)\n",
      "\tsocial medium (1, 5)\n",
      "\tdevelop centuri (1, 1)\n",
      "\tuser interfac (2, 4)\n",
      "\tdecis make (1, 2)\n",
      "\tkey term (1, 1)\n",
      "\tread exampl (1, 1)\n",
      "\tsearch interfac (1, 1)\n",
      "\tuser search (1, 4)\n",
      "\tdocument contain (1, 1)\n",
      "\tbase user (1, 1)\n",
      "\tdocument search (2, 1)\n",
      "\tsearch process (1, 1)\n",
      "\tnatur languag process (1, 17)\n",
      "Doc1 has 3243 unique terms\n",
      "Doc2 has 4447 unique terms\n",
      "Total like terms: \t19\n",
      "Total unlike terms: \t7690\n",
      "Cosine Similarity: 0.007510843149733369\n",
      "\n",
      "p123-zhang.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (1, 45)\n",
      "\tinform retriev (2, 7)\n",
      "\tlanguag process (1, 20)\n",
      "\tevalu system (1, 2)\n",
      "\tsystem propos (1, 1)\n",
      "\tpresent semant (1, 2)\n",
      "\tsystem method (1, 1)\n",
      "\tprocess model (1, 1)\n",
      "\tproceed confer (1, 9)\n",
      "\tproceed annual (1, 4)\n",
      "\tproceed intern (1, 6)\n",
      "\tintern confer (2, 7)\n",
      "\tequal import (1, 1)\n",
      "\torder search (1, 1)\n",
      "\textract lexic (1, 1)\n",
      "\tversion system (1, 1)\n",
      "\tworld wide (1, 1)\n",
      "\twide web (1, 1)\n",
      "\tshow tabl (1, 8)\n",
      "\tsystem work (1, 2)\n",
      "\tsystem build (1, 1)\n",
      "\tfollow exampl (1, 3)\n",
      "\tsemant base (6, 1)\n",
      "\tsystem allow (2, 1)\n",
      "\trun time (2, 1)\n",
      "\tcurrent version (2, 1)\n",
      "\trelat exist (1, 1)\n",
      "\trole play (1, 3)\n",
      "\tfigur show (1, 6)\n",
      "\tsemant link (3, 2)\n",
      "\trelat case (2, 3)\n",
      "\tcase relat (1, 2)\n",
      "\tcertain case (1, 1)\n",
      "\tdescrib semant (2, 1)\n",
      "\tknowledg base (1, 2)\n",
      "\tsection describ (2, 9)\n",
      "\tlist import (1, 1)\n",
      "\trelat link (1, 2)\n",
      "\tuse semant (1, 2)\n",
      "\tsemant network (2, 8)\n",
      "\tidentifi possibl (1, 1)\n",
      "\trandom select (1, 1)\n",
      "\tevalu conduct (1, 1)\n",
      "\tinterfac allow (1, 1)\n",
      "\tcontext semant (1, 1)\n",
      "\tmachin learn (1, 2)\n",
      "\tsuggest high (1, 1)\n",
      "\tfollow section (1, 4)\n",
      "\tsystem section (1, 1)\n",
      "\tconclud system (1, 1)\n",
      "\tshow exampl (1, 1)\n",
      "\tlink give (1, 1)\n",
      "\tconstruct semant (1, 1)\n",
      "\tconstruct proceed (1, 1)\n",
      "\tcase let (1, 1)\n",
      "\tderiv word (1, 1)\n",
      "\tsection discuss (1, 3)\n",
      "\tcase rule (1, 1)\n",
      "\tcase treat (1, 1)\n",
      "\tresult show (1, 4)\n",
      "\tcase high (1, 1)\n",
      "\tnode link (1, 1)\n",
      "\tstart network (1, 1)\n",
      "\tnumber link (1, 2)\n",
      "\tnetwork system (2, 1)\n",
      "\tcase consid (1, 1)\n",
      "\texampl network (1, 1)\n",
      "\tfigur network (7, 1)\n",
      "\tsection paper (1, 1)\n",
      "\tdescrib section (1, 2)\n",
      "\tsearch engin (2, 8)\n",
      "\trecent research (1, 1)\n",
      "\tconclus futur (1, 2)\n",
      "\tforward backward (1, 2)\n",
      "\tqueri keyword (1, 1)\n",
      "\tsearch result (1, 1)\n",
      "\tresearch paper (1, 1)\n",
      "\tmodel describ (2, 1)\n",
      "\tsimilar measur (1, 3)\n",
      "\tmeasur similar (1, 1)\n",
      "\tdocument corpus (1, 1)\n",
      "\tdocument contain (1, 1)\n",
      "\tpoint interest (1, 1)\n",
      "\tannual intern (1, 1)\n",
      "\tweb search (2, 1)\n",
      "\tnatur languag process (1, 13)\n",
      "\tproceed intern confer (1, 4)\n",
      "\tworld wide web (1, 1)\n",
      "\tfollow section describ (1, 2)\n",
      "\tfigur show exampl (1, 1)\n",
      "\tresult show tabl (1, 1)\n",
      "\tannual intern confer (1, 1)\n",
      "Doc1 has 3170 unique terms\n",
      "Doc2 has 37901 unique terms\n",
      "Total like terms: \t92\n",
      "Total unlike terms: \t41071\n",
      "Cosine Similarity: 0.004721095602586679\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and QueryEffectiveness.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tinform retriev (1, 14)\n",
      "\tnatur languag (2, 2)\n",
      "\torder magnitud (1, 1)\n",
      "\taddress question (1, 1)\n",
      "\tword second (1, 1)\n",
      "Doc1 has 2296 unique terms\n",
      "Doc2 has 1846 unique terms\n",
      "Total like terms: \t5\n",
      "Total unlike terms: \t4142\n",
      "Cosine Similarity: 0.0005483364566385071\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and Text Clustering Algorithms.txt\n",
      "Like terms (doc1, doc2):\n",
      "\timprov accuraci (1, 2)\n",
      "\trandom sampl (1, 1)\n",
      "\tcorpus contain (1, 2)\n",
      "\texampl case (1, 1)\n",
      "\tinform retriev (1, 12)\n",
      "\ttechniqu base (1, 1)\n",
      "\thigh level (2, 2)\n",
      "\tparticular applic (1, 1)\n",
      "\tlow level (1, 1)\n",
      "\tpropos use (1, 1)\n",
      "\ttechniqu appli (1, 2)\n",
      "\tword second (1, 1)\n",
      "\trandom variabl (1, 8)\n",
      "\tnatur languag (2, 2)\n",
      "\tvariabl valu (1, 1)\n",
      "\tword distribut (1, 1)\n",
      "\tmaximum likelihood (1, 4)\n",
      "\tlikelihood estim (1, 1)\n",
      "\treal time (1, 1)\n",
      "\tword stream (1, 1)\n",
      "\tcomput linguist (1, 2)\n",
      "\ttree sequenc (1, 1)\n",
      "\trandom variabl valu (1, 1)\n",
      "Doc1 has 2278 unique terms\n",
      "Doc2 has 13332 unique terms\n",
      "Total like terms: \t23\n",
      "Total unlike terms: \t15610\n",
      "Cosine Similarity: 0.0008275372820241479\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (2, 23)\n",
      "\thigh level (2, 2)\n",
      "\tcomput linguist (1, 1)\n",
      "\tfast accur (1, 1)\n",
      "\treal time (1, 1)\n",
      "\tkey concept (1, 1)\n",
      "Doc1 has 2295 unique terms\n",
      "Doc2 has 4460 unique terms\n",
      "Total like terms: \t6\n",
      "Total unlike terms: \t6755\n",
      "Cosine Similarity: 0.0009894874456592418\n",
      "\n",
      "Partial Parsing Finite-State Cascades.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (2, 45)\n",
      "\tcomput linguist (1, 26)\n",
      "\tparser test (1, 4)\n",
      "\tpartial pars (3, 8)\n",
      "\tevalu parser (2, 6)\n",
      "\tlanguag communic (1, 1)\n",
      "\tinform retriev (1, 7)\n",
      "\tsyntact analysi (2, 4)\n",
      "\tpars pars (1, 3)\n",
      "\tbase pars (1, 3)\n",
      "\tfinit state (9, 4)\n",
      "\tfree text (1, 1)\n",
      "\tbroad coverag (1, 8)\n",
      "\tparser certain (1, 1)\n",
      "\tgarden path (2, 5)\n",
      "\tpath sentenc (1, 2)\n",
      "\tmaximum likelihood (1, 2)\n",
      "\tlikelihood estim (1, 2)\n",
      "\tcontext free (2, 4)\n",
      "\trule probabl (1, 1)\n",
      "\tunrestrict text (2, 3)\n",
      "\tproceed confer (1, 9)\n",
      "\tlow level (1, 6)\n",
      "\tincrement pars (1, 1)\n",
      "\tlexic depend (1, 2)\n",
      "\tpars proceed (1, 5)\n",
      "\tspeed robust (1, 1)\n",
      "\tlarg corpora (1, 4)\n",
      "\tgrammar grammar (1, 3)\n",
      "\targument structur (1, 5)\n",
      "\tstate long (1, 1)\n",
      "\tfunction grammar (1, 4)\n",
      "\tgrammar size (1, 1)\n",
      "\tnoun noun (1, 2)\n",
      "\ttrain corpus (1, 3)\n",
      "\thigh level (2, 8)\n",
      "\tfast robust (1, 1)\n",
      "\tsyntact structur (1, 5)\n",
      "\tpars chunk (1, 2)\n",
      "\tpattern pattern (1, 1)\n",
      "\tstructur pattern (1, 1)\n",
      "\tprecis recal (1, 6)\n",
      "\tparser perform (2, 1)\n",
      "\tchoos task (1, 1)\n",
      "\tlinguist structur (1, 4)\n",
      "\tgenerat languag (1, 1)\n",
      "\tcorpus contain (1, 1)\n",
      "\tword percentag (1, 1)\n",
      "\tchunk corpus (1, 1)\n",
      "\tlanguag engin (1, 2)\n",
      "\tproceed workshop (1, 2)\n",
      "\tacadem publish (1, 2)\n",
      "\tcategori grammar (1, 1)\n",
      "\tparser english (1, 1)\n",
      "\tproc workshop (1, 1)\n",
      "\tcoverag parser (1, 2)\n",
      "\tmeasur accuraci (1, 1)\n",
      "\tword exampl (1, 1)\n",
      "\tparser word (1, 1)\n",
      "\tparser tag (1, 1)\n",
      "\tchunk word (1, 1)\n",
      "\tstate pars (1, 1)\n",
      "\tworkshop larg (1, 2)\n",
      "\tpartial parser (1, 3)\n",
      "\tparser rule (1, 1)\n",
      "\tlevel parser (1, 1)\n",
      "\tlevel special (1, 1)\n",
      "\tlevel level (1, 1)\n",
      "\tresult parser (1, 2)\n",
      "\tcase correct (1, 3)\n",
      "\tstructur like (1, 1)\n",
      "\tword level (1, 4)\n",
      "\tlevel pars (1, 1)\n",
      "\tparser corpus (1, 1)\n",
      "\tsyntact rule (1, 3)\n",
      "\tword second (1, 1)\n",
      "\tpropos use (1, 2)\n",
      "\tpreliminari evalu (1, 1)\n",
      "\tperform task (1, 1)\n",
      "\tfeatur output (1, 1)\n",
      "\timprov parser (1, 1)\n",
      "\tparser describ (2, 1)\n",
      "\tpars tree (9, 3)\n",
      "\tnoun phrase (1, 2)\n",
      "\tsyntact featur (1, 12)\n",
      "\tstate syntact (1, 1)\n",
      "\tfeatur set (1, 1)\n",
      "\tfeatur assign (2, 2)\n",
      "\tcomput describ (1, 1)\n",
      "\tprogram languag (1, 1)\n",
      "\tphrase phrase (1, 1)\n",
      "\tlike noun (1, 1)\n",
      "\tmatch rule (1, 6)\n",
      "\tphrase begin (1, 1)\n",
      "\tparser attribut (1, 1)\n",
      "\tstate recogn (1, 1)\n",
      "\tlevel recogn (1, 1)\n",
      "\tfeatur structur (2, 1)\n",
      "\tfeatur combin (1, 1)\n",
      "\ttag corpus (1, 1)\n",
      "\torder magnitud (1, 2)\n",
      "\tbase evalu (1, 1)\n",
      "\ttechniqu base (1, 1)\n",
      "\tcambridg univers (1, 1)\n",
      "\tvalid base (1, 4)\n",
      "\tcorrect answer (1, 8)\n",
      "\tperform linguist (1, 1)\n",
      "\tsecond estim (1, 1)\n",
      "\twork describ (1, 1)\n",
      "\tapproach take (1, 1)\n",
      "\tmean current (1, 1)\n",
      "\tpercentag correct (1, 1)\n",
      "\tpossibl valu (1, 1)\n",
      "\ttest datum (1, 2)\n",
      "\tset possibl (1, 1)\n",
      "\tnatur languag communic (1, 1)\n",
      "\tgarden path sentenc (1, 2)\n",
      "\tmaximum likelihood estim (1, 2)\n",
      "\tnatur languag engin (1, 1)\n",
      "\tbroad coverag parser (1, 1)\n",
      "\tfinit state pars (1, 1)\n",
      "\tproceed workshop larg (1, 1)\n",
      "\tworkshop larg corpora (1, 2)\n",
      "Doc1 has 2178 unique terms\n",
      "Doc2 has 37870 unique terms\n",
      "Total like terms: \t123\n",
      "Total unlike terms: \t40048\n",
      "Cosine Similarity: 0.0050980168019175\n",
      "\n",
      "QueryEffectiveness.txt and Text Clustering Algorithms.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like terms (doc1, doc2):\n",
      "\tstudi datum (2, 1)\n",
      "\timprov retriev (3, 1)\n",
      "\treduct topic (1, 1)\n",
      "\tcontain descript (1, 1)\n",
      "\tword word (1, 3)\n",
      "\tdata set (1, 6)\n",
      "\tset construct (1, 1)\n",
      "\tinform retriev (14, 12)\n",
      "\tobserv suggest (1, 1)\n",
      "\thypothesi hypothesi (2, 1)\n",
      "\ttopic topic (1, 1)\n",
      "\tword second (1, 1)\n",
      "\tword associ (1, 1)\n",
      "\tnatur languag (2, 2)\n",
      "\tassoci document (1, 1)\n",
      "\tgoal topic (1, 1)\n",
      "\ttopic size (2, 1)\n",
      "\ttask perform (2, 2)\n",
      "\ttime depend (1, 1)\n",
      "\tcorrel analysi (2, 1)\n",
      "\tgenerat topic (1, 1)\n",
      "\texperiment result (1, 2)\n",
      "\tintern confer (3, 1)\n",
      "\tretriev document (2, 1)\n",
      "\tcomput scienc (2, 1)\n",
      "\ttext retriev (3, 1)\n",
      "\tunivers ithaca (2, 1)\n",
      "Doc1 has 1824 unique terms\n",
      "Doc2 has 13328 unique terms\n",
      "Total like terms: \t27\n",
      "Total unlike terms: \t15152\n",
      "Cosine Similarity: 0.002036330979646199\n",
      "\n",
      "QueryEffectiveness.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (2, 23)\n",
      "\tlanguag process (1, 17)\n",
      "\tsearch result (1, 4)\n",
      "\teffect primari (1, 1)\n",
      "\tcomput scienc (2, 1)\n",
      "\tuser interfac (1, 4)\n",
      "\tdesign evalu (1, 1)\n",
      "\thelp improv (2, 2)\n",
      "\tlanguag queri (1, 1)\n",
      "\tsystem differ (1, 1)\n",
      "\trelev inform (1, 2)\n",
      "\tprecis result (1, 1)\n",
      "\tnatur languag process (1, 17)\n",
      "\tnatur languag queri (1, 1)\n",
      "Doc1 has 1837 unique terms\n",
      "Doc2 has 4452 unique terms\n",
      "Total like terms: \t14\n",
      "Total unlike terms: \t6289\n",
      "Cosine Similarity: 0.008498081941411155\n",
      "\n",
      "QueryEffectiveness.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (2, 45)\n",
      "\tinform retriev (14, 7)\n",
      "\tdiffer degre (1, 1)\n",
      "\tlanguag process (1, 20)\n",
      "\tevalu system (1, 2)\n",
      "\tassoci document (1, 1)\n",
      "\tlike thank (1, 1)\n",
      "\tsentenc process (1, 6)\n",
      "\tword word (1, 6)\n",
      "\tanalysi conduct (1, 1)\n",
      "\tsecond point (1, 2)\n",
      "\tproceed intern (3, 6)\n",
      "\tintern confer (3, 7)\n",
      "\tsystem base (1, 2)\n",
      "\tshow tabl (2, 8)\n",
      "\ttest hypothesi (1, 1)\n",
      "\ttask perform (2, 1)\n",
      "\tcomput scienc (2, 1)\n",
      "\tanalysi result (1, 1)\n",
      "\tmeasur measur (1, 1)\n",
      "\tresult word (1, 2)\n",
      "\tsystem inform (2, 1)\n",
      "\tnew york (2, 6)\n",
      "\trelat word (1, 6)\n",
      "\tprocess strategi (1, 1)\n",
      "\tword second (1, 1)\n",
      "\taverag length (2, 1)\n",
      "\tperform result (1, 1)\n",
      "\tqueri queri (2, 1)\n",
      "\tindic differ (1, 1)\n",
      "\trank accord (1, 1)\n",
      "\tresult number (1, 4)\n",
      "\texperi perform (1, 1)\n",
      "\tresult show (1, 4)\n",
      "\tperform consist (1, 1)\n",
      "\texperi set (1, 2)\n",
      "\tprecis result (1, 1)\n",
      "\trespect precis (1, 1)\n",
      "\tprecis measur (2, 2)\n",
      "\torder magnitud (1, 2)\n",
      "\ttabl precis (1, 1)\n",
      "\tperform approxim (1, 1)\n",
      "\tprocess larg (1, 1)\n",
      "\tdesign evalu (1, 1)\n",
      "\texperiment result (1, 1)\n",
      "\tsystem develop (1, 2)\n",
      "\tdocument document (1, 2)\n",
      "\tcomput time (1, 2)\n",
      "\treport degre (1, 1)\n",
      "\tperform system (1, 1)\n",
      "\tsystem select (1, 1)\n",
      "\tretriev queri (1, 1)\n",
      "\tretriev document (2, 1)\n",
      "\tdocument answer (1, 1)\n",
      "\tmeasur rate (1, 2)\n",
      "\tretriev system (12, 1)\n",
      "\tsearch result (1, 1)\n",
      "\thelp improv (2, 1)\n",
      "\tdescrib impact (1, 1)\n",
      "\tdata set (1, 1)\n",
      "\texperi repeat (1, 1)\n",
      "\tmeasur recal (1, 1)\n",
      "\tdocument respect (1, 1)\n",
      "\tresult result (1, 1)\n",
      "\thigh valu (1, 1)\n",
      "\tindic valu (1, 1)\n",
      "\taltern manual (1, 1)\n",
      "\tautomat index (2, 1)\n",
      "\tnatur languag process (1, 13)\n",
      "\tproceed intern confer (3, 4)\n",
      "\tresult show tabl (1, 1)\n",
      "\tretriev document answer (1, 1)\n",
      "\tinform retriev system (5, 1)\n",
      "Doc1 has 1778 unique terms\n",
      "Doc2 has 37920 unique terms\n",
      "Total like terms: \t73\n",
      "Total unlike terms: \t39698\n",
      "Cosine Similarity: 0.006908619507647084\n",
      "\n",
      "Text Clustering Algorithms.txt and Text-Analytics-and-Natural-Language-Processing--.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (2, 23)\n",
      "\ttext base (2, 2)\n",
      "\tmachin learn (2, 10)\n",
      "\tproblem good (1, 2)\n",
      "\ttext datum (17, 3)\n",
      "\tdatum mine (3, 2)\n",
      "\tcomput comput (2, 1)\n",
      "\tgenerat model (2, 2)\n",
      "\tmodel predict (2, 6)\n",
      "\tlarg data (1, 2)\n",
      "\tmodel requir (1, 1)\n",
      "\tevolut text (1, 1)\n",
      "\tprocess new (1, 1)\n",
      "\tqualiti datum (1, 1)\n",
      "\tstart point (1, 1)\n",
      "\thigh level (2, 2)\n",
      "\tapproach simpl (1, 1)\n",
      "\tmain approach (1, 1)\n",
      "\ttrain datum (3, 1)\n",
      "\tdatum essenti (1, 1)\n",
      "\tcomput scienc (1, 1)\n",
      "\tcomput linguist (2, 1)\n",
      "\thuman languag (1, 1)\n",
      "\treal time (1, 1)\n",
      "\talgorithm techniqu (1, 1)\n",
      "\trequir need (1, 1)\n",
      "\timprov qualiti (3, 1)\n",
      "\tdatum point (3, 1)\n",
      "\tlarg collect (2, 1)\n",
      "\texampl similar (1, 1)\n",
      "\tdocument train (1, 1)\n",
      "\tdatum includ (1, 1)\n",
      "\tuse approxim (1, 1)\n",
      "\tcelesti bodi (3, 1)\n",
      "\tsum squar (1, 1)\n",
      "\tneural network (1, 1)\n",
      "\tdocument contain (5, 1)\n",
      "\tlarg amount (1, 1)\n",
      "\tbase inform (2, 1)\n",
      "\tterm datum (1, 1)\n",
      "\tsearch process (1, 1)\n",
      "\tprocess provid (1, 1)\n",
      "Doc1 has 13313 unique terms\n",
      "Doc2 has 4424 unique terms\n",
      "Total like terms: \t42\n",
      "Total unlike terms: \t17737\n",
      "Cosine Similarity: 0.0012867609513929862\n",
      "\n",
      "Text Clustering Algorithms.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n",
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (2, 45)\n",
      "\tcomput linguist (2, 26)\n",
      "\tkeyword associ (1, 14)\n",
      "\tbase text (3, 5)\n",
      "\ttext document (9, 7)\n",
      "\tdocument cluster (54, 15)\n",
      "\trobust model (1, 3)\n",
      "\ttechniqu propos (1, 1)\n",
      "\tform mean (1, 1)\n",
      "\tinform retriev (12, 7)\n",
      "\tkind datum (1, 1)\n",
      "\thybrid approach (1, 2)\n",
      "\tcognit scienc (1, 8)\n",
      "\ttopic relat (1, 1)\n",
      "\tmethod semant (1, 1)\n",
      "\trecent advanc (2, 3)\n",
      "\tstate art (1, 5)\n",
      "\tdirect use (1, 1)\n",
      "\tmethod featur (1, 1)\n",
      "\tnetwork text (2, 1)\n",
      "\tweb document (2, 2)\n",
      "\tdocument propos (2, 1)\n",
      "\tassoci document (1, 1)\n",
      "\tbrows document (1, 1)\n",
      "\tdocument represent (8, 1)\n",
      "\twide varieti (4, 1)\n",
      "\tmodel base (1, 5)\n",
      "\tbase techniqu (1, 1)\n",
      "\tmaximum likelihood (4, 2)\n",
      "\tlikelihood estim (1, 2)\n",
      "\tmodel probabilist (2, 1)\n",
      "\tlarg number (6, 5)\n",
      "\tmodel propos (2, 3)\n",
      "\tparticular interest (1, 1)\n",
      "\tpropos model (1, 1)\n",
      "\ttext problem (2, 1)\n",
      "\tmodel increment (1, 1)\n",
      "\tword word (3, 6)\n",
      "\tmodel discuss (2, 1)\n",
      "\tperform model (1, 4)\n",
      "\tsmall set (1, 1)\n",
      "\tset exampl (1, 1)\n",
      "\tset correspond (1, 1)\n",
      "\tlanguag model (4, 2)\n",
      "\tbase gram (1, 2)\n",
      "\tmodel predict (2, 2)\n",
      "\ttime earli (1, 1)\n",
      "\tinform word (4, 6)\n",
      "\tphrasal inform (1, 1)\n",
      "\thuman languag (1, 1)\n",
      "\tuse inform (2, 2)\n",
      "\tprobabilist model (3, 2)\n",
      "\tconfer empir (1, 4)\n",
      "\tlow level (1, 6)\n",
      "\tintern confer (1, 7)\n",
      "\tchapter associ (1, 2)\n",
      "\tassoci comput (1, 9)\n",
      "\tannual meet (1, 3)\n",
      "\tmeet associ (1, 4)\n",
      "\tgenerat model (2, 4)\n",
      "\tconfer european (1, 1)\n",
      "\teuropean chapter (1, 1)\n",
      "\tstatist approach (1, 4)\n",
      "\tbase approach (4, 8)\n",
      "\trelat relat (1, 3)\n",
      "\tapproach extract (1, 1)\n",
      "\tsearch space (1, 1)\n",
      "\tsolut problem (2, 2)\n",
      "\torgan follow (1, 3)\n",
      "\tapproach use (2, 1)\n",
      "\tmodel use (1, 2)\n",
      "\ttext applic (1, 1)\n",
      "\ttext datum (17, 1)\n",
      "\tsemant analysi (2, 15)\n",
      "\tgreat improv (1, 1)\n",
      "\tapproach show (1, 1)\n",
      "\texampl general (1, 1)\n",
      "\thigh level (2, 8)\n",
      "\ttask hand (1, 1)\n",
      "\trelat use (1, 2)\n",
      "\tnode tree (3, 1)\n",
      "\tspars datum (1, 1)\n",
      "\tprobabl model (2, 1)\n",
      "\tuse number (2, 3)\n",
      "\tnumber model (2, 1)\n",
      "\ttotal number (2, 3)\n",
      "\tmodel probabl (3, 1)\n",
      "\tsuggest larg (1, 1)\n",
      "\timport structur (1, 1)\n",
      "\tmain advantag (2, 2)\n",
      "\tline research (1, 1)\n",
      "\tdocument depend (1, 1)\n",
      "\tgeneral purpos (1, 3)\n",
      "\tcorpus contain (2, 1)\n",
      "\tsimilar larg (1, 1)\n",
      "\torder perform (7, 1)\n",
      "\tphrase extract (2, 1)\n",
      "\ttask perform (2, 1)\n",
      "\tgeneral text (1, 1)\n",
      "\ttext proceed (1, 1)\n",
      "\ttechnic report (1, 3)\n",
      "\tcomput scienc (1, 1)\n",
      "\tmodel natur (1, 1)\n",
      "\tknow ledg (1, 3)\n",
      "\tbase method (3, 3)\n",
      "\tnote comput (1, 1)\n",
      "\tproceed page (1, 1)\n",
      "\tcomput good (1, 1)\n",
      "\tgood possibl (1, 2)\n",
      "\ttrain method (1, 1)\n",
      "\tstep process (1, 2)\n",
      "\tlink node (1, 5)\n",
      "\tstep creat (1, 1)\n",
      "\tnode network (2, 3)\n",
      "\tdatum new (1, 1)\n",
      "\tnode node (2, 6)\n",
      "\tnode repres (3, 5)\n",
      "\trepres squar (1, 1)\n",
      "\tmethod combin (1, 1)\n",
      "\tuse matrix (1, 1)\n",
      "\tdatum set (5, 2)\n",
      "\tdatum refer (1, 1)\n",
      "\tdatum number (1, 3)\n",
      "\tdatum repres (1, 1)\n",
      "\tterm word (3, 1)\n",
      "\torder repres (2, 1)\n",
      "\tstructur node (1, 2)\n",
      "\tbase propos (1, 2)\n",
      "\tcreat initi (1, 1)\n",
      "\tmethod base (1, 5)\n",
      "\tset structur (1, 1)\n",
      "\tdatum contain (1, 1)\n",
      "\trelat node (1, 1)\n",
      "\tdatum provid (1, 1)\n",
      "\tdatum exampl (1, 2)\n",
      "\tword provid (1, 1)\n",
      "\texampl provid (1, 1)\n",
      "\tgreat number (1, 1)\n",
      "\tdatum let (1, 1)\n",
      "\tshort term (1, 3)\n",
      "\tword sens (3, 10)\n",
      "\tmodel inform (2, 2)\n",
      "\tpoint view (1, 2)\n",
      "\tword base (1, 2)\n",
      "\tlabel indic (1, 1)\n",
      "\tneed improv (1, 1)\n",
      "\tdocument particular (2, 1)\n",
      "\tcomput case (1, 2)\n",
      "\tterm term (1, 1)\n",
      "\tlike low (1, 1)\n",
      "\tparticular categori (1, 1)\n",
      "\tcase avail (1, 1)\n",
      "\tprocedur appli (1, 1)\n",
      "\tword case (1, 2)\n",
      "\tprocess main (1, 1)\n",
      "\tword add (1, 2)\n",
      "\tlow case (1, 1)\n",
      "\tword determin (1, 2)\n",
      "\tprocedur follow (1, 1)\n",
      "\tgraph node (1, 2)\n",
      "\tgraph order (1, 1)\n",
      "\trun time (5, 1)\n",
      "\tprocess use (1, 1)\n",
      "\tlet assum (2, 1)\n",
      "\tvalu repres (1, 2)\n",
      "\texampl common (1, 1)\n",
      "\tassoci graph (1, 1)\n",
      "\tconfer inform (1, 1)\n",
      "\tmorgan publish (1, 1)\n",
      "\taddress issu (1, 1)\n",
      "\tunivers word (1, 2)\n",
      "\tvarieti applic (2, 1)\n",
      "\tword second (1, 1)\n",
      "\tlarg collect (2, 2)\n",
      "\tpropos use (1, 2)\n",
      "\trepres set (1, 1)\n",
      "\tsemant ambigu (1, 3)\n",
      "\tfollow express (1, 1)\n",
      "\tinstant time (1, 1)\n",
      "\tapproach present (2, 2)\n",
      "\tinform repres (1, 1)\n",
      "\tfeatur associ (1, 3)\n",
      "\tparticular featur (1, 1)\n",
      "\tdetermin relat (1, 1)\n",
      "\tcase possibl (2, 2)\n",
      "\tprocess word (1, 2)\n",
      "\tamount text (1, 1)\n",
      "\tattribut kind (1, 1)\n",
      "\trule show (1, 2)\n",
      "\torder inform (2, 1)\n",
      "\tcorrespond word (1, 2)\n",
      "\tpresenc absenc (1, 3)\n",
      "\tconfer evalu (1, 1)\n",
      "\tlanguag technolog (1, 1)\n",
      "\tdatum show (1, 1)\n",
      "\tsimilar spirit (1, 1)\n",
      "\timprov qualiti (3, 1)\n",
      "\ttext process (1, 1)\n",
      "\tmean text (1, 1)\n",
      "\tword similar (1, 1)\n",
      "\tstring word (1, 1)\n",
      "\tsimilar mean (1, 1)\n",
      "\tsolv problem (1, 1)\n",
      "\tknowledg word (1, 1)\n",
      "\tcontain inform (1, 1)\n",
      "\tsection describ (1, 9)\n",
      "\tfeatur correspond (1, 1)\n",
      "\tstart point (1, 2)\n",
      "\ttarget word (2, 16)\n",
      "\tfeatur extract (2, 2)\n",
      "\tfeatur word (1, 2)\n",
      "\tlist word (1, 1)\n",
      "\tadd attribut (1, 1)\n",
      "\tuse semant (1, 2)\n",
      "\tassign use (1, 1)\n",
      "\tbase featur (2, 3)\n",
      "\tconsist set (1, 1)\n",
      "\tfeatur select (15, 1)\n",
      "\tsecond step (1, 4)\n",
      "\tapproach base (1, 3)\n",
      "\taccord sens (1, 1)\n",
      "\tprocess exampl (3, 1)\n",
      "\tperiod time (1, 1)\n",
      "\tdatum work (1, 2)\n",
      "\twork word (1, 1)\n",
      "\talgorithm use (3, 1)\n",
      "\tprocess particular (2, 1)\n",
      "\tparticular use (3, 1)\n",
      "\tpriori approach (1, 1)\n",
      "\tbase close (1, 1)\n",
      "\tdata mean (1, 1)\n",
      "\tpropos algorithm (1, 1)\n",
      "\ttext mean (2, 1)\n",
      "\tmachin learn (2, 2)\n",
      "\tlearn model (1, 2)\n",
      "\tmodel requir (1, 2)\n",
      "\ttradit method (2, 1)\n",
      "\tfollow section (1, 4)\n",
      "\tsecond phase (2, 1)\n",
      "\tsmall number (5, 2)\n",
      "\tbase inform (2, 1)\n",
      "\tstructur complet (1, 1)\n",
      "\tapproach general (1, 1)\n",
      "\tapproach work (2, 1)\n",
      "\tcorpus success (1, 1)\n",
      "\tarea research (2, 1)\n",
      "\tbroad overview (1, 1)\n",
      "\trelat tradit (1, 1)\n",
      "\tbase algorithm (1, 1)\n",
      "\texampl use (1, 1)\n",
      "\tuse approxim (1, 1)\n",
      "\talgorithm good (1, 2)\n",
      "\tprobabilist framework (3, 1)\n",
      "\talgorithm algorithm (1, 2)\n",
      "\talgorithm describ (1, 2)\n",
      "\tassoci word (1, 2)\n",
      "\tindividu word (2, 2)\n",
      "\tsection discuss (3, 3)\n",
      "\treduc number (1, 3)\n",
      "\tsection contain (1, 1)\n",
      "\tsimilar approach (2, 3)\n",
      "\tlatent semant (8, 5)\n",
      "\tsimilar base (3, 1)\n",
      "\twork show (1, 1)\n",
      "\tsimpl algorithm (1, 1)\n",
      "\tdistribut frequenc (1, 1)\n",
      "\tmultipl time (1, 2)\n",
      "\trelat low (1, 1)\n",
      "\tnewli creat (1, 1)\n",
      "\tword give (1, 1)\n",
      "\tgive word (2, 1)\n",
      "\tdocument give (1, 1)\n",
      "\ttext collect (2, 1)\n",
      "\tprior distribut (1, 1)\n",
      "\tbase frequenc (1, 2)\n",
      "\tmethod follow (1, 1)\n",
      "\tinterest observ (1, 3)\n",
      "\talgorithm approxim (1, 1)\n",
      "\tnumber word (1, 3)\n",
      "\tword token (1, 2)\n",
      "\tword work (1, 1)\n",
      "\tword list (1, 1)\n",
      "\twork particular (1, 1)\n",
      "\tstandard algorithm (1, 1)\n",
      "\tunsupervis learn (1, 1)\n",
      "\tinform storag (1, 1)\n",
      "\tstorag retriev (1, 1)\n",
      "\tmodel comput (1, 2)\n",
      "\tweb base (2, 2)\n",
      "\tknowledg acquisit (1, 2)\n",
      "\texperiment result (2, 1)\n",
      "\tcombin mean (1, 1)\n",
      "\trelat concept (1, 1)\n",
      "\tnode link (1, 1)\n",
      "\tway order (1, 1)\n",
      "\tclassic exampl (1, 1)\n",
      "\tparticular context (1, 1)\n",
      "\tbelong particular (1, 1)\n",
      "\tmodel approach (3, 1)\n",
      "\tapproach network (1, 1)\n",
      "\tword phrase (1, 2)\n",
      "\ttext case (2, 1)\n",
      "\ttext relat (1, 1)\n",
      "\tnode correspond (6, 1)\n",
      "\trate weight (1, 1)\n",
      "\tsimpl form (1, 1)\n",
      "\tsuccess pair (1, 1)\n",
      "\tlink model (1, 1)\n",
      "\tgive node (1, 1)\n",
      "\tnode cluster (1, 4)\n",
      "\tmaximum number (1, 2)\n",
      "\tnumber give (1, 1)\n",
      "\tgive cluster (5, 1)\n",
      "\tgraph cluster (2, 1)\n",
      "\tnetwork cluster (1, 2)\n",
      "\thuman knowledg (1, 1)\n",
      "\tgraph repres (2, 1)\n",
      "\tiniti phase (1, 1)\n",
      "\tdocument consid (1, 2)\n",
      "\tcommunic node (1, 1)\n",
      "\tfollow step (1, 1)\n",
      "\tcontain word (1, 2)\n",
      "\tword retriev (1, 1)\n",
      "\tcontain number (1, 1)\n",
      "\tcontent associ (1, 1)\n",
      "\tnew node (1, 1)\n",
      "\tadd graph (1, 1)\n",
      "\tnew document (4, 2)\n",
      "\tdocument link (2, 1)\n",
      "\tnumber node (1, 1)\n",
      "\tdocument number (1, 1)\n",
      "\tweight associ (1, 1)\n",
      "\tretriev inform (2, 3)\n",
      "\tfrequent word (4, 1)\n",
      "\tshow cluster (1, 1)\n",
      "\tcomparison cluster (1, 2)\n",
      "\ttext topic (1, 1)\n",
      "\tmeasur base (1, 1)\n",
      "\tdescrib section (1, 2)\n",
      "\tsimilar pair (3, 2)\n",
      "\tpair word (1, 2)\n",
      "\tmatrix order (1, 1)\n",
      "\tcompar similar (1, 1)\n",
      "\tlink type (1, 1)\n",
      "\tdocument exampl (1, 1)\n",
      "\tlink consid (1, 1)\n",
      "\tcomput probabl (1, 1)\n",
      "\ttechniqu base (1, 1)\n",
      "\tdocument queri (1, 1)\n",
      "\tcontext document (1, 1)\n",
      "\tsearch engin (1, 8)\n",
      "\ttechnolog search (1, 1)\n",
      "\tdocument set (2, 1)\n",
      "\tretriev document (1, 1)\n",
      "\tmethod propos (1, 2)\n",
      "\tapproach requir (2, 1)\n",
      "\tcomput expens (1, 1)\n",
      "\tpropos method (2, 2)\n",
      "\tsection present (2, 3)\n",
      "\tretriev page (1, 1)\n",
      "\tpage contain (1, 1)\n",
      "\tlet set (1, 1)\n",
      "\tselect appropri (1, 3)\n",
      "\tbase analysi (1, 1)\n",
      "\tstrength term (2, 1)\n",
      "\tdescrib method (3, 1)\n",
      "\tselect select (1, 3)\n",
      "\tbasic method (1, 1)\n",
      "\tselect method (1, 4)\n",
      "\tmethod similar (1, 1)\n",
      "\tmethod use (5, 2)\n",
      "\tintroduc method (1, 1)\n",
      "\tmaximum valu (1, 1)\n",
      "\tfollow number (1, 1)\n",
      "\tuse set (3, 2)\n",
      "\tfollow function (1, 1)\n",
      "\tweb collect (2, 1)\n",
      "\tcollect document (1, 3)\n",
      "\tdocument relat (7, 1)\n",
      "\tdocument collect (15, 1)\n",
      "\tmethod number (3, 1)\n",
      "\tcondit probabl (1, 2)\n",
      "\tsolv method (1, 1)\n",
      "\tmethod paper (1, 1)\n",
      "\tcollect term (1, 1)\n",
      "\tcluster great (1, 1)\n",
      "\tcluster algorithm (35, 2)\n",
      "\tsimilar document (12, 3)\n",
      "\trepresent document (2, 1)\n",
      "\tcluster group (6, 1)\n",
      "\tprovid way (1, 1)\n",
      "\tdocument result (1, 2)\n",
      "\tcluster cluster (14, 6)\n",
      "\tcluster result (1, 2)\n",
      "\tcluster speed (1, 1)\n",
      "\tbase cluster (12, 2)\n",
      "\tcluster term (3, 1)\n",
      "\tqualiti document (1, 1)\n",
      "\tdocument transform (1, 1)\n",
      "\ttransform document (1, 1)\n",
      "\tdatum cluster (5, 2)\n",
      "\tpartit algorithm (4, 1)\n",
      "\thierarch algorithm (2, 1)\n",
      "\talgorithm document (3, 1)\n",
      "\tdocument probabl (3, 1)\n",
      "\tchoic algorithm (1, 1)\n",
      "\twell cluster (3, 2)\n",
      "\tcluster model (1, 1)\n",
      "\tmodel relat (1, 1)\n",
      "\trepresent cluster (5, 1)\n",
      "\tcluster qualiti (4, 5)\n",
      "\tcluster document (21, 2)\n",
      "\tcluster similar (4, 4)\n",
      "\tdocument follow (1, 1)\n",
      "\tcluster evalu (1, 2)\n",
      "\tdiscuss section (1, 1)\n",
      "\tclose relat (8, 1)\n",
      "\tvector space (1, 2)\n",
      "\tdescrib work (1, 1)\n",
      "\tdocument repres (2, 1)\n",
      "\taddit combin (1, 1)\n",
      "\tdata set (6, 1)\n",
      "\tword document (11, 1)\n",
      "\tdocument vector (2, 3)\n",
      "\tword occur (1, 1)\n",
      "\tterm weight (5, 1)\n",
      "\timprov cluster (3, 3)\n",
      "\tcomparison techniqu (1, 1)\n",
      "\tlow frequenc (1, 1)\n",
      "\tcontain document (1, 3)\n",
      "\tdocument word (5, 1)\n",
      "\tweight weight (2, 1)\n",
      "\tfrequenc invers (1, 3)\n",
      "\tinvers document (3, 3)\n",
      "\tdocument describ (1, 1)\n",
      "\tterm frequenc (3, 5)\n",
      "\tfrequenc document (3, 1)\n",
      "\tnumber term (2, 1)\n",
      "\tterm document (17, 1)\n",
      "\tdocument length (2, 2)\n",
      "\tdocument frequenc (6, 2)\n",
      "\tnumber document (3, 3)\n",
      "\tindividu document (1, 1)\n",
      "\thierarch cluster (15, 1)\n",
      "\tcluster method (12, 1)\n",
      "\tmean algorithm (4, 1)\n",
      "\talgorithm similar (1, 1)\n",
      "\tsimilar measur (2, 3)\n",
      "\tmeasur similar (2, 1)\n",
      "\tdot product (2, 1)\n",
      "\tprobabl cluster (1, 1)\n",
      "\tcluster entropi (1, 2)\n",
      "\tcluster describ (1, 1)\n",
      "\tcluster averag (1, 2)\n",
      "\tsimilar calcul (3, 1)\n",
      "\taverag similar (3, 1)\n",
      "\tpair document (7, 1)\n",
      "\tqualiti cluster (4, 1)\n",
      "\tcluster choic (1, 1)\n",
      "\tassign document (4, 1)\n",
      "\tcluster techniqu (11, 2)\n",
      "\tcompar studi (1, 1)\n",
      "\tproblem solv (3, 1)\n",
      "\tvalu topic (1, 1)\n",
      "\tcorpus document (3, 1)\n",
      "\toccur extrem (1, 1)\n",
      "\tcategori contain (1, 3)\n",
      "\tdocument corpus (6, 1)\n",
      "\tcluster potenti (1, 1)\n",
      "\tbase select (2, 1)\n",
      "\tmethod perform (1, 1)\n",
      "\tdocument entir (1, 1)\n",
      "\tstandard deviat (1, 3)\n",
      "\tdocument contain (5, 1)\n",
      "\tiniti cluster (1, 1)\n",
      "\tcluster centroid (5, 1)\n",
      "\torder reduc (4, 1)\n",
      "\tsimilar valu (1, 1)\n",
      "\tcluster refer (2, 1)\n",
      "\tnumber cluster (3, 4)\n",
      "\tcluster creat (6, 1)\n",
      "\tsection number (1, 1)\n",
      "\tvalu singl (1, 1)\n",
      "\twell qualiti (1, 1)\n",
      "\tresult main (1, 1)\n",
      "\tparticular document (2, 1)\n",
      "\tcluster small (1, 1)\n",
      "\tcluster contain (4, 1)\n",
      "\tsingl cluster (1, 1)\n",
      "\tperform way (1, 1)\n",
      "\tsimilar cluster (9, 1)\n",
      "\treduc nois (1, 1)\n",
      "\tterm frequent (1, 1)\n",
      "\tresult cluster (2, 1)\n",
      "\tdata point (12, 1)\n",
      "\tproblem graph (1, 1)\n",
      "\tgeneral concept (1, 2)\n",
      "\tfrequenc term (4, 1)\n",
      "\tvector case (2, 1)\n",
      "\treduc weight (1, 1)\n",
      "\tweight common (1, 1)\n",
      "\tbad case (2, 1)\n",
      "\tcontribut document (1, 1)\n",
      "\tsuccess improv (2, 1)\n",
      "\tapproach common (1, 1)\n",
      "\tlanguag comput (1, 1)\n",
      "\ttext cluster (45, 1)\n",
      "\tcluster base (5, 1)\n",
      "\ttext categor (6, 2)\n",
      "\tmethod word (1, 1)\n",
      "\tsemant applic (1, 1)\n",
      "\ttechniqu workshop (1, 1)\n",
      "\tworkshop text (1, 1)\n",
      "\ttext mine (2, 1)\n",
      "\tsecond edit (1, 1)\n",
      "\tonlin cluster (2, 1)\n",
      "\tbase text document (1, 5)\n",
      "\tchapter associ comput (1, 2)\n",
      "\tassoci comput linguist (1, 8)\n",
      "\tannual meet associ (1, 1)\n",
      "\tconfer european chapter (1, 1)\n",
      "\teuropean chapter associ (1, 1)\n",
      "\tmodel natur languag (1, 1)\n",
      "\tlatent semant analysi (2, 4)\n",
      "\tinform storag retriev (1, 1)\n",
      "\tnumber give cluster (1, 1)\n",
      "\tsection describ method (1, 1)\n",
      "\tdocument cluster cluster (4, 1)\n",
      "\timprov cluster qualiti (2, 2)\n",
      "\tfrequenc invers document (1, 3)\n",
      "\tinvers document frequenc (3, 2)\n",
      "\tdocument frequenc invers (1, 1)\n",
      "\thierarch cluster method (1, 1)\n",
      "\tbase cluster cluster (1, 1)\n",
      "\tcluster cluster document (1, 1)\n",
      "\tcluster averag similar (1, 1)\n",
      "\taverag similar pair (1, 1)\n",
      "\tsimilar pair document (1, 1)\n",
      "\tpair document cluster (1, 1)\n",
      "\tmodel inform retriev (1, 1)\n",
      "\tnatur languag comput (1, 1)\n",
      "\tdocument cluster techniqu (2, 1)\n",
      "\ttechniqu workshop text (1, 1)\n",
      "\tworkshop text mine (1, 1)\n",
      "Doc1 has 12811 unique terms\n",
      "Doc2 has 37449 unique terms\n",
      "Total like terms: \t544\n",
      "Total unlike terms: \t50260\n",
      "Cosine Similarity: 0.010751463517823256\n",
      "\n",
      "Text-Analytics-and-Natural-Language-Processing--.txt and Workshop on Robust Methods in Analysis of Natural Language Data.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like terms (doc1, doc2):\n",
      "\tnatur languag (23, 45)\n",
      "\tcomput linguist (1, 26)\n",
      "\texact know (2, 1)\n",
      "\tartifici intellig (5, 1)\n",
      "\thuman comput (1, 1)\n",
      "\tlanguag process (17, 20)\n",
      "\tdifficult problem (1, 1)\n",
      "\tinclud content (1, 1)\n",
      "\thelp user (1, 1)\n",
      "\tprobabl predict (1, 4)\n",
      "\tmeasur model (1, 1)\n",
      "\tmodel predict (6, 2)\n",
      "\tabl predict (1, 2)\n",
      "\tmodel show (1, 2)\n",
      "\thuman languag (1, 1)\n",
      "\tgenerat model (2, 4)\n",
      "\ttext datum (3, 1)\n",
      "\thigh level (2, 8)\n",
      "\tpattern match (1, 3)\n",
      "\timprov perform (2, 1)\n",
      "\tinform decis (1, 1)\n",
      "\tshow number (1, 2)\n",
      "\tcomput scienc (1, 1)\n",
      "\tprocess natur (1, 1)\n",
      "\tlanguag write (1, 1)\n",
      "\tdatum need (1, 1)\n",
      "\tdatum good (1, 1)\n",
      "\tprocess extract (1, 1)\n",
      "\trate data (1, 3)\n",
      "\tproduc datum (1, 2)\n",
      "\ttype inform (1, 2)\n",
      "\tdatum drive (1, 1)\n",
      "\teffect nois (1, 1)\n",
      "\tlinguist rule (2, 1)\n",
      "\tapproach implement (1, 1)\n",
      "\tyear ago (3, 1)\n",
      "\trelev result (3, 1)\n",
      "\tautomat translat (2, 2)\n",
      "\tlarg collect (1, 2)\n",
      "\tamount datum (2, 2)\n",
      "\tword appear (1, 2)\n",
      "\timprov qualiti (1, 1)\n",
      "\tdatum mean (2, 1)\n",
      "\tstart point (1, 2)\n",
      "\tprovid inform (1, 1)\n",
      "\textract assign (1, 1)\n",
      "\tinform avail (1, 2)\n",
      "\tappli reason (1, 1)\n",
      "\tanalyz provid (1, 1)\n",
      "\tcorrect semant (1, 1)\n",
      "\tcapabl analyz (1, 1)\n",
      "\tmachin learn (10, 2)\n",
      "\tmodel requir (1, 2)\n",
      "\tlanguag analyz (1, 1)\n",
      "\tresult semant (1, 2)\n",
      "\tmake possibl (1, 2)\n",
      "\tlanguag base (1, 1)\n",
      "\tbase inform (1, 1)\n",
      "\tinform algorithm (1, 1)\n",
      "\tidentifi text (1, 1)\n",
      "\tuse approxim (1, 1)\n",
      "\tsimpl approach (1, 1)\n",
      "\tinform languag (1, 1)\n",
      "\tmodel build (1, 1)\n",
      "\tbase larg (1, 1)\n",
      "\timport note (1, 1)\n",
      "\tlong possibl (2, 1)\n",
      "\tlike train (1, 1)\n",
      "\thigh precis (1, 1)\n",
      "\tprecis result (1, 1)\n",
      "\tperfect fit (2, 1)\n",
      "\tdesign evalu (1, 1)\n",
      "\ttextual content (2, 3)\n",
      "\tcontent document (1, 1)\n",
      "\tlong term (1, 8)\n",
      "\tconcept knowledg (2, 1)\n",
      "\tcase work (1, 1)\n",
      "\tproject problem (2, 1)\n",
      "\tmodel remain (1, 1)\n",
      "\tbase concept (1, 1)\n",
      "\tcomput vision (1, 1)\n",
      "\tscienc comput (1, 1)\n",
      "\tstate univers (1, 1)\n",
      "\tname entiti (1, 1)\n",
      "\tdeep understand (1, 2)\n",
      "\tset show (1, 1)\n",
      "\twell implement (1, 1)\n",
      "\tinternet search (2, 1)\n",
      "\tsearch result (4, 1)\n",
      "\tearli day (1, 1)\n",
      "\thelp improv (2, 1)\n",
      "\tword mean (1, 1)\n",
      "\tresult follow (1, 1)\n",
      "\tdocument summari (1, 1)\n",
      "\tdocument contain (1, 1)\n",
      "\tgreat care (1, 1)\n",
      "\tdocument add (1, 1)\n",
      "\tnew concept (3, 1)\n",
      "\tresult improv (2, 1)\n",
      "\twell result (1, 1)\n",
      "\tnatur languag process (17, 13)\n",
      "\tprocess natur languag (1, 1)\n",
      "\ttextual content document (1, 1)\n",
      "Doc1 has 4363 unique terms\n",
      "Doc2 has 37890 unique terms\n",
      "Total like terms: \t103\n",
      "Total unlike terms: \t42253\n",
      "Cosine Similarity: 0.029313427759387604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def calcCosSim(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        print(\"Lists do not have the same number of elements\")\n",
    "        return -1\n",
    "    sumProd = sum(n1*n2 for n1, n2 in zip(list1, list2))\n",
    "    sumL1 = sum(n**2 for n in list1)\n",
    "    sumL2 = sum(n**2 for n in list2)\n",
    "    \n",
    "    return sumProd/(math.sqrt(sumL1)*math.sqrt(sumL2))\n",
    "\n",
    "# Create vectors for all n-grams (with freq>1?) between two docs, multiply them,\n",
    "#     then divide by the product Euclidian norms\n",
    "# Print out similar phrases\n",
    "def cossim(ngrams, jsonDoc = jsonDocIndex, jsonInv = jsonInvIndex, absPath = os.getcwd()):\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonInv not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    \n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    with open(jsonDocPath, 'r') as jsonDoc:\n",
    "        docIndex = json.load(jsonDoc)\n",
    "    with open(jsonInvPath, 'r') as jsonInv:\n",
    "        invIndex = json.load(jsonInv)\n",
    "            \n",
    "        \n",
    "    for doc1 in invIndex:\n",
    "        doc1Terms = []\n",
    "        for ngram, terms in invIndex[doc1].items():\n",
    "            if int(ngram) in ngrams:\n",
    "                for term in terms:\n",
    "                    doc1Terms.append(term)\n",
    "        for doc2 in invIndex:\n",
    "            if doc2 > doc1:\n",
    "                print(docIndex[doc1]+\" and \"+docIndex[doc2])\n",
    "                allTerms = doc1Terms.copy()\n",
    "                likeTerms = []\n",
    "                for ngram, terms in invIndex[doc2].items():\n",
    "                    if int(ngram) in ngrams:\n",
    "                        for term in terms:\n",
    "                            if term not in allTerms:\n",
    "                                allTerms.append(term)\n",
    "                            else:\n",
    "                                likeTerms.append(term)\n",
    "                allTerms.sort()\n",
    "                doc1freq = []\n",
    "                doc2freq = []\n",
    "                for term in allTerms:\n",
    "                    weight = ngrams[len(term.split())]\n",
    "                    ngram = str(len(term.split()))\n",
    "                    doc1freq.append(invIndex[doc1][ngram].get(term, 0)*weight)\n",
    "                    doc2freq.append(invIndex[doc2][ngram].get(term, 0)*weight)\n",
    "                cosSimilarity = calcCosSim(doc1freq, doc2freq)\n",
    "                print(\"Like terms (doc1, doc2):\")\n",
    "                for term in likeTerms:\n",
    "                    print(\"\\t\"+term+\" (\"+str(invIndex[doc1][str(len(term.split()))][term])+\", \"+str(invIndex[doc2][str(len(term.split()))][term])+\")\")\n",
    "                numLike = len(likeTerms)\n",
    "                print(\"Doc1 has \"+str(len(doc1Terms)-numLike)+\" unique terms\")\n",
    "                print(\"Doc2 has \"+str(len(allTerms)-len(doc1Terms))+\" unique terms\")\n",
    "                print(\"Total like terms: \\t\"+str(numLike))\n",
    "                print(\"Total unlike terms: \\t\"+str(len(allTerms)-numLike))\n",
    "                print(\"Cosine Similarity: \"+ str(cosSimilarity), end='\\n\\n')\n",
    "gramWeight = {2:1, 3:5}\n",
    "cossim(gramWeight, absPath = absolute)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram parser-based inverted file \n",
    "# (TF-DIF to remove trigrams common to most or all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithm based on trigram inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigram parser-based info to inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement clustering on bigram inverted file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
