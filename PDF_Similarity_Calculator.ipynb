{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future improvements\n",
    "# Separate dir for each step\n",
    "# Create JSON array to assign IDs, \n",
    "#     keep track of PDF files process (each step?) etc.\n",
    "# Implement 'fi'/'fl' check at beginning and end of words\n",
    "# Remove numbers, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os # For file/directory interaction\n",
    "import time, sys\n",
    "import PyPDF2 # Library for converting PDF to text\n",
    "from datetime import datetime, date # For log data\n",
    "import decimal # Exceptions caught from this package\n",
    "import re # For text replacement\n",
    "import spacy # Pipeline processes (stopword and punctuation removal, lemmatization)\n",
    "from nltk.stem.snowball import SnowballStemmer # Pipeline process for stemming\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txtFilesDir = 'Text Files'\n",
    "rtnFilesDir = 'n Removed'\n",
    "swFilesDir = 'Stop Words'\n",
    "spaceFilesDir = 'No Spaces'\n",
    "absolute = 'C:/Users/micah/Documents/IWU/CIS Practicum/Files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on 'Lafferty_pcfg-notes.pdf'. . . done\n",
      "\n",
      "Now on 'LearningExecutableSemanticParsers.pdf'. . . done\n",
      "\n",
      "Now on 'p123-zhang.pdf'. . . "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "\n",
      "Now on 'Partial Parsing Finite-State Cascades.pdf'. . . done\n",
      "\n",
      "Now on 'QueryEffectiveness.pdf'. . . done\n",
      "\n",
      "Now on 'Text Clustering Algorithms.pdf'. . . done\n",
      "\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.pdf'. . . done\n",
      "\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.pdf'. . . done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-condition: All PDF files to be processed are in the sub-directory\n",
    "#     pdfDir, and pdfDir is in absPath. absPath is by default the \n",
    "#     directory in which the program is executed\n",
    "# Post-condition: All PDF files processed without error are converted to\n",
    "#     text files which are placed in a new sub-directory 'Text Files'\n",
    "def pdfToText(pdfDir, absPath = os.getcwd(), txtDir = txtFilesDir):\n",
    "    pdfPath = absPath+'/'+pdfDir\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if pdfDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + directory + '\" does not exist')\n",
    "        return\n",
    "# Creates 'Text Files' directory for converted PDFs\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        os.mkdir(txtPath)\n",
    "    \n",
    "    docNum = 0\n",
    "    stopAt = 25\n",
    "    totalNum = len([file for file in os.scandir(pdfPath) if file.name.endswith('.pdf')])\n",
    "    with open(absPath+'/'+'log.txt', 'a+', encoding=\"utf-8\") as log:\n",
    "        log.write(\"PDF to Text\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")        \n",
    "# Moves on to next entity if the current entity is not a PDF\n",
    "        for entity in os.scandir(pdfPath):\n",
    "            if not entity.name.endswith('.pdf') or entity.name[0] in '1234567890':\n",
    "                continue\n",
    "            docNum += 1\n",
    "            index = -4 # Remove '.pdf' from file name when creating '.txt' file\n",
    "            fileName = entity.name[:index]+'.txt'\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            \n",
    "# This block attempts to read the PDF file, extract text from each page,\n",
    "#     and write the text to a text file with the same name\n",
    "# Some documents are protected, corrupted, etc. and text cannot be extracted\n",
    "# Exceptions are recorded in log.txt\n",
    "# hasError remains true until each step in the try block is complete\n",
    "            if fileName not in os.listdir(txtPath): \n",
    "                hasError = True\n",
    "                with open(pdfPath+'/'+entity.name, 'rb') as pdfFile:\n",
    "                    try:\n",
    "                        pdfReader = PyPDF2.PdfFileReader(pdfFile)\n",
    "                        numPages = pdfReader.getNumPages()\n",
    "                        pageObject = pdfReader.getPage(0)\n",
    "                        textFile = open(txtPath+'/'+fileName, 'a+', encoding=\"utf-8\")\n",
    "                        i = 0;\n",
    "                        while i < numPages:\n",
    "                            pageObject = pdfReader.getPage(i)\n",
    "                            textFile.write(pageObject.extractText())\n",
    "                            i += 1\n",
    "                        print(\"done\\n\")\n",
    "                        hasError = False\n",
    "#                     except TypeError as e:\n",
    "#                         log.write(\"TypeError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except PyPDF2.utils.PdfReadError as e:\n",
    "#                         log.write(\"PdfReadError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except OSError as e:\n",
    "#                         log.write(\"OSError: \" + entity.name + \":\\n\" + str(e))\n",
    "#                     except decimal.InvalidOperation as e:\n",
    "#                         log.write(\"InvalidOperation: \" + entity.name + \":\\n\" + str(e))\n",
    "                    except Exception as e:\n",
    "                        log.write(str(docNum)+\": \" + entity.name + \": \\n\\t\" + str(e)+\"\\n\")\n",
    "                        textFile.close()\n",
    "                        if fileName in os.listdir(txtPath):\n",
    "                            os.remove(txtPath+'/'+fileName)\n",
    "                    pdfFile.close()\n",
    "\n",
    "                if hasError:\n",
    "                    print(\"there was an error reading this document. See log for details. Reference number \"+str(docNum)+\".\\n\")\n",
    "            else:\n",
    "                max = 10\n",
    "                if len(fileName) > max:\n",
    "                    print('\"'+fileName[:11]+'\"' + '...', end='')\n",
    "                else:\n",
    "                    print('\"'+fileName+'\"', end='')\n",
    "                print(' already exists\\n')\n",
    "            if docNum == stopAt:\n",
    "                print(\"PDF to Text was stopped after \"+str(docNum)+\" documents.\")\n",
    "                break\n",
    "        log.write(\"\\n\\n\")\n",
    "pdfToText('Working', absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on 'Lafferty_pcfg-notes.txt'. . . done\n",
      "Now on 'LearningExecutableSemanticParsers.txt'. . . done\n",
      "Now on 'p123-zhang.txt'. . . done\n",
      "Now on 'Partial Parsing Finite-State Cascades.txt'. . . done\n",
      "Now on 'QueryEffectiveness.txt'. . . done\n",
      "Now on 'Text Clustering Algorithms.txt'. . . done\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.txt'. . . done\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.txt'. . . done\n"
     ]
    }
   ],
   "source": [
    "# Work on 10 (good) files at a time until pipeline works\n",
    "#   then incrementally add files and clean up errors\n",
    "\n",
    "# Function to remove \\n\n",
    "def rmvN(txtDir = txtFilesDir, rtnDir = rtnFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    rtnPath = absPath+'/'+rtnDir\n",
    "    if rtnDir not in os.listdir(absPath):\n",
    "        os.mkdir(rtnPath)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as rtnFile:\n",
    "                text = txtFile.read()\n",
    "                text = re.sub('-\\n', '', text)\n",
    "                text = re.sub('\\n', '', text)\n",
    "                rtnFile.write(text)\n",
    "        print(\"done\")\n",
    "rmvN(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on 'Lafferty_pcfg-notes.txt'. . . done\n",
      "Now on 'LearningExecutableSemanticParsers.txt'. . . done\n",
      "Now on 'p123-zhang.txt'. . . done\n",
      "Now on 'Partial Parsing Finite-State Cascades.txt'. . . done\n",
      "Now on 'QueryEffectiveness.txt'. . . done\n",
      "Now on 'Text Clustering Algorithms.txt'. . . done\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.txt'. . . done\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.txt'. . . done\n"
     ]
    }
   ],
   "source": [
    "# Funtion to move files without spaces to new 'Without Spaces' directory         \n",
    "def checkSpaces(txtDir = txtFilesDir, spacesDir = spaceFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    spacesPath = absPath+'/'+spacesDir\n",
    "    if spacesDir not in os.listdir(absPath):\n",
    "        os.mkdir(spacesPath)\n",
    "        \n",
    "    with open(absPath+'/'+'Spaces.txt', 'a+', encoding='utf-8') as spaces: \n",
    "        spaces.write(\"Check Spaces\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")\n",
    "        for entity in os.scandir(txtPath):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            txtFile = open(txtPath+'/'+entity.name, 'r', encoding='utf-8')\n",
    "            text = txtFile.read()\n",
    "            split = text.split(' ')\n",
    "            if len(split) < len(text)/10 or len(text) < 100 or text == '':\n",
    "                txtFile.close()\n",
    "                spaces.write(entity.name+'\\n')\n",
    "                if entity.name not in os.listdir(spacesPath):\n",
    "                    os.rename(txtPath+'/'+entity.name, spacesPath+'/'+entity.name)\n",
    "                else:\n",
    "                    os.remove(txtPath+'/'+entity.name)\n",
    "            print(\"done\")\n",
    "        spaces.write('\\n\\n')\n",
    "checkSpaces(absPath = absolute) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on 'LearningExecutableSemanticParsers.txt'. . . done\n",
      "Now on 'QueryEffectiveness.txt'. . . done\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.txt'. . . done\n"
     ]
    }
   ],
   "source": [
    "# Function to remove stopwords\n",
    "# NLTK or SpaCy\n",
    "# Inverted File: gram:[doc1, doc3] or gram:[[doc1,freq], [doc3,freq]]\n",
    "def rmvStopWords(nlp, txtDir = txtFilesDir, swDir = swFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    swPath = absPath+'/'+swDir\n",
    "    if swDir not in os.listdir(absPath):\n",
    "        os.mkdir(swPath)\n",
    "        \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(swPath+'/'+entity.name, 'w+', encoding='utf-8') as swFile:\n",
    "                doc = nlp(txtFile.read())\n",
    "                noStopWords = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "#                 Quotation marks appear as 'fi' or 'fl' in text files (merged with words)\n",
    "#                 maybe compare to words that start with fi\n",
    "                swFile.write(\" \".join(noStopWords))\n",
    "        print(\"done\")\n",
    "\n",
    "rmvStopWords(nlp, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Words : \n",
      "                              term      rank\n",
      "114277           scrum scrum meet  0.495291\n",
      "17558           arxiv 1412 6980v9  0.408248\n",
      "7453                 6980v9 cs lg  0.408248\n",
      "5566                  30 jan 2017  0.408248\n",
      "75278                   lg 30 jan  0.408248\n",
      "36343                    cs lg 30  0.408248\n",
      "1149               1412 6980v9 cs  0.408248\n",
      "38145        decis support system  0.389146\n",
      "104414             quarter vol 25  0.321385\n",
      "48376        eni organiz communic  0.321385\n",
      "139423                vol 25 june  0.316201\n",
      "12142                af mil af073  0.302430\n",
      "10990         adapt decis support  0.299287\n",
      "45049           dual use commerci  0.297304\n",
      "136891      use commerci militari  0.297304\n",
      "28678    commerci militari applic  0.297304\n",
      "81569   mental retard development  0.246098\n",
      "111366  retard development disabl  0.246098\n",
      "93855         peopl mental retard  0.218242\n",
      "45888           educ train mental  0.211735\n",
      "88217        normal truncat model  0.208413\n",
      "115191        self determin learn  0.207465\n",
      "40987        determin learn model  0.207465\n",
      "76644   literatur relat hypothesi  0.204229\n",
      "55246       food thought question  0.203718\n",
      "74338        learn model instruct  0.195940\n",
      "108304      relat hypothesi model  0.194730\n",
      "133926        train mental retard  0.191305\n",
      "140679     weinberg gradi stanton  0.179712\n",
      "122059      stanton effect person  0.179712\n",
      "58784        gradi stanton effect  0.179712\n",
      "115207      self determin qualiti  0.177382\n",
      "41045       determin qualiti life  0.177382\n",
      "97001      poor typographi condit  0.173651\n",
      "11042     adapt niblack algorithm  0.168517\n",
      "128492             te eni organiz  0.165876\n",
      "104289         qualiti life score  0.163470\n",
      "78883         maneuv target track  0.162099\n",
      "85906       multipl passiv sensor  0.162099\n",
      "109215       report report report  0.161362\n",
      "29244       communic itmi quarter  0.160692\n",
      "70681            itmi quarter vol  0.160692\n",
      "91471       organiz communic itmi  0.160692\n",
      "49516                  et al 1993  0.153034\n",
      "20097         baltimor paul brook  0.150445\n",
      "102362       promot self determin  0.147657\n",
      "87717   niblack sauvola algorithm  0.147452\n",
      "51798        extend kalman filter  0.138942\n",
      "104262         qualiti life group  0.138321\n",
      "77411               look aud read  0.137736\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity score for each document\n",
    "# Files: id file_name; [id,]\n",
    "def tfdif_cossim(txtDir = txtFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+filesDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + directory + '\" does not exist')\n",
    "        return\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(3,3), stop_words=None, use_idf=True)\n",
    "    X2 = vectorizer.fit_transform([open(directory+'/'+file, \"r\", encoding='utf-8').read() for file in os.listdir(directory)[:25]])\n",
    "#     X2 = vectorizer.fit_transform(open(directory+'/'+'A Bible for the Disability Field.txt', 'r', encoding='utf-8'))\n",
    "    features = vectorizer.get_feature_names()\n",
    "    print(features)\n",
    "    scores = (X2.toarray()) \n",
    "#     print(\"\\n\\nScores : \\n\", scores) \n",
    "\n",
    "    # Getting top ranking features \n",
    "    sums = X2.sum(axis = 0) \n",
    "    data1 = [] \n",
    "    for col, term in enumerate(features): \n",
    "        data1.append( (term, sums[0, col] )) \n",
    "    ranking = pd.DataFrame(data1, columns = ['term', 'rank']) \n",
    "    words = (ranking.sort_values('rank', ascending = False)) \n",
    "    print (\"\\n\\nWords : \\n\", words.head(50)) \n",
    "    \n",
    "# tfdif_cossim(txtFilesDir, absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram parser-based inverted file \n",
    "# (TF-DIF to remove trigrams common to most or all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithm based on trigram inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigram parser-based info to inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement clustering on bigram inverted file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
