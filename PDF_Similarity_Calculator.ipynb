{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future improvements\n",
    "# Separate dir for each step\n",
    "# Create JSON array to assign IDs, \n",
    "#     keep track of PDF files process (each step?) etc.\n",
    "# Remove numbers, urls\n",
    "# Change variable names of jsonDoc, jsonInv, jsonGram to avoid redefiining argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os # For file/directory interaction\n",
    "import time, sys\n",
    "from datetime import datetime, date # For log data\n",
    "import re # For text replacement\n",
    "import spacy # Pipeline processes (stopword and punctuation removal, lemmatization)\n",
    "from nltk.stem.snowball import SnowballStemmer # Pipeline process for stemming\n",
    "import json\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txtFilesDir = 'Text Files'\n",
    "rtnFilesDir = 'n Removed'\n",
    "spaceFilesDir = 'No Spaces'\n",
    "swFilesDir = 'Stop Words'\n",
    "engFilesDir = 'English Words'\n",
    "stemFilesDir = 'Stemmed'\n",
    "jsonDocIndex = 'doc_dictionary.json'\n",
    "jsonInvIndex = 'inverted_index.json'\n",
    "jsonGramIndex = 'gram_index.json'\n",
    "jsonTFMatrix = 'tf_matrix.json'\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "absolute = 'C:/Users/micah/Documents/IWU/CIS Practicum/Files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on '00023.pdf'. . . \"00023.txt\" already exists\n",
      "\n",
      "Now on '0028_504_uchida_s.pdf'. . . \"0028_504_u...\" already exists\n",
      "\n",
      "Now on '0044_559_ishitani_y.pdf'. . . \"0044_559_i...\" already exists\n",
      "\n",
      "Now on '00708543.pdf'. . . \"00708543.t...\" already exists\n",
      "\n",
      "Now on '00823977.pdf'. . . \"00823977.t...\" already exists\n",
      "\n",
      "Now on '00863988.pdf'. . . \"00863988.t...\" already exists\n",
      "\n",
      "Now on '009.pdf'. . . \"009.txt\" already exists\n",
      "\n",
      "Now on '00969115.pdf'. . . \"00969115.t...\" already exists\n",
      "\n",
      "Now on '00a.pdf'. . . \"00a.txt\" already exists\n",
      "\n",
      "Now on '01-Altamura_Esposito_Malerba--Transforming_paper_documents_into_XML_format_with_WISDOM__.pdf'. . . \"01-Altamur...\" already exists\n",
      "\n",
      "Now on '01046104.pdf'. . . \"01046104.t...\" already exists\n",
      "\n",
      "Now on '01217605.pdf'. . . \"01217605.t...\" already exists\n",
      "\n",
      "Now on '01237554.pdf'. . . \"01237554.t...\" already exists\n",
      "\n",
      "Now on '01308672.pdf'. . . \"01308672.t...\" already exists\n",
      "\n",
      "Now on '01673370.pdf'. . . \"01673370.t...\" already exists\n",
      "\n",
      "Now on '01_AMI_Alcaniz.pdf'. . . \"01_AMI_Alc...\" already exists\n",
      "\n",
      "Now on '02_AMI_Riva.pdf'. . . \"02_AMI_Riv...\" already exists\n",
      "\n",
      "Now on '03_AMI_Gaggioli.pdf'. . . \"03_AMI_Gag...\" already exists\n",
      "\n",
      "Now on '04100664.pdf'. . . \"04100664.t...\" already exists\n",
      "\n",
      "Now on '04359339.pdf'. . . \"04359339.t...\" already exists\n",
      "\n",
      "Now on '04407722.pdf'. . . \"04407722.t...\" already exists\n",
      "\n",
      "Now on '04_AMI_istag.pdf'. . . \"04_AMI_ist...\" already exists\n",
      "\n",
      "Now on '05_AMI_Cortese.pdf'. . . \"05_AMI_Cor...\" already exists\n",
      "\n",
      "Now on '06_AMI__Piva.pdf'. . . \"06_AMI__Pi...\" already exists\n",
      "\n",
      "Now on '07_AMI_Kameas.pdf'. . . \"07_AMI_Kam...\" already exists\n",
      "\n",
      "Now on '08_AMI_Kleiner.pdf'. . . \"08_AMI_Kle...\" already exists\n",
      "\n",
      "Now on '09.pdf'. . . \"09.txt\" already exists\n",
      "\n",
      "Now on '09_AMI_Schmidt.pdf'. . . \"09_AMI_Sch...\" already exists\n",
      "\n",
      "Now on '0e.pdf'. . . \"0e.txt\" already exists\n",
      "\n",
      "Now on '1.pdf'. . . \"1.txt\" already exists\n",
      "\n",
      "Now on '10.1.1.115.7110.pdf'. . . \"10.1.1.115...\" already exists\n",
      "\n",
      "Now on '10.1.1.123.2158.pdf'. . . \"10.1.1.123...\" already exists\n",
      "\n",
      "Now on '10.1.1.130.6691.pdf'. . . \"10.1.1.130...\" already exists\n",
      "\n",
      "Now on '10.1.1.32.4689.pdf'. . . \"10.1.1.32....\" already exists\n",
      "\n",
      "Now on '10.1.1.62.889.pdf'. . . \"10.1.1.62....\" already exists\n",
      "\n",
      "Now on '10.1.1.65.7635.pdf'. . . \"10.1.1.65....\" already exists\n",
      "\n",
      "Now on '10.1.1.72.8127.pdf'. . . \"10.1.1.72....\" already exists\n",
      "\n",
      "Now on '10.1.1.91.9906.pdf'. . . \"10.1.1.91....\" already exists\n",
      "\n",
      "Now on '10.pdf'. . . \"10.txt\" already exists\n",
      "\n",
      "Now on '103583991.pdf'. . . \"103583991....\" already exists\n",
      "\n",
      "Now on '10_AMI_Simplicity.pdf'. . . \"10_AMI_Sim...\" already exists\n",
      "\n",
      "Now on '11.pdf'. . . \"11.txt\" already exists\n",
      "\n",
      "Now on '11_AMI_Cantoni.pdf'. . . \"11_AMI_Can...\" already exists\n",
      "\n",
      "Now on '12.pdf'. . . \"12.txt\" already exists\n",
      "\n",
      "Now on '1203.pdf'. . . \"1203.txt\" already exists\n",
      "\n",
      "Now on '122808.pdf'. . . \"122808.txt\" already exists\n",
      "\n",
      "Now on '124972.pdf'. . . \"124972.txt\" already exists\n",
      "\n",
      "Now on '124973.pdf'. . . \"124973.txt\" already exists\n",
      "\n",
      "Now on '124974.pdf'. . . \"124974.txt\" already exists\n",
      "\n",
      "Now on '124977.pdf'. . . \"124977.txt\" already exists\n",
      "\n",
      "Now on '124978.pdf'. . . \"124978.txt\" already exists\n",
      "\n",
      "Now on '124979.pdf'. . . \"124979.txt\" already exists\n",
      "\n",
      "Now on '12628324.pdf'. . . \"12628324.t...\" already exists\n",
      "\n",
      "Now on '12_AMI_Bettiol.pdf'. . . \"12_AMI_Bet...\" already exists\n",
      "\n",
      "Now on '12_steps_paper.pdf'. . . \"12_steps_p...\" already exists\n",
      "\n",
      "Now on '13.pdf'. . . \"13.txt\" already exists\n",
      "\n",
      "Now on '1306267704-OptimalExperienceinWorkandLeisure.pdf'. . . \"1306267704...\" already exists\n",
      "\n",
      "Now on '130771.pdf'. . . \"130771.txt\" already exists\n",
      "\n",
      "Now on '1311.2524v5.pdf'. . . \"1311.2524v...\" already exists\n",
      "\n",
      "Now on '1311.2901v3.pdf'. . . \"1311.2901v...\" already exists\n",
      "\n",
      "Now on '135706.pdf'. . . \"135706.txt\" already exists\n",
      "\n",
      "Now on '136540.pdf'. . . \"136540.txt\" already exists\n",
      "\n",
      "Now on '136542.pdf'. . . \"136542.txt\" already exists\n",
      "\n",
      "Now on '136543.pdf'. . . \"136543.txt\" already exists\n",
      "\n",
      "Now on '136546.pdf'. . . \"136546.txt\" already exists\n",
      "\n",
      "Now on '136644.pdf'. . . \"136644.txt\" already exists\n",
      "\n",
      "Now on '1369.pdf'. . . there was an error reading this document. See log for details. Reference number 67.\n",
      "\n",
      "Now on '139679.pdf'. . . \"139679.txt\" already exists\n",
      "\n",
      "Now on '139681.pdf'. . . \"139681.txt\" already exists\n",
      "\n",
      "Now on '139682.pdf'. . . \"139682.txt\" already exists\n",
      "\n",
      "Now on '139684.pdf'. . . \"139684.txt\" already exists\n",
      "\n",
      "Now on '139973.pdf'. . . \"139973.txt\" already exists\n",
      "\n",
      "Now on '139978.pdf'. . . \"139978.txt\" already exists\n",
      "\n",
      "Now on '139979.pdf'. . . \"139979.txt\" already exists\n",
      "\n",
      "Now on '13_AMI_Laso.pdf'. . . \"13_AMI_Las...\" already exists\n",
      "\n",
      "Now on '14-18436.pdf'. . . \"14-18436.t...\" already exists\n",
      "\n",
      "Now on '14.pdf'. . . \"14.txt\" already exists\n",
      "\n",
      "Now on '140594.pdf'. . . \"140594.txt\" already exists\n",
      "\n",
      "Now on '140595.pdf'. . . \"140595.txt\" already exists\n",
      "\n",
      "Now on '140597.pdf'. . . \"140597.txt\" already exists\n",
      "\n",
      "Now on '140599.pdf'. . . \"140599.txt\" already exists\n",
      "\n",
      "Now on '1406.2661v1.pdf'. . . \"1406.2661v...\" already exists\n",
      "\n",
      "Now on '1409.1556.pdf'. . . \"1409.1556....\" already exists\n",
      "\n",
      "Now on '1411416708528.pdf'. . . \"1411416708...\" already exists\n",
      "\n",
      "Now on '1412.2306v2.pdf'. . . \"1412.2306v...\" already exists\n",
      "\n",
      "Now on '142480.pdf'. . . \"142480.txt\" already exists\n",
      "\n",
      "Now on '142481.pdf'. . . \"142481.txt\" already exists\n",
      "\n",
      "Now on '142482.pdf'. . . \"142482.txt\" already exists\n",
      "\n",
      "Now on '142541.pdf'. . . \"142541.txt\" already exists\n",
      "\n",
      "Now on '142709.pdf'. . . \"142709.txt\" already exists\n",
      "\n",
      "Now on '148001.pdf'. . . \"148001.txt\" already exists\n",
      "\n",
      "Now on '1492645.pdf'. . . \"1492645.tx...\" already exists\n",
      "\n",
      "Now on '14_AMI_Cabrera.pdf'. . . \"14_AMI_Cab...\" already exists\n",
      "\n",
      "Now on '15.pdf'. . . \"15.txt\" already exists\n",
      "\n",
      "Now on '1504.08083.pdf'. . . \"1504.08083...\" already exists\n",
      "\n",
      "Now on '1506.01497v3.pdf'. . . \"1506.01497...\" already exists\n",
      "\n",
      "Now on '1506.02025.pdf'. . . \"1506.02025...\" already exists\n",
      "\n",
      "Now on '150752.pdf'. . . \"150752.txt\" already exists\n",
      "\n",
      "Now on '1512.03385v1.pdf'. . . \"1512.03385...\" already exists\n",
      "\n",
      "Now on '156469.pdf'. . . \"156469.txt\" already exists\n",
      "\n",
      "PDF to Text was stopped after 100 documents.\n"
     ]
    }
   ],
   "source": [
    "# Pre-condition: All PDF files to be processed are in the sub-directory\n",
    "#     pdfDir, and pdfDir is in absPath. absPath is by default the \n",
    "#     directory in which the program is executed\n",
    "# Post-condition: All PDF files processed without error are converted to\n",
    "#     text files which are placed in a new sub-directory 'Text Files'\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter, process_pdf#process_pdf\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "# Utilizes PDFminer3k to extract text from PDF documents\n",
    "def getText(pdfPath):\n",
    "# Sets up necessary objects\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    sio = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, sio, laparams=laparams)    \n",
    "    \n",
    "# Reads file to the text converter\n",
    "    with open(pdfPath, 'rb') as pdfFile:\n",
    "        process_pdf(rsrcmgr, device, pdfFile)                \n",
    "\n",
    "# Retrieves text result\n",
    "    text = sio.getvalue()\n",
    "    \n",
    "    device.close()\n",
    "    sio.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def pdfToText(pdfDir, absPath = os.getcwd(), txtDir = txtFilesDir, stopAt = -1):\n",
    "    \n",
    "    pdfPath = absPath+'/'+pdfDir\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if pdfDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + directory + '\" does not exist')\n",
    "        return\n",
    "# Creates 'Text Files' directory for converted PDFs\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        os.mkdir(txtPath)\n",
    "    \n",
    "    docNum = 0\n",
    "    totalNum = len([file for file in os.scandir(pdfPath) if file.name.endswith('.pdf')])\n",
    "    with open(absPath+'/'+'log.txt', 'a+', encoding=\"utf-8\") as log:    \n",
    "        for entity in os.scandir(pdfPath):\n",
    "        # Moves on to next entity if the current entity is not a PDF\n",
    "            if not entity.name.endswith('.pdf'):\n",
    "                continue\n",
    "            log.write(\"PDF to Text\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")    \n",
    "            index = -4 # Remove '.pdf' from file name when creating '.txt' file\n",
    "            fileName = entity.name[:index]+'.txt'\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            \n",
    "        # This block attempts to read the PDF file, extract text from each page,\n",
    "        #     and write the text to a text file with the same name\n",
    "        # Some documents are protected, corrupted, etc. and text cannot be extracted\n",
    "        # Exceptions are recorded in log.txt\n",
    "        # hasError remains true until each step in the try block is complete\n",
    "            if fileName not in os.listdir(txtPath): \n",
    "                docNum += 1\n",
    "                hasError = True\n",
    "            # Extracts text via getText method and writes to a text file. \n",
    "            # Errors are reported in log.txt\n",
    "                try:\n",
    "                    text = getText(pdfPath+'/'+entity.name)\n",
    "                    txtFile = open(txtPath+'/'+fileName, 'w+', encoding=\"utf-8\")\n",
    "                    txtFile.write(text)\n",
    "                    print(\"done\")\n",
    "                    hasError = False\n",
    "                except Exception as e:\n",
    "                    log.write(str(docNum)+\": \" + entity.name + \": \\n\\t\" + str(e)+\"\\n\")\n",
    "\n",
    "                if hasError:\n",
    "                    print(\"there was an error reading this document. See log for details. Reference number \"+str(docNum)+\".\\n\")\n",
    "            else:\n",
    "                max = 10\n",
    "                if len(fileName) > max:\n",
    "                    print('\"'+fileName[:max]+'...txt\"', end='')\n",
    "                else:\n",
    "                    print('\"'+fileName+'\"', end='')\n",
    "                print(' already exists')\n",
    "            if docNum >= stopAt and stopAt > 0:\n",
    "                print(\"PDF to Text was stopped after \"+str(docNum)+\" documents.\")\n",
    "                break\n",
    "        log.write(\"\\n\\n\")\n",
    "pdfToText('PDF', absPath = absolute, stopAt = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on 10 (good) files at a time until pipeline works\n",
    "#   then incrementally add files and clean up errors\n",
    "\n",
    "# Function to remove \\n\n",
    "def rmvN(txtDir = txtFilesDir, rtnDir = rtnFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    rtnPath = absPath+'/'+rtnDir\n",
    "    if rtnDir not in os.listdir(absPath):\n",
    "        os.mkdir(rtnPath)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(rtnPath+'/'+entity.name, 'w+', encoding='utf-8') as rtnFile:\n",
    "                text = txtFile.read()\n",
    "                text = re.sub('-\\n', '', text)\n",
    "                text = re.sub('\\n', '', text)\n",
    "                rtnFile.write(text)\n",
    "                rtnFile.truncate()\n",
    "        print(\"done\")\n",
    "rmvN(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Funtion to move files without spaces to new 'Without Spaces' directory         \n",
    "def checkSpaces(txtDir = rtnFilesDir, spacesDir = spaceFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    spacesPath = absPath+'/'+spacesDir\n",
    "    if spacesDir not in os.listdir(absPath):\n",
    "        os.mkdir(spacesPath)\n",
    "        \n",
    "    with open(absPath+'/'+'Spaces.txt', 'a+', encoding='utf-8') as spaces: \n",
    "        spaces.write(\"Check Spaces\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")\n",
    "        for entity in os.scandir(txtPath):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            txtFile = open(txtPath+'/'+entity.name, 'r', encoding='utf-8')\n",
    "            text = txtFile.read()\n",
    "            split = text.split(' ')\n",
    "            if len(split) < len(text)/10 or len(text) < 100 or text == '':\n",
    "                txtFile.close()\n",
    "                spaces.write(entity.name+'\\n')\n",
    "                if entity.name not in os.listdir(spacesPath):\n",
    "                    os.rename(txtPath+'/'+entity.name, spacesPath+'/'+entity.name)\n",
    "                else:\n",
    "                    os.remove(txtPath+'/'+entity.name)\n",
    "            print(\"done\")\n",
    "        spaces.write('\\n\\n')\n",
    "checkSpaces(absPath = absolute) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "# NLTK or SpaCy\n",
    "# Inverted File: gram:[doc1, doc3] or gram:[[doc1,freq], [doc3,freq]]\n",
    "def rmvStopWords(nlp, txtDir = rtnFilesDir, swDir = swFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    swPath = absPath+'/'+swDir\n",
    "    if swDir not in os.listdir(absPath):\n",
    "        os.mkdir(swPath)\n",
    "\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(swPath+'/'+entity.name, 'w+', encoding='utf-8') as swFile:\n",
    "                doc = nlp(txtFile.read())\n",
    "                noStopWords = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.text.isnumeric()]\n",
    "                swFile.write(\" \".join(noStopWords))\n",
    "                swFile.truncate()\n",
    "        print(\"done\")\n",
    "\n",
    "rmvStopWords(nlp, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove non-english words\n",
    "def rmvNonEng(txtDir = swFilesDir, engDir = engFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    engPath = absPath+'/'+engDir\n",
    "    if engDir not in os.listdir(absPath):\n",
    "        os.mkdir(engPath)\n",
    "    with open(absPath+'/'+'words_dictionary.json') as json_file:\n",
    "        words = json.load(json_file)\n",
    "        \n",
    "    lets = []\n",
    "    alph = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for let in alph:\n",
    "        lets.append(let)\n",
    "        for char in alph:\n",
    "            lets.append(let+char)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(engPath+'/'+entity.name, 'w+', encoding='utf-8') as engFile:\n",
    "                text = txtFile.read().split(' ')\n",
    "                engChars = [word for word in text if word in words and word not in lets]\n",
    "                engFile.write(\" \".join(engChars))\n",
    "                engFile.truncate()\n",
    "        print(\"done\")\n",
    "rmvNonEng(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stem words in all documents\n",
    "def stem(txtDir = engFilesDir, stemDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    stemPath = absPath+'/'+stemDir\n",
    "    if stemDir not in os.listdir(absPath):\n",
    "        os.mkdir(stemPath)\n",
    "        \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(stemPath+'/'+entity.name, 'w+', encoding='utf-8') as stemFile:\n",
    "                text = txtFile.read().split(' ')\n",
    "                stemmed = [stemmer.stem(word) for word in text]\n",
    "                stemFile.write(\" \".join(stemmed))\n",
    "                stemFile.truncate()\n",
    "        print(\"done\")\n",
    "stem(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create JSON with file_name : doc_id\n",
    "def update_doc_index(jsonDoc = jsonDocIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    jsonPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc in os.listdir(absPath):\n",
    "        with open(jsonPath, 'r') as jsonFile:\n",
    "            docIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        docIndex = {}\n",
    "    for fileName in os.listdir(txtPath):\n",
    "        if fileName not in docIndex.values():\n",
    "            docIndex[len(docIndex)+1] = fileName\n",
    "    with open(jsonPath, 'w') as jsonFile:\n",
    "        json.dump(docIndex, jsonFile, indent=4)\n",
    "\n",
    "update_doc_index(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_inv_index(ngrams, jsonDoc = jsonDocIndex, jsonInv = jsonInvIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv in os.listdir(absPath):\n",
    "        with open(jsonInvPath, 'r') as jsonFile:\n",
    "            invIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        invIndex = {}\n",
    "#     Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "        \n",
    "    for ngram in ngrams:\n",
    "        for docID in docIndex:\n",
    "            if docID not in invIndex:\n",
    "                invIndex[docID] = {}\n",
    "            if ngram not in invIndex[docID]:\n",
    "                invIndex[docID][ngram] = {}\n",
    "                with open(txtPath+'/'+docIndex[docID], 'r', encoding='utf-8') as txtFile:\n",
    "                    text = txtFile.read().split(' ')\n",
    "                    while len(text) > ngram-1:\n",
    "                        term = \" \".join(text[:ngram])\n",
    "                        if term in invIndex[docID][ngram]:\n",
    "                            invIndex[docID][ngram][term] += 1\n",
    "                        else:\n",
    "                            invIndex[docID][ngram][term] = 1\n",
    "                        text.pop(0)\n",
    "    with open(jsonInvPath, 'w') as jsonFile:\n",
    "        json.dump(invIndex, jsonFile, indent=4)\n",
    "\n",
    "update_inv_index(list(range(2,4)), absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_gram_index(minFreq = 1, jsonInv = jsonInvIndex, jsonGram = jsonGramIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "#     Makes sure all files and directories exist (jsonInv, txtDir)\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonInvPath + '\" does not exist')\n",
    "        return\n",
    "#     jsonGram is created if it does not exist\n",
    "    jsonGramPath = absPath+'/'+jsonGram\n",
    "    if jsonGram in os.listdir(absPath):\n",
    "        with open(jsonGramPath, 'r') as jsonFile:\n",
    "            gramIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        gramIndex = {}\n",
    "#     Loads Inverted File index\n",
    "    with open(jsonInvPath, 'r') as jsonFile:\n",
    "        invIndex = json.load(jsonFile)\n",
    "        \n",
    "    for docID, ngrams in invIndex.items():\n",
    "        for terms in ngrams.values():\n",
    "            for term, freq in terms.items():\n",
    "                if freq >= minFreq:\n",
    "                    if term not in gramIndex:\n",
    "                        gramIndex[term] = {}\n",
    "                    gramIndex[term][docID] = freq\n",
    "#     Writes gramIndex to gram JSON file\n",
    "    with open(jsonGramPath, 'w') as jsonFile:\n",
    "        json.dump(gramIndex, jsonFile, indent=4)\n",
    "\n",
    "update_gram_index(absPath = absolute)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print grams found in 3 or more documents\n",
    "def getGrams(numDocs = 3, jsonDoc = jsonDocIndex, jsonGram = jsonGramIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "#     Makes sure all files and directories exist (textDir, jsonDoc, txtDir)\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonGramPath = absPath+'/'+jsonGram\n",
    "    if jsonGram not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonGramPath + '\" does not exist')\n",
    "        return\n",
    "#    Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "#    Loads gram index \n",
    "    with open(jsonGramPath, 'r') as jsonFile:\n",
    "        gramIndex = json.load(jsonFile)\n",
    "    \n",
    "    for gram, docs in gramIndex.items():\n",
    "        if len(docs) >= numDocs:\n",
    "            print(\"\\\"\"+gram+\"\\\" found in \"+str(len(docs))+\" documents\")\n",
    "            for docID in docs:\n",
    "                print(\"\\t\"+ str(docIndex[docID]))\n",
    "                \n",
    "getGrams(10, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create vectors for all n-grams (with freq>1?) between two docs, multiply them,\n",
    "#     then divide by the product Euclidian norms\n",
    "# Print out similar phrases\n",
    "def freqMatrices(ngrams, jsonDoc = jsonDocIndex, jsonInv = jsonInvIndex, jsonTFM = jsonTFMatrix, absPath = os.getcwd()):\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonInvPath + '\" does not exist')\n",
    "        return\n",
    "    \n",
    "    with open(jsonDocPath, 'r') as jsonDoc:\n",
    "        docIndex = json.load(jsonDoc)\n",
    "    with open(jsonInvPath, 'r') as jsonInv:\n",
    "        invIndex = json.load(jsonInv)\n",
    "        \n",
    "#     jsonTFM is created if it does not exist\n",
    "    jsonTFMPath = absPath+'/'+jsonTFM\n",
    "    if jsonTFM in os.listdir(absPath):\n",
    "        with open(jsonTFMPath, 'r') as jsonFile:\n",
    "            tfMatrix = json.load(jsonFile)\n",
    "    else:\n",
    "        tfMatrix = {}     \n",
    "        \n",
    "    for doc1 in invIndex:\n",
    "        doc1Terms = []\n",
    "        if doc1 not in tfMatrix:\n",
    "            tfMatrix[doc1] = {}\n",
    "        for ngram, terms in invIndex[doc1].items():\n",
    "            if int(ngram) in ngrams:\n",
    "                for term in terms:\n",
    "                    doc1Terms.append(term)\n",
    "        for doc2 in invIndex:\n",
    "            if doc2 >= doc1 and doc2 not in tfMatrix[doc1]:\n",
    "                print(docIndex[doc1]+\" and \"+docIndex[doc2])\n",
    "                if doc2 not in tfMatrix[doc1]:\n",
    "                    tfMatrix[doc1][doc2] = {}\n",
    "                    tfMatrix[doc1][doc2][\"all\"] = {}\n",
    "                    tfMatrix[doc1][doc2][\"like\"] = {}\n",
    "                allTerms = doc1Terms.copy()\n",
    "                likeTerms = []\n",
    "                for ngram, terms in invIndex[doc2].items():\n",
    "                    if int(ngram) in ngrams:\n",
    "                        for term in terms:\n",
    "                            if term not in allTerms:\n",
    "                                allTerms.append(term)\n",
    "                            else:\n",
    "                                likeTerms.append(term)\n",
    "                allTerms.sort()\n",
    "                likeTerms.sort()\n",
    "                doc1all = {}\n",
    "                doc2all = {}\n",
    "                doc1like = {}\n",
    "                doc2like = {}\n",
    "#                 ngramFreq = {}\n",
    "                for gramLen in ngrams:\n",
    "#                     ngramFreq[gramLen] = 0\n",
    "                    doc1all[gramLen] = []\n",
    "                    doc2all[gramLen] = []\n",
    "                    doc1like[gramLen] = []\n",
    "                    doc2like[gramLen] = []\n",
    "    \n",
    "                for term in allTerms:\n",
    "                    length = len(term.split())\n",
    "#                     ngramFreq[length] += 1\n",
    "#                     weight = ngrams[length]\n",
    "                    ngram = str(len(term.split()))\n",
    "                    doc1all[length].append(invIndex[doc1][ngram].get(term, 0))\n",
    "                    doc2all[length].append(invIndex[doc2][ngram].get(term, 0))\n",
    "                for term in likeTerms:\n",
    "                    length = len(term.split())\n",
    "#                     ngramFreq[length] += 1\n",
    "#                     weight = ngrams[length]\n",
    "                    ngram = str(len(term.split()))\n",
    "                    doc1like[length].append(invIndex[doc1][ngram].get(term))\n",
    "                    doc2like[length].append(invIndex[doc2][ngram].get(term))\n",
    "#                     doc1weighted.append(invIndex[doc1][ngram].get(term)*weight)\n",
    "#                     doc2weighted.append(invIndex[doc2][ngram].get(term)*weight)\n",
    "#                 tfMatrix[doc1][doc2][\"ngrams\"] = ngramFreq\n",
    "                tfMatrix[doc1][doc2][\"all\"][doc1] = doc1all\n",
    "                tfMatrix[doc1][doc2][\"all\"][doc2] = doc2all\n",
    "                tfMatrix[doc1][doc2][\"like\"][doc1] = doc1like\n",
    "                tfMatrix[doc1][doc2][\"like\"][doc2] = doc2like\n",
    "#                 cosSimilarity = calcCosSim(doc1freq, doc2freq)\n",
    "#                 weightedCosSim = calcCosSim(doc1weighted, doc2weighted)\n",
    "#                 print(\"Like terms (doc1, doc2):\")\n",
    "#                 for term in likeTerms:\n",
    "#                     print(\"\\t\"+term+\" (\"+str(invIndex[doc1][str(len(term.split()))][term])+\", \"+str(invIndex[doc2][str(len(term.split()))][term])+\")\")\n",
    "                numLike = len(likeTerms)\n",
    "                print(\"Doc1 has \"+str(len(doc1Terms)-numLike)+\" unique terms\")\n",
    "                print(\"Doc2 has \"+str(len(allTerms)-len(doc1Terms))+\" unique terms\")\n",
    "                for gramLength in ngrams:\n",
    "                    print(\"{}-grams: {}\".format(gramLength, len(doc1like[gramLength])))\n",
    "                print(\"Total like terms: \"+str(numLike))\n",
    "                print(\"Total unlike terms: \"+str(len(allTerms)-numLike), end=\"\\n\\n\")\n",
    "#                 print(\"Cosine Similarity: \"+ str(cosSimilarity))\n",
    "#                 print(\"Weighted Cosine Similarity: \"+ str(weightedCosSim), end='\\n\\n')\n",
    "#     Writes tfMatrix to term-freq matrix JSON file\n",
    "                with open(jsonTFMPath, 'w') as jsonFile:\n",
    "                    json.dump(tfMatrix, jsonFile, indent=4)\n",
    "grams = [2, 3]\n",
    "freqMatrices(grams, absPath = absolute)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def calcCosSim(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        print(\"Lists do not have the same number of elements\")\n",
    "        return -1\n",
    "    sumProd = sum(n1*n2 for n1, n2 in zip(list1, list2))\n",
    "    sumL1 = sum(n**2 for n in list1)\n",
    "    sumL2 = sum(n**2 for n in list2)\n",
    "    \n",
    "    return sumProd/(math.sqrt(sumL1)*math.sqrt(sumL2))\n",
    "\n",
    "def cossim(gramWeights, jsonDoc = jsonDocIndex, jsonTFM = jsonTFMatrix, absPath = os.getcwd()):\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonTFMPath = absPath+'/'+jsonTFM\n",
    "    if jsonTFM not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonTFMPath + '\" does not exist')\n",
    "        return\n",
    "    \n",
    "    with open(jsonDocPath, 'r') as jsonDoc:\n",
    "        docIndex = json.load(jsonDoc)\n",
    "    with open(jsonTFMPath, 'r') as jsonInv:\n",
    "        tfMatrix = json.load(jsonInv)\n",
    "    \n",
    "    for doc1 in tfMatrix:\n",
    "        for doc2, values in tfMatrix[doc1].items():\n",
    "            print(docIndex[doc1]+\"({}) and \".format(doc1)+docIndex[doc2]+\"({})\".format(doc2))\n",
    "            doc1freq = []\n",
    "            doc2freq = []\n",
    "            doc1weighted = []\n",
    "            doc2weighted = []\n",
    "            for gramLen, weight in gramWeights.items():\n",
    "                gramLen = str(gramLen)\n",
    "                doc1freq += values[\"like\"][doc1][gramLen]\n",
    "                doc2freq += values[\"like\"][doc2][gramLen]\n",
    "                doc1weighted += [ val*weight for val in values[\"like\"][doc1][gramLen] ]\n",
    "                doc2weighted += [ val*weight for val in values[\"like\"][doc2][gramLen] ]\n",
    "                \n",
    "            print(\"Cosine Similarity: \", calcCosSim(doc1freq, doc2freq))\n",
    "            print(\"Weighted: \", calcCosSim(doc1weighted, doc2weighted), end=\"\\n\\n\")\n",
    "            \n",
    "            \n",
    "gramWeight = {2:1, 3:5}\n",
    "cossim(gramWeight, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram parser-based inverted file \n",
    "# (TF-DIF to remove trigrams common to most or all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithm based on trigram inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigram parser-based info to inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement clustering on bigram inverted file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
