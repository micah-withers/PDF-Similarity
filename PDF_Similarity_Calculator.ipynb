{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future improvements\n",
    "# Separate dir for each step\n",
    "# Create JSON array to assign IDs, \n",
    "#     keep track of PDF files process (each step?) etc.\n",
    "# Remove numbers, urls\n",
    "# Change variable names of jsonDoc, jsonInv, jsonGram to avoid redefiining argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os # For file/directory interaction\n",
    "import time, sys\n",
    "from datetime import datetime, date # For log data\n",
    "import re # For text replacement\n",
    "import spacy # Pipeline processes (stopword and punctuation removal, lemmatization)\n",
    "from nltk.stem.snowball import SnowballStemmer # Pipeline process for stemming\n",
    "import json\n",
    "\n",
    "txtFilesDir = 'Text Files'\n",
    "rtnFilesDir = 'n Removed'\n",
    "spaceFilesDir = 'No Spaces'\n",
    "swFilesDir = 'Stop Words'\n",
    "engFilesDir = 'English Words'\n",
    "stemFilesDir = 'Stemmed'\n",
    "jsonDocIndex = 'doc_dictionary.json'\n",
    "jsonInvIndex = 'inverted_index.json'\n",
    "jsonGramIndex = 'gram_index.json'\n",
    "jsonTFMatrix = 'tf_matrix.json'\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "absolute = 'C:/Users/micah/Documents/IWU/CIS Practicum/Files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on '00023.pdf'. . . \"00023.txt\" already exists\n",
      "Now on '0028_504_uchida_s.pdf'. . . \"0028_504_u...txt\" already exists\n",
      "Now on '0044_559_ishitani_y.pdf'. . . \"0044_559_i...txt\" already exists\n",
      "Now on '00708543.pdf'. . . \"00708543.t...txt\" already exists\n",
      "Now on '00823977.pdf'. . . \"00823977.t...txt\" already exists\n",
      "Now on '00863988.pdf'. . . \"00863988.t...txt\" already exists\n",
      "Now on '009.pdf'. . . \"009.txt\" already exists\n",
      "Now on '00969115.pdf'. . . \"00969115.t...txt\" already exists\n",
      "Now on '00a.pdf'. . . \"00a.txt\" already exists\n",
      "Now on '01-Altamura_Esposito_Malerba--Transforming_paper_documents_into_XML_format_with_WISDOM__.pdf'. . . \"01-Altamur...txt\" already exists\n",
      "Now on '01046104.pdf'. . . \"01046104.t...txt\" already exists\n",
      "Now on '01217605.pdf'. . . \"01217605.t...txt\" already exists\n",
      "Now on '01237554.pdf'. . . \"01237554.t...txt\" already exists\n",
      "Now on '01308672.pdf'. . . \"01308672.t...txt\" already exists\n",
      "Now on '01673370.pdf'. . . \"01673370.t...txt\" already exists\n",
      "Now on '01_AMI_Alcaniz.pdf'. . . \"01_AMI_Alc...txt\" already exists\n",
      "Now on '02_AMI_Riva.pdf'. . . \"02_AMI_Riv...txt\" already exists\n",
      "Now on '03_AMI_Gaggioli.pdf'. . . \"03_AMI_Gag...txt\" already exists\n",
      "Now on '04100664.pdf'. . . \"04100664.t...txt\" already exists\n",
      "Now on '04359339.pdf'. . . \"04359339.t...txt\" already exists\n",
      "Now on '04407722.pdf'. . . \"04407722.t...txt\" already exists\n",
      "Now on '04_AMI_istag.pdf'. . . \"04_AMI_ist...txt\" already exists\n",
      "Now on '05_AMI_Cortese.pdf'. . . \"05_AMI_Cor...txt\" already exists\n",
      "Now on '06_AMI__Piva.pdf'. . . \"06_AMI__Pi...txt\" already exists\n",
      "Now on '07_AMI_Kameas.pdf'. . . \"07_AMI_Kam...txt\" already exists\n",
      "Now on '08_AMI_Kleiner.pdf'. . . \"08_AMI_Kle...txt\" already exists\n",
      "Now on '09.pdf'. . . \"09.txt\" already exists\n",
      "Now on '09_AMI_Schmidt.pdf'. . . \"09_AMI_Sch...txt\" already exists\n",
      "Now on '0e.pdf'. . . \"0e.txt\" already exists\n",
      "Now on '1.pdf'. . . \"1.txt\" already exists\n",
      "Now on '10.1.1.115.7110.pdf'. . . \"10.1.1.115...txt\" already exists\n",
      "Now on '10.1.1.123.2158.pdf'. . . \"10.1.1.123...txt\" already exists\n",
      "Now on '10.1.1.130.6691.pdf'. . . \"10.1.1.130...txt\" already exists\n",
      "Now on '10.1.1.32.4689.pdf'. . . \"10.1.1.32....txt\" already exists\n",
      "Now on '10.1.1.62.889.pdf'. . . \"10.1.1.62....txt\" already exists\n",
      "Now on '10.1.1.65.7635.pdf'. . . \"10.1.1.65....txt\" already exists\n",
      "Now on '10.1.1.72.8127.pdf'. . . \"10.1.1.72....txt\" already exists\n",
      "Now on '10.1.1.91.9906.pdf'. . . \"10.1.1.91....txt\" already exists\n",
      "Now on '10.pdf'. . . \"10.txt\" already exists\n",
      "Now on '103583991.pdf'. . . \"103583991....txt\" already exists\n",
      "Now on '10_AMI_Simplicity.pdf'. . . \"10_AMI_Sim...txt\" already exists\n",
      "Now on '11.pdf'. . . \"11.txt\" already exists\n",
      "Now on '11_AMI_Cantoni.pdf'. . . \"11_AMI_Can...txt\" already exists\n",
      "Now on '12.pdf'. . . \"12.txt\" already exists\n",
      "Now on '1203.pdf'. . . \"1203.txt\" already exists\n",
      "Now on '122808.pdf'. . . \"122808.txt\" already exists\n",
      "Now on '124972.pdf'. . . \"124972.txt\" already exists\n",
      "Now on '124973.pdf'. . . \"124973.txt\" already exists\n",
      "Now on '124974.pdf'. . . \"124974.txt\" already exists\n",
      "Now on '124977.pdf'. . . \"124977.txt\" already exists\n",
      "Now on '124978.pdf'. . . \"124978.txt\" already exists\n",
      "Now on '124979.pdf'. . . \"124979.txt\" already exists\n",
      "Now on '12628324.pdf'. . . \"12628324.t...txt\" already exists\n",
      "Now on '12_AMI_Bettiol.pdf'. . . \"12_AMI_Bet...txt\" already exists\n",
      "Now on '12_steps_paper.pdf'. . . \"12_steps_p...txt\" already exists\n",
      "Now on '13.pdf'. . . \"13.txt\" already exists\n",
      "Now on '1306267704-OptimalExperienceinWorkandLeisure.pdf'. . . \"1306267704...txt\" already exists\n",
      "Now on '130771.pdf'. . . \"130771.txt\" already exists\n",
      "Now on '1311.2524v5.pdf'. . . \"1311.2524v...txt\" already exists\n",
      "Now on '1311.2901v3.pdf'. . . \"1311.2901v...txt\" already exists\n",
      "Now on '135706.pdf'. . . \"135706.txt\" already exists\n",
      "Now on '136540.pdf'. . . \"136540.txt\" already exists\n",
      "Now on '136542.pdf'. . . \"136542.txt\" already exists\n",
      "Now on '136543.pdf'. . . \"136543.txt\" already exists\n",
      "Now on '136546.pdf'. . . \"136546.txt\" already exists\n",
      "Now on '136644.pdf'. . . \"136644.txt\" already exists\n",
      "Now on '1369.pdf'. . . there was an error reading this document. See log for details. Reference number 1.\n",
      "\n",
      "Now on '139679.pdf'. . . \"139679.txt\" already exists\n",
      "Now on '139681.pdf'. . . \"139681.txt\" already exists\n",
      "Now on '139682.pdf'. . . \"139682.txt\" already exists\n",
      "Now on '139684.pdf'. . . \"139684.txt\" already exists\n",
      "Now on '139973.pdf'. . . \"139973.txt\" already exists\n",
      "Now on '139978.pdf'. . . \"139978.txt\" already exists\n",
      "Now on '139979.pdf'. . . \"139979.txt\" already exists\n",
      "Now on '13_AMI_Laso.pdf'. . . \"13_AMI_Las...txt\" already exists\n",
      "Now on '14-18436.pdf'. . . \"14-18436.t...txt\" already exists\n",
      "Now on '14.pdf'. . . \"14.txt\" already exists\n",
      "Now on '140594.pdf'. . . \"140594.txt\" already exists\n",
      "Now on '140595.pdf'. . . \"140595.txt\" already exists\n",
      "Now on '140597.pdf'. . . \"140597.txt\" already exists\n",
      "Now on '140599.pdf'. . . \"140599.txt\" already exists\n",
      "Now on '1406.2661v1.pdf'. . . \"1406.2661v...txt\" already exists\n",
      "Now on '1409.1556.pdf'. . . \"1409.1556....txt\" already exists\n",
      "Now on '1411416708528.pdf'. . . \"1411416708...txt\" already exists\n",
      "Now on '1412.2306v2.pdf'. . . \"1412.2306v...txt\" already exists\n",
      "Now on '142480.pdf'. . . \"142480.txt\" already exists\n",
      "Now on '142481.pdf'. . . \"142481.txt\" already exists\n",
      "Now on '142482.pdf'. . . \"142482.txt\" already exists\n",
      "Now on '142541.pdf'. . . \"142541.txt\" already exists\n",
      "Now on '142709.pdf'. . . \"142709.txt\" already exists\n",
      "Now on '148001.pdf'. . . \"148001.txt\" already exists\n",
      "Now on '1492645.pdf'. . . \"1492645.tx...txt\" already exists\n",
      "Now on '14_AMI_Cabrera.pdf'. . . \"14_AMI_Cab...txt\" already exists\n",
      "Now on '15.pdf'. . . \"15.txt\" already exists\n",
      "Now on '1504.08083.pdf'. . . \"1504.08083...txt\" already exists\n",
      "Now on '1506.01497v3.pdf'. . . \"1506.01497...txt\" already exists\n",
      "Now on '1506.02025.pdf'. . . \"1506.02025...txt\" already exists\n",
      "Now on '150752.pdf'. . . \"150752.txt\" already exists\n",
      "Now on '1512.03385v1.pdf'. . . \"1512.03385...txt\" already exists\n",
      "Now on '156469.pdf'. . . \"156469.txt\" already exists\n",
      "Now on '156665.pdf'. . . \"156665.txt\" already exists\n",
      "Now on '159814.pdf'. . . \"159814.txt\" already exists\n",
      "Now on '159815.pdf'. . . \"159815.txt\" already exists\n",
      "Now on '15_AMI_Morganti.pdf'. . . \"15_AMI_Mor...txt\" already exists\n",
      "Now on '16.pdf'. . . \"16.txt\" already exists\n",
      "Now on '1606.03507.pdf'. . . \"1606.03507...txt\" already exists\n",
      "Now on '1699_ftp.pdf;jsessionid=B4F0526473A4AAE435BDEACE7DE27074.f04t03.pdf'. . . \"1699_ftp.p...txt\" already exists\n",
      "Now on '16_58_10_733_Lead_Magazine_feature.pdf'. . . \"16_58_10_7...txt\" already exists\n",
      "Now on '17.pdf'. . . \"17.txt\" already exists\n",
      "Now on '1708.02924.pdf'. . . \"1708.02924...txt\" already exists\n",
      "Now on '173-pap0297-bateman.pdf'. . . \"173-pap029...txt\" already exists\n",
      "Now on '175922.pdf'. . . \"175922.txt\" already exists\n",
      "Now on '18.pdf'. . . \"18.txt\" already exists\n",
      "Now on '180953.pdf'. . . \"180953.txt\" already exists\n",
      "Now on '19.pdf'. . . \"19.txt\" already exists\n",
      "Now on '1900008.1900088.pdf'. . . \"1900008.19...txt\" already exists\n",
      "Now on '1993_Krishnamoorthy_etal_PAMI93-syntactic.pdf'. . . \"1993_Krish...txt\" already exists\n",
      "Now on '1f.pdf'. . . \"1f.txt\" already exists\n",
      "Now on '1st Stop Checklist- Business_Secretarial_Consulting Service.pdf'. . . \"1st Stop C...txt\" already exists\n",
      "Now on '1st Stop Starting Your Business in Ohio Nonemployer.pdf'. . . \"1st Stop S...txt\" already exists\n",
      "Now on '2.pdf'. . . \"2.txt\" already exists\n",
      "Now on '20.pdf'. . . \"20.txt\" already exists\n",
      "Now on '2002-breuel-spie.pdf'. . . \"2002-breue...txt\" already exists\n",
      "Now on '20030125.pdf'. . . \"20030125.t...txt\" already exists\n",
      "Now on '2003_CSE_approach_to_decision_support_systems - Smith and Geddes.pdf'. . . \"2003_CSE_a...txt\" already exists\n",
      "Now on '2006ICAD-WalkerNanceLindsay.pdf'. . . \"2006ICAD-W...txt\" already exists\n",
      "Now on '2008 Neuroprosthetics.pdf'. . . \"2008 Neuro...txt\" already exists\n",
      "Now on '2009BookChapter.pdf'. . . \"2009BookCh...txt\" already exists\n",
      "Now on '2009HFES-JeonWalker-cameraready.pdf'. . . \"2009HFES-J...txt\" already exists\n",
      "Now on '2009_trouva.pdf'. . . \"2009_trouv...txt\" already exists\n",
      "Now on '2010-03-SDS TechnicalBriefing.pdf'. . . \"2010-03-SD...txt\" already exists\n",
      "Now on '20100100869.pdf'. . . \"2010010086...txt\" already exists\n",
      "Now on '20131125093820659.pdf'. . . \"2013112509...txt\" already exists\n",
      "Now on '2014 1099 Report #1 - Keefer.pdf'. . . \"2014 1099 ...txt\" already exists\n",
      "Now on '2014 1099 Report #2 - Keefer.pdf'. . . \"2014 1099 ...txt\" already exists\n",
      "Now on '20140313_031900_0000d31ccabc.pdf'. . . \"20140313_0...txt\" already exists\n",
      "Now on '2015.10.12_Millenson_Berenson.pdf'. . . \"2015.10.12...txt\" already exists\n",
      "Now on '2016HealthcareBenchmarks_Care_Coordination_preview.pdf'. . . \"2016Health...txt\" already exists\n",
      "Now on '21.pdf'. . . \"21.txt\" already exists\n",
      "Now on '213.pdf'. . . \"213.txt\" already exists\n",
      "Now on '21851407.pdf'. . . \"21851407.t...txt\" already exists\n",
      "Now on '22.pdf'. . . \"22.txt\" already exists\n",
      "Now on '23.pdf'. . . \"23.txt\" already exists\n",
      "Now on '2393216.2393281.pdf'. . . \"2393216.23...txt\" already exists\n",
      "Now on '24.pdf'. . . \"24.txt\" already exists\n",
      "Now on '25.pdf'. . . \"25.txt\" already exists\n",
      "Now on '2553062.2553065.pdf'. . . \"2553062.25...txt\" already exists\n",
      "Now on '2666795.2666817.pdf'. . . \"2666795.26...txt\" already exists\n",
      "Now on '280-p276-winberg.pdf'. . . \"280-p276-w...txt\" already exists\n",
      "Now on '2891868.pdf'. . . \"2891868.tx...txt\" already exists\n",
      "Now on '3.pdf'. . . \"3.txt\" already exists\n",
      "Now on '30 Years of Minix.pdf'. . . \"30 Years o...txt\" already exists\n",
      "Now on '30.pdf'. . . \"30.txt\" already exists\n",
      "Now on '3178_18_1.pdf'. . . \"3178_18_1....txt\" already exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on '33163615.pdf'. . . \"33163615.t...txt\" already exists\n",
      "Now on '33451B Medical Device Manufacturing in the US Industry Report.pdf'. . . \"33451B Med...txt\" already exists\n",
      "Now on '34-the-power-of-full-engagement.pdf'. . . \"34-the-pow...txt\" already exists\n",
      "Now on '343362a0.pdf'. . . \"343362a0.t...txt\" already exists\n",
      "Now on '37_ExampleBased_JMSP_SpecialIsuue.pdf'. . . \"37_Example...txt\" already exists\n",
      "Now on '3_Embedded_Care_Coordination_Models_To_Manage_Diverse.pdf'. . . \"3_Embedded...txt\" already exists\n",
      "Now on '3_Toms.pdf'. . . \"3_Toms.txt\" already exists\n",
      "Now on '4.pdf'. . . \"4.txt\" already exists\n",
      "Now on '419.full.pdf'. . . \"419.full.t...txt\" already exists\n",
      "Now on '420-1094-2-PB.pdf'. . . \"420-1094-2...txt\" already exists\n",
      "Now on '42345 Medical Supplies Wholesaling in the US Industry Report.pdf'. . . \"42345 Medi...txt\" already exists\n",
      "Now on '427-1101-2-PB.pdf'. . . \"427-1101-2...txt\" already exists\n",
      "Now on '4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf'. . . \"4824-image...txt\" already exists\n",
      "Now on '5 Transitions Great Leaders Make That Average Leaders Don't - Forbes.pdf'. . . \"5 Transiti...txt\" already exists\n",
      "Now on '5.pdf'. . . \"5.txt\" already exists\n",
      "Now on '5001766_Apparatus_and_method_for_skew_co.pdf'. . . \"5001766_Ap...txt\" already exists\n",
      "Now on '50fb36cd-2708-4308-b75e-2d6e970efc3f.pdf'. . . \"50fb36cd-2...txt\" already exists\n",
      "Now on '5100326.pdf'. . . \"5100326.tx...txt\" already exists\n",
      "Now on '51121C Business Analytics & Enterprise Software Publishing in the US Industry Report.pdf'. . . \"51121C Bus...txt\" already exists\n",
      "Now on '52.pdf'. . . \"52.txt\" already exists\n",
      "Now on '52411B Health & Medical Insurance in the US Industry Report.pdf'. . . \"52411B Hea...txt\" already exists\n",
      "Now on '54151 IT Consulting in the US Industry Report.pdf'. . . \"54151 IT C...txt\" already exists\n",
      "Now on '56160540.pdf'. . . \"56160540.t...txt\" already exists\n",
      "Now on '5741258.pdf'. . . \"5741258.tx...txt\" already exists\n",
      "Now on '5845-15287-1-PB.pdf'. . . \"5845-15287...txt\" already exists\n",
      "Now on '5CsofAgileManagement.pdf'. . . \"5CsofAgile...txt\" already exists\n",
      "Now on '6.pdf'. . . \"6.txt\" already exists\n",
      "Now on '6144888_Modular_system_and_architecture.pdf'. . . \"6144888_Mo...txt\" already exists\n",
      "Now on '62151 Diagnostic & Medical Laboratories in the US Industry Report.pdf'. . . \"62151 Diag...txt\" already exists\n",
      "Now on '62211 Hospitals in the US Industry Report.pdf'. . . \"62211 Hosp...txt\" already exists\n",
      "Now on '62231 Specialty Hospitals in the US Industry Report.pdf'. . . \"62231 Spec...txt\" already exists\n",
      "Now on '6368110.pdf'. . . \"6368110.tx...txt\" already exists\n",
      "Now on '6505087_Modular_system_and_architecture.pdf'. . . \"6505087_Mo...txt\" already exists\n",
      "Now on '6685188.pdf'. . . \"6685188.tx...txt\" already exists\n",
      "Now on '6948938.pdf'. . . \"6948938.tx...txt\" already exists\n",
      "Now on '7 Rules for Job Interview Questions That Result in Great Hires.pdf'. . . \"7 Rules fo...txt\" already exists\n",
      "Now on '7 Ways To Improve Patient Satisfaction, Experience, And Customer Service,.pdf'. . . \"7 Ways To ...txt\" already exists\n",
      "Now on '7-1-13_SR_GivingUSA.pdf'. . . \"7-1-13_SR_...txt\" already exists\n",
      "Now on '7-Things-Health-Insurance-Customers-Not-Telling-You.pdf'. . . \"7-Things-H...txt\" already exists\n",
      "Now on '7.pdf'. . . \"7.txt\" already exists\n",
      "Now on '7007809.pdf'. . . \"7007809.tx...txt\" already exists\n",
      "Now on '7409422.pdf'. . . \"7409422.tx...txt\" already exists\n",
      "Now on '7469_Chap08.pdf'. . . there was an error reading this document. See log for details. Reference number 2.\n",
      "\n",
      "Now on '769.pdf'. . . \"769.txt\" already exists\n",
      "Now on '7890926.pdf'. . . \"7890926.tx...txt\" already exists\n",
      "Now on '8.pdf'. . . \"8.txt\" already exists\n",
      "Now on '8229871.pdf'. . . \"8229871.tx...txt\" already exists\n",
      "Now on '831-828-1-PB.pdf'. . . \"831-828-1-...txt\" already exists\n",
      "Now on '8815fe56-9e95-414b-b5e3-1f8363e5471d.pdf'. . . \"8815fe56-9...txt\" already exists\n",
      "Now on '9-Planning.pdf'. . . there was an error reading this document. See log for details. Reference number 3.\n",
      "\n",
      "Now on '9.pdf'. . . \"9.txt\" already exists\n",
      "Now on '90.pdf'. . . there was an error reading this document. See log for details. Reference number 4.\n",
      "\n",
      "Now on 'A  cognitive affective model of organizational communication for designing IT.pdf'. . . \"A  cogniti...txt\" already exists\n",
      "Now on 'A Bible for the Disability Field.pdf'. . . \"A Bible fo...txt\" already exists\n",
      "Now on 'A Comparisoon of Binarization Methods for Historical Archive Documents.pdf'. . . \"A Comparis...txt\" already exists\n",
      "Now on 'A light-weight text image processing method for handheld embedded cameras.pdf'. . . \"A light-we...txt\" already exists\n",
      "Now on 'A Mathematical Theory of Communication.pdf'. . . \"A Mathemat...txt\" already exists\n",
      "Now on 'A Model for Types and Levels of Human Interaction with Automation.pdf'. . . \"A Model fo...txt\" already exists\n",
      "Now on 'A Parser for Real-Time Speech Synthesis of Conversational Texts.pdf'. . . \"A Parser f...txt\" already exists\n",
      "Now on 'A Physically Based Approach to 2-D Shape Blending.pdf'. . . \"A Physical...txt\" already exists\n",
      "Now on 'A Review of 326 Children with Developmental and Physical Disabilities, Consecutively Taught at the Movement Development Clinic.pdf'. . . \"A Review o...txt\" already exists\n",
      "Now on 'A Review of Quasi-Linear Pilot Models.pdf'. . . done (1/10)\n",
      "Now on 'A Set of C++ Classes for Co-routine Style Programming.pdf'. . . done (2/10)\n",
      "Now on 'A Spiral Model of Software Development and Enhancement.pdf'. . . done (3/10)\n",
      "Now on 'A Study of Design Requirements for Mobile Learning Environments.pdf'. . . done (4/10)\n",
      "Now on 'A Target Tracking Algorithm of Passive Sensor Normal Truncated Model.pdf'. . . done (5/10)\n",
      "Now on 'a12.pdf'. . . done (6/10)\n",
      "Now on 'a2-jeong.pdf'. . . done (7/10)\n",
      "Now on 'a2.pdf'. . . done (8/10)\n",
      "Now on 'AACOpenNewWorld.pdf'. . . done (9/10)\n",
      "Now on 'aar.pdf'. . . done (10/10)\n",
      "PDF to Text was stopped after 10 documents.\n"
     ]
    }
   ],
   "source": [
    "# Pre-condition: All PDF files to be processed are in the sub-directory\n",
    "#     pdfDir, and pdfDir is in absPath. absPath is by default the \n",
    "#     directory in which the program is executed\n",
    "# Post-condition: All PDF files processed without error are converted to\n",
    "#     text files which are placed in a new sub-directory 'Text Files'\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter, process_pdf#process_pdf\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "# Utilizes PDFminer3k to extract text from PDF documents\n",
    "def getText(pdfPath):\n",
    "# Sets up necessary objects\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    sio = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, sio, laparams=laparams)    \n",
    "    \n",
    "# Reads file to the text converter\n",
    "    with open(pdfPath, 'rb') as pdfFile:\n",
    "        process_pdf(rsrcmgr, device, pdfFile)                \n",
    "\n",
    "# Retrieves text result\n",
    "    text = sio.getvalue()\n",
    "    \n",
    "    device.close()\n",
    "    sio.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def pdfToText(pdfDir, absPath = os.getcwd(), txtDir = txtFilesDir, stopAt = -1):\n",
    "    \n",
    "    pdfPath = absPath+'/'+pdfDir\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if pdfDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + directory + '\" does not exist')\n",
    "        return\n",
    "# Creates 'Text Files' directory for converted PDFs\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        os.mkdir(txtPath)\n",
    "    \n",
    "    docNum = 0\n",
    "    docErr = 1\n",
    "    totalNum = len([file for file in os.scandir(pdfPath) if file.name.endswith('.pdf')])\n",
    "    with open(absPath+'/'+'log.txt', 'a+', encoding=\"utf-8\") as log:    \n",
    "        for entity in os.scandir(pdfPath):\n",
    "        # Moves on to next entity if the current entity is not a PDF\n",
    "            if not entity.name.endswith('.pdf'):\n",
    "                continue\n",
    "            log.write(\"PDF to Text\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")    \n",
    "            index = -4 # Remove '.pdf' from file name when creating '.txt' file\n",
    "            fileName = entity.name[:index]+'.txt'\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            \n",
    "        # This block attempts to read the PDF file, extract text from each page,\n",
    "        #     and write the text to a text file with the same name\n",
    "        # Some documents are protected, corrupted, etc. and text cannot be extracted\n",
    "        # Exceptions are recorded in log.txt\n",
    "        # hasError remains true until each step in the try block is complete\n",
    "            if fileName not in os.listdir(txtPath): \n",
    "                hasError = True\n",
    "            # Extracts text via getText method and writes to a text file. \n",
    "            # Errors are reported in log.txt\n",
    "                try:\n",
    "                    text = getText(pdfPath+'/'+entity.name)\n",
    "                    txtFile = open(txtPath+'/'+fileName, 'w+', encoding=\"utf-8\")\n",
    "                    txtFile.write(text)\n",
    "                    docNum += 1\n",
    "                    print(\"done ({}/{})\".format(docNum, stopAt))\n",
    "                    hasError = False\n",
    "                except Exception as e:\n",
    "                    log.write(str(docErr)+\": \" + entity.name + \": \\n\\t\" + str(e)+\"\\n\")\n",
    "\n",
    "                if hasError:\n",
    "                    print(\"there was an error reading this document. See log for details. Reference number \"+str(docErr)+\".\\n\")\n",
    "                    docErr += 1\n",
    "            else:\n",
    "                max = 10\n",
    "                if len(fileName) > max:\n",
    "                    print('\"'+fileName[:max]+'...txt\"', end='')\n",
    "                else:\n",
    "                    print('\"'+fileName+'\"', end='')\n",
    "                print(' already exists')\n",
    "            if docNum >= stopAt and stopAt > 0:\n",
    "                print(\"PDF to Text was stopped after \"+str(docNum)+\" documents.\")\n",
    "                break\n",
    "        log.write(\"\\n\\n\")\n",
    "pdfToText('PDF', absPath = absolute, stopAt = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on '00023.txt'. . . done\n",
      "Now on '0028_504_uchida_s.txt'. . . done\n",
      "Now on '0044_559_ishitani_y.txt'. . . done\n",
      "Now on '00708543.txt'. . . done\n",
      "Now on '00823977.txt'. . . done\n",
      "Now on '00863988.txt'. . . done\n",
      "Now on '009.txt'. . . done\n",
      "Now on '00969115.txt'. . . done\n",
      "Now on '00a.txt'. . . done\n",
      "Now on '01-Altamura_Esposito_Malerba--Transforming_paper_documents_into_XML_format_with_WISDOM__.txt'. . . done\n",
      "Now on '01046104.txt'. . . done\n",
      "Now on '01217605.txt'. . . done\n",
      "Now on '01237554.txt'. . . done\n",
      "Now on '01308672.txt'. . . done\n",
      "Now on '01673370.txt'. . . done\n",
      "Now on '01_AMI_Alcaniz.txt'. . . done\n",
      "Now on '02_AMI_Riva.txt'. . . done\n",
      "Now on '03_AMI_Gaggioli.txt'. . . done\n",
      "Now on '04100664.txt'. . . done\n",
      "Now on '04359339.txt'. . . done\n",
      "Now on '04407722.txt'. . . done\n",
      "Now on '04_AMI_istag.txt'. . . done\n",
      "Now on '05_AMI_Cortese.txt'. . . done\n",
      "Now on '06_AMI__Piva.txt'. . . done\n",
      "Now on '07_AMI_Kameas.txt'. . . done\n",
      "Now on '08_AMI_Kleiner.txt'. . . done\n",
      "Now on '09.txt'. . . done\n",
      "Now on '09_AMI_Schmidt.txt'. . . done\n",
      "Now on '0e.txt'. . . done\n",
      "Now on '1.txt'. . . done\n",
      "Now on '10.1.1.115.7110.txt'. . . done\n",
      "Now on '10.1.1.123.2158.txt'. . . done\n",
      "Now on '10.1.1.130.6691.txt'. . . done\n",
      "Now on '10.1.1.32.4689.txt'. . . done\n",
      "Now on '10.1.1.62.889.txt'. . . done\n",
      "Now on '10.1.1.65.7635.txt'. . . done\n",
      "Now on '10.1.1.72.8127.txt'. . . done\n",
      "Now on '10.1.1.91.9906.txt'. . . done\n",
      "Now on '10.txt'. . . done\n",
      "Now on '103583991.txt'. . . done\n",
      "Now on '10_AMI_Simplicity.txt'. . . done\n",
      "Now on '11.txt'. . . done\n",
      "Now on '11_AMI_Cantoni.txt'. . . done\n",
      "Now on '12.txt'. . . done\n",
      "Now on '1203.txt'. . . done\n",
      "Now on '122808.txt'. . . done\n",
      "Now on '124972.txt'. . . done\n",
      "Now on '124973.txt'. . . done\n",
      "Now on '124974.txt'. . . done\n",
      "Now on '124977.txt'. . . done\n",
      "Now on '124978.txt'. . . done\n",
      "Now on '124979.txt'. . . done\n",
      "Now on '12628324.txt'. . . done\n",
      "Now on '12_AMI_Bettiol.txt'. . . done\n",
      "Now on '12_steps_paper.txt'. . . done\n",
      "Now on '13.txt'. . . done\n",
      "Now on '1306267704-OptimalExperienceinWorkandLeisure.txt'. . . done\n",
      "Now on '130771.txt'. . . done\n",
      "Now on '1311.2524v5.txt'. . . done\n",
      "Now on '1311.2901v3.txt'. . . done\n",
      "Now on '135706.txt'. . . done\n",
      "Now on '136540.txt'. . . done\n",
      "Now on '136542.txt'. . . done\n",
      "Now on '136543.txt'. . . done\n",
      "Now on '136546.txt'. . . done\n",
      "Now on '136644.txt'. . . done\n",
      "Now on '139679.txt'. . . done\n",
      "Now on '139681.txt'. . . done\n",
      "Now on '139682.txt'. . . done\n",
      "Now on '139684.txt'. . . done\n",
      "Now on '139973.txt'. . . done\n",
      "Now on '139978.txt'. . . done\n",
      "Now on '139979.txt'. . . done\n",
      "Now on '13_AMI_Laso.txt'. . . done\n",
      "Now on '14-18436.txt'. . . done\n",
      "Now on '14.txt'. . . done\n",
      "Now on '140594.txt'. . . done\n",
      "Now on '140595.txt'. . . done\n",
      "Now on '140597.txt'. . . done\n",
      "Now on '140599.txt'. . . done\n",
      "Now on '1406.2661v1.txt'. . . done\n",
      "Now on '1409.1556.txt'. . . done\n",
      "Now on '1411416708528.txt'. . . done\n",
      "Now on '1412.2306v2.txt'. . . done\n",
      "Now on '142480.txt'. . . done\n",
      "Now on '142481.txt'. . . done\n",
      "Now on '142482.txt'. . . done\n",
      "Now on '142541.txt'. . . done\n",
      "Now on '142709.txt'. . . done\n",
      "Now on '148001.txt'. . . done\n",
      "Now on '1492645.txt'. . . done\n",
      "Now on '14_AMI_Cabrera.txt'. . . done\n",
      "Now on '15.txt'. . . done\n",
      "Now on '1504.08083.txt'. . . done\n",
      "Now on '1506.01497v3.txt'. . . done\n",
      "Now on '1506.02025.txt'. . . done\n",
      "Now on '150752.txt'. . . done\n",
      "Now on '1512.03385v1.txt'. . . done\n",
      "Now on '156469.txt'. . . done\n",
      "Now on '156665.txt'. . . done\n",
      "Now on '159814.txt'. . . done\n",
      "Now on '159815.txt'. . . done\n",
      "Now on '15_AMI_Morganti.txt'. . . done\n",
      "Now on '16.txt'. . . done\n",
      "Now on '1606.03507.txt'. . . done\n",
      "Now on '1699_ftp.pdf;jsessionid=B4F0526473A4AAE435BDEACE7DE27074.f04t03.txt'. . . done\n",
      "Now on '16_58_10_733_Lead_Magazine_feature.txt'. . . done\n",
      "Now on '17.txt'. . . done\n",
      "Now on '1708.02924.txt'. . . done\n",
      "Now on '173-pap0297-bateman.txt'. . . done\n",
      "Now on '175922.txt'. . . done\n",
      "Now on '18.txt'. . . done\n",
      "Now on '180953.txt'. . . done\n",
      "Now on '19.txt'. . . done\n",
      "Now on '1900008.1900088.txt'. . . done\n",
      "Now on '1993_Krishnamoorthy_etal_PAMI93-syntactic.txt'. . . done\n",
      "Now on '1f.txt'. . . done\n",
      "Now on '1st Stop Checklist- Business_Secretarial_Consulting Service.txt'. . . done\n",
      "Now on '1st Stop Starting Your Business in Ohio Nonemployer.txt'. . . done\n",
      "Now on '2.txt'. . . done\n",
      "Now on '20.txt'. . . done\n",
      "Now on '2002-breuel-spie.txt'. . . done\n",
      "Now on '20030125.txt'. . . done\n",
      "Now on '2003_CSE_approach_to_decision_support_systems - Smith and Geddes.txt'. . . done\n",
      "Now on '2006ICAD-WalkerNanceLindsay.txt'. . . done\n",
      "Now on '2008 Neuroprosthetics.txt'. . . done\n",
      "Now on '2009BookChapter.txt'. . . done\n",
      "Now on '2009HFES-JeonWalker-cameraready.txt'. . . done\n",
      "Now on '2009_trouva.txt'. . . done\n",
      "Now on '2010-03-SDS TechnicalBriefing.txt'. . . done\n",
      "Now on '20100100869.txt'. . . done\n",
      "Now on '20131125093820659.txt'. . . done\n",
      "Now on '2014 1099 Report #1 - Keefer.txt'. . . done\n",
      "Now on '2014 1099 Report #2 - Keefer.txt'. . . done\n",
      "Now on '20140313_031900_0000d31ccabc.txt'. . . done\n",
      "Now on '2015.10.12_Millenson_Berenson.txt'. . . done\n",
      "Now on '2016HealthcareBenchmarks_Care_Coordination_preview.txt'. . . done\n",
      "Now on '21.txt'. . . done\n",
      "Now on '213.txt'. . . done\n",
      "Now on '21851407.txt'. . . done\n",
      "Now on '22.txt'. . . done\n",
      "Now on '23.txt'. . . done\n",
      "Now on '2393216.2393281.txt'. . . done\n",
      "Now on '24.txt'. . . done\n",
      "Now on '25.txt'. . . done\n",
      "Now on '2553062.2553065.txt'. . . done\n",
      "Now on '2666795.2666817.txt'. . . done\n",
      "Now on '280-p276-winberg.txt'. . . done\n",
      "Now on '2891868.txt'. . . done\n",
      "Now on '3.txt'. . . done\n",
      "Now on '30 Years of Minix.txt'. . . done\n",
      "Now on '30.txt'. . . done\n",
      "Now on '3178_18_1.txt'. . . done\n",
      "Now on '33163615.txt'. . . done\n",
      "Now on '33451B Medical Device Manufacturing in the US Industry Report.txt'. . . done\n",
      "Now on '34-the-power-of-full-engagement.txt'. . . done\n",
      "Now on '343362a0.txt'. . . done\n",
      "Now on '37_ExampleBased_JMSP_SpecialIsuue.txt'. . . done\n",
      "Now on '3_Embedded_Care_Coordination_Models_To_Manage_Diverse.txt'. . . done\n",
      "Now on '3_Toms.txt'. . . done\n",
      "Now on '4.txt'. . . done\n",
      "Now on '419.full.txt'. . . done\n",
      "Now on '420-1094-2-PB.txt'. . . done\n",
      "Now on '42345 Medical Supplies Wholesaling in the US Industry Report.txt'. . . done\n",
      "Now on '427-1101-2-PB.txt'. . . done\n",
      "Now on '4824-imagenet-classification-with-deep-convolutional-neural-networks.txt'. . . done\n",
      "Now on '5 Transitions Great Leaders Make That Average Leaders Don't - Forbes.txt'. . . done\n",
      "Now on '5.txt'. . . done\n",
      "Now on '5001766_Apparatus_and_method_for_skew_co.txt'. . . done\n",
      "Now on '50fb36cd-2708-4308-b75e-2d6e970efc3f.txt'. . . done\n",
      "Now on '5100326.txt'. . . done\n",
      "Now on '51121C Business Analytics & Enterprise Software Publishing in the US Industry Report.txt'. . . done\n",
      "Now on '52.txt'. . . done\n",
      "Now on '52411B Health & Medical Insurance in the US Industry Report.txt'. . . done\n",
      "Now on '54151 IT Consulting in the US Industry Report.txt'. . . done\n",
      "Now on '56160540.txt'. . . done\n",
      "Now on '5741258.txt'. . . done\n",
      "Now on '5845-15287-1-PB.txt'. . . done\n",
      "Now on '5CsofAgileManagement.txt'. . . done\n",
      "Now on '6.txt'. . . done\n",
      "Now on '6144888_Modular_system_and_architecture.txt'. . . done\n",
      "Now on '62151 Diagnostic & Medical Laboratories in the US Industry Report.txt'. . . done\n",
      "Now on '62211 Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '62231 Specialty Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '6368110.txt'. . . done\n",
      "Now on '6505087_Modular_system_and_architecture.txt'. . . done\n",
      "Now on '6685188.txt'. . . done\n",
      "Now on '6948938.txt'. . . done\n",
      "Now on '7 Rules for Job Interview Questions That Result in Great Hires.txt'. . . done\n",
      "Now on '7 Ways To Improve Patient Satisfaction, Experience, And Customer Service,.txt'. . . done\n",
      "Now on '7-1-13_SR_GivingUSA.txt'. . . done\n",
      "Now on '7-Things-Health-Insurance-Customers-Not-Telling-You.txt'. . . done\n",
      "Now on '7.txt'. . . done\n",
      "Now on '7007809.txt'. . . done\n",
      "Now on '7409422.txt'. . . done\n",
      "Now on '769.txt'. . . done\n",
      "Now on '7890926.txt'. . . done\n",
      "Now on '8.txt'. . . done\n",
      "Now on '8229871.txt'. . . done\n",
      "Now on '831-828-1-PB.txt'. . . done\n",
      "Now on '8815fe56-9e95-414b-b5e3-1f8363e5471d.txt'. . . done\n",
      "Now on '9.txt'. . . done\n",
      "Now on 'A  cognitive affective model of organizational communication for designing IT.txt'. . . done\n",
      "Now on 'A Bible for the Disability Field.txt'. . . done\n",
      "Now on 'A Comparisoon of Binarization Methods for Historical Archive Documents.txt'. . . done\n",
      "Now on 'A light-weight text image processing method for handheld embedded cameras.txt'. . . done\n",
      "Now on 'A Mathematical Theory of Communication.txt'. . . done\n",
      "Now on 'A Model for Types and Levels of Human Interaction with Automation.txt'. . . done\n",
      "Now on 'A Parser for Real-Time Speech Synthesis of Conversational Texts.txt'. . . done\n",
      "Now on 'A Physically Based Approach to 2-D Shape Blending.txt'. . . done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on 'A Review of 326 Children with Developmental and Physical Disabilities, Consecutively Taught at the Movement Development Clinic.txt'. . . done\n",
      "Now on 'A Review of Quasi-Linear Pilot Models.txt'. . . done\n",
      "Now on 'A Set of C++ Classes for Co-routine Style Programming.txt'. . . done\n",
      "Now on 'A Spiral Model of Software Development and Enhancement.txt'. . . done\n",
      "Now on 'A Study of Design Requirements for Mobile Learning Environments.txt'. . . done\n",
      "Now on 'A Target Tracking Algorithm of Passive Sensor Normal Truncated Model.txt'. . . done\n",
      "Now on 'a12.txt'. . . done\n",
      "Now on 'a2-jeong.txt'. . . done\n",
      "Now on 'a2.txt'. . . done\n",
      "Now on 'AACOpenNewWorld.txt'. . . done\n",
      "Now on 'aar.txt'. . . done\n",
      "Now on 'Lafferty_pcfg-notes.txt'. . . done\n",
      "Now on 'LearningExecutableSemanticParsers.txt'. . . done\n",
      "Now on 'p123-zhang.txt'. . . done\n",
      "Now on 'Partial Parsing Finite-State Cascades.txt'. . . done\n",
      "Now on 'QueryEffectiveness.txt'. . . done\n",
      "Now on 'Text Clustering Algorithms.txt'. . . done\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.txt'. . . done\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.txt'. . . done\n"
     ]
    }
   ],
   "source": [
    "# THIS MUST BE AFTER TEXT CONVERSION BEFORE ANY OTHER FUNCTIONS\n",
    "# Function to remove \\n\n",
    "def rmvN(txtDir = txtFilesDir, rtnDir = rtnFilesDir, absPath = os.getcwd()):\n",
    "# Checks that text file directory exists/is correct\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    rtnPath = absPath+'/'+rtnDir\n",
    "    if rtnDir not in os.listdir(absPath):\n",
    "        os.mkdir(rtnPath)\n",
    "\n",
    "# Substitutes returns and hyphens at the end of each line with empty strings\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(rtnPath+'/'+entity.name, 'w+', encoding='utf-8') as rtnFile:\n",
    "                text = txtFile.read()\n",
    "                text = re.sub('-\\n', '', text)\n",
    "                text = re.sub('\\n', '', text)\n",
    "                rtnFile.write(text)\n",
    "                rtnFile.truncate()\n",
    "        print(\"done\")\n",
    "rmvN(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on '00023.txt'. . . done\n",
      "Now on '0028_504_uchida_s.txt'. . . done\n",
      "Now on '0044_559_ishitani_y.txt'. . . done\n",
      "Now on '00708543.txt'. . . done\n",
      "Now on '00823977.txt'. . . done\n",
      "Now on '00863988.txt'. . . done\n",
      "Now on '009.txt'. . . done\n",
      "Now on '00969115.txt'. . . done\n",
      "Now on '00a.txt'. . . done\n",
      "Now on '01-Altamura_Esposito_Malerba--Transforming_paper_documents_into_XML_format_with_WISDOM__.txt'. . . done\n",
      "Now on '01046104.txt'. . . done\n",
      "Now on '01217605.txt'. . . done\n",
      "Now on '01237554.txt'. . . done\n",
      "Now on '01308672.txt'. . . done\n",
      "Now on '01673370.txt'. . . done\n",
      "Now on '01_AMI_Alcaniz.txt'. . . done\n",
      "Now on '02_AMI_Riva.txt'. . . done\n",
      "Now on '03_AMI_Gaggioli.txt'. . . done\n",
      "Now on '04100664.txt'. . . done\n",
      "Now on '04359339.txt'. . . done\n",
      "Now on '04407722.txt'. . . done\n",
      "Now on '04_AMI_istag.txt'. . . done\n",
      "Now on '05_AMI_Cortese.txt'. . . done\n",
      "Now on '06_AMI__Piva.txt'. . . done\n",
      "Now on '07_AMI_Kameas.txt'. . . done\n",
      "Now on '08_AMI_Kleiner.txt'. . . done\n",
      "Now on '09.txt'. . . done\n",
      "Now on '09_AMI_Schmidt.txt'. . . done\n",
      "Now on '0e.txt'. . . done\n",
      "Now on '1.txt'. . . done\n",
      "Now on '10.1.1.115.7110.txt'. . . done\n",
      "Now on '10.1.1.123.2158.txt'. . . done\n",
      "Now on '10.1.1.130.6691.txt'. . . done\n",
      "Now on '10.1.1.32.4689.txt'. . . done\n",
      "Now on '10.1.1.62.889.txt'. . . done\n",
      "Now on '10.1.1.65.7635.txt'. . . done\n",
      "Now on '10.1.1.72.8127.txt'. . . done\n",
      "Now on '10.1.1.91.9906.txt'. . . done\n",
      "Now on '10.txt'. . . done\n",
      "Now on '103583991.txt'. . . done\n",
      "Now on '10_AMI_Simplicity.txt'. . . done\n",
      "Now on '11.txt'. . . done\n",
      "Now on '11_AMI_Cantoni.txt'. . . done\n",
      "Now on '12.txt'. . . done\n",
      "Now on '1203.txt'. . . done\n",
      "Now on '122808.txt'. . . done\n",
      "Now on '124972.txt'. . . done\n",
      "Now on '124973.txt'. . . done\n",
      "Now on '124974.txt'. . . done\n",
      "Now on '124977.txt'. . . done\n",
      "Now on '124978.txt'. . . done\n",
      "Now on '124979.txt'. . . done\n",
      "Now on '12628324.txt'. . . done\n",
      "Now on '12_AMI_Bettiol.txt'. . . done\n",
      "Now on '12_steps_paper.txt'. . . done\n",
      "Now on '13.txt'. . . done\n",
      "Now on '1306267704-OptimalExperienceinWorkandLeisure.txt'. . . done\n",
      "Now on '130771.txt'. . . done\n",
      "Now on '1311.2524v5.txt'. . . done\n",
      "Now on '1311.2901v3.txt'. . . done\n",
      "Now on '135706.txt'. . . done\n",
      "Now on '136540.txt'. . . done\n",
      "Now on '136542.txt'. . . done\n",
      "Now on '136543.txt'. . . done\n",
      "Now on '136546.txt'. . . done\n",
      "Now on '136644.txt'. . . done\n",
      "Now on '139679.txt'. . . done\n",
      "Now on '139681.txt'. . . done\n",
      "Now on '139682.txt'. . . done\n",
      "Now on '139684.txt'. . . done\n",
      "Now on '139973.txt'. . . done\n",
      "Now on '139978.txt'. . . done\n",
      "Now on '139979.txt'. . . done\n",
      "Now on '13_AMI_Laso.txt'. . . done\n",
      "Now on '14-18436.txt'. . . done\n",
      "Now on '14.txt'. . . done\n",
      "Now on '140594.txt'. . . done\n",
      "Now on '140595.txt'. . . done\n",
      "Now on '140597.txt'. . . done\n",
      "Now on '140599.txt'. . . done\n",
      "Now on '1406.2661v1.txt'. . . done\n",
      "Now on '1409.1556.txt'. . . done\n",
      "Now on '1411416708528.txt'. . . done\n",
      "Now on '1412.2306v2.txt'. . . done\n",
      "Now on '142480.txt'. . . done\n",
      "Now on '142481.txt'. . . done\n",
      "Now on '142482.txt'. . . done\n",
      "Now on '142541.txt'. . . done\n",
      "Now on '142709.txt'. . . done\n",
      "Now on '148001.txt'. . . done\n",
      "Now on '1492645.txt'. . . done\n",
      "Now on '14_AMI_Cabrera.txt'. . . done\n",
      "Now on '15.txt'. . . done\n",
      "Now on '1504.08083.txt'. . . done\n",
      "Now on '1506.01497v3.txt'. . . done\n",
      "Now on '1506.02025.txt'. . . done\n",
      "Now on '150752.txt'. . . done\n",
      "Now on '1512.03385v1.txt'. . . done\n",
      "Now on '156469.txt'. . . done\n",
      "Now on '156665.txt'. . . done\n",
      "Now on '159814.txt'. . . done\n",
      "Now on '159815.txt'. . . done\n",
      "Now on '15_AMI_Morganti.txt'. . . done\n",
      "Now on '16.txt'. . . done\n",
      "Now on '1606.03507.txt'. . . done\n",
      "Now on '1699_ftp.pdf;jsessionid=B4F0526473A4AAE435BDEACE7DE27074.f04t03.txt'. . . done\n",
      "Now on '16_58_10_733_Lead_Magazine_feature.txt'. . . done\n",
      "Now on '17.txt'. . . done\n",
      "Now on '1708.02924.txt'. . . done\n",
      "Now on '173-pap0297-bateman.txt'. . . done\n",
      "Now on '175922.txt'. . . done\n",
      "Now on '18.txt'. . . done\n",
      "Now on '180953.txt'. . . done\n",
      "Now on '19.txt'. . . done\n",
      "Now on '1900008.1900088.txt'. . . done\n",
      "Now on '1993_Krishnamoorthy_etal_PAMI93-syntactic.txt'. . . done\n",
      "Now on '1f.txt'. . . done\n",
      "Now on '1st Stop Checklist- Business_Secretarial_Consulting Service.txt'. . . done\n",
      "Now on '1st Stop Starting Your Business in Ohio Nonemployer.txt'. . . done\n",
      "Now on '2.txt'. . . done\n",
      "Now on '20.txt'. . . done\n",
      "Now on '2002-breuel-spie.txt'. . . done\n",
      "Now on '20030125.txt'. . . done\n",
      "Now on '2003_CSE_approach_to_decision_support_systems - Smith and Geddes.txt'. . . done\n",
      "Now on '2006ICAD-WalkerNanceLindsay.txt'. . . done\n",
      "Now on '2008 Neuroprosthetics.txt'. . . done\n",
      "Now on '2009BookChapter.txt'. . . done\n",
      "Now on '2009HFES-JeonWalker-cameraready.txt'. . . done\n",
      "Now on '2009_trouva.txt'. . . done\n",
      "Now on '2010-03-SDS TechnicalBriefing.txt'. . . done\n",
      "Now on '20100100869.txt'. . . done\n",
      "Now on '20131125093820659.txt'. . . done\n",
      "Now on '2014 1099 Report #1 - Keefer.txt'. . . done\n",
      "Now on '2014 1099 Report #2 - Keefer.txt'. . . done\n",
      "Now on '20140313_031900_0000d31ccabc.txt'. . . done\n",
      "Now on '2015.10.12_Millenson_Berenson.txt'. . . done\n",
      "Now on '2016HealthcareBenchmarks_Care_Coordination_preview.txt'. . . done\n",
      "Now on '21.txt'. . . done\n",
      "Now on '213.txt'. . . done\n",
      "Now on '21851407.txt'. . . done\n",
      "Now on '22.txt'. . . done\n",
      "Now on '23.txt'. . . done\n",
      "Now on '2393216.2393281.txt'. . . done\n",
      "Now on '24.txt'. . . done\n",
      "Now on '25.txt'. . . done\n",
      "Now on '2553062.2553065.txt'. . . done\n",
      "Now on '2666795.2666817.txt'. . . done\n",
      "Now on '280-p276-winberg.txt'. . . done\n",
      "Now on '2891868.txt'. . . done\n",
      "Now on '3.txt'. . . done\n",
      "Now on '30 Years of Minix.txt'. . . done\n",
      "Now on '30.txt'. . . done\n",
      "Now on '3178_18_1.txt'. . . done\n",
      "Now on '33163615.txt'. . . done\n",
      "Now on '33451B Medical Device Manufacturing in the US Industry Report.txt'. . . done\n",
      "Now on '34-the-power-of-full-engagement.txt'. . . done\n",
      "Now on '343362a0.txt'. . . done\n",
      "Now on '37_ExampleBased_JMSP_SpecialIsuue.txt'. . . done\n",
      "Now on '3_Embedded_Care_Coordination_Models_To_Manage_Diverse.txt'. . . done\n",
      "Now on '3_Toms.txt'. . . done\n",
      "Now on '4.txt'. . . done\n",
      "Now on '419.full.txt'. . . done\n",
      "Now on '420-1094-2-PB.txt'. . . done\n",
      "Now on '42345 Medical Supplies Wholesaling in the US Industry Report.txt'. . . done\n",
      "Now on '427-1101-2-PB.txt'. . . done\n",
      "Now on '4824-imagenet-classification-with-deep-convolutional-neural-networks.txt'. . . done\n",
      "Now on '5 Transitions Great Leaders Make That Average Leaders Don't - Forbes.txt'. . . done\n",
      "Now on '5.txt'. . . done\n",
      "Now on '5001766_Apparatus_and_method_for_skew_co.txt'. . . done\n",
      "Now on '50fb36cd-2708-4308-b75e-2d6e970efc3f.txt'. . . done\n",
      "Now on '5100326.txt'. . . done\n",
      "Now on '51121C Business Analytics & Enterprise Software Publishing in the US Industry Report.txt'. . . done\n",
      "Now on '52.txt'. . . done\n",
      "Now on '52411B Health & Medical Insurance in the US Industry Report.txt'. . . done\n",
      "Now on '54151 IT Consulting in the US Industry Report.txt'. . . done\n",
      "Now on '56160540.txt'. . . done\n",
      "Now on '5741258.txt'. . . done\n",
      "Now on '5845-15287-1-PB.txt'. . . done\n",
      "Now on '5CsofAgileManagement.txt'. . . done\n",
      "Now on '6.txt'. . . done\n",
      "Now on '6144888_Modular_system_and_architecture.txt'. . . done\n",
      "Now on '62151 Diagnostic & Medical Laboratories in the US Industry Report.txt'. . . done\n",
      "Now on '62211 Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '62231 Specialty Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '6368110.txt'. . . done\n",
      "Now on '6505087_Modular_system_and_architecture.txt'. . . done\n",
      "Now on '6685188.txt'. . . done\n",
      "Now on '6948938.txt'. . . done\n",
      "Now on '7 Rules for Job Interview Questions That Result in Great Hires.txt'. . . done\n",
      "Now on '7 Ways To Improve Patient Satisfaction, Experience, And Customer Service,.txt'. . . done\n",
      "Now on '7-1-13_SR_GivingUSA.txt'. . . done\n",
      "Now on '7-Things-Health-Insurance-Customers-Not-Telling-You.txt'. . . done\n",
      "Now on '7.txt'. . . done\n",
      "Now on '7007809.txt'. . . done\n",
      "Now on '7409422.txt'. . . done\n",
      "Now on '769.txt'. . . done\n",
      "Now on '7890926.txt'. . . done\n",
      "Now on '8.txt'. . . done\n",
      "Now on '8229871.txt'. . . done\n",
      "Now on '831-828-1-PB.txt'. . . done\n",
      "Now on '8815fe56-9e95-414b-b5e3-1f8363e5471d.txt'. . . done\n",
      "Now on '9.txt'. . . done\n",
      "Now on 'A  cognitive affective model of organizational communication for designing IT.txt'. . . done\n",
      "Now on 'A Bible for the Disability Field.txt'. . . done\n",
      "Now on 'A Comparisoon of Binarization Methods for Historical Archive Documents.txt'. . . done\n",
      "Now on 'A light-weight text image processing method for handheld embedded cameras.txt'. . . done\n",
      "Now on 'A Mathematical Theory of Communication.txt'. . . done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on 'A Model for Types and Levels of Human Interaction with Automation.txt'. . . done\n",
      "Now on 'A Parser for Real-Time Speech Synthesis of Conversational Texts.txt'. . . done\n",
      "Now on 'A Physically Based Approach to 2-D Shape Blending.txt'. . . done\n",
      "Now on 'A Review of 326 Children with Developmental and Physical Disabilities, Consecutively Taught at the Movement Development Clinic.txt'. . . done\n",
      "Now on 'A Review of Quasi-Linear Pilot Models.txt'. . . done\n",
      "Now on 'A Set of C++ Classes for Co-routine Style Programming.txt'. . . done\n",
      "Now on 'A Spiral Model of Software Development and Enhancement.txt'. . . done\n",
      "Now on 'A Study of Design Requirements for Mobile Learning Environments.txt'. . . done\n",
      "Now on 'A Target Tracking Algorithm of Passive Sensor Normal Truncated Model.txt'. . . done\n",
      "Now on 'a12.txt'. . . done\n",
      "Now on 'a2-jeong.txt'. . . done\n",
      "Now on 'a2.txt'. . . done\n",
      "Now on 'AACOpenNewWorld.txt'. . . done\n",
      "Now on 'aar.txt'. . . done\n",
      "Now on 'Lafferty_pcfg-notes.txt'. . . done\n",
      "Now on 'LearningExecutableSemanticParsers.txt'. . . done\n",
      "Now on 'p123-zhang.txt'. . . done\n",
      "Now on 'Partial Parsing Finite-State Cascades.txt'. . . done\n",
      "Now on 'QueryEffectiveness.txt'. . . done\n",
      "Now on 'Text Clustering Algorithms.txt'. . . done\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.txt'. . . done\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.txt'. . . done\n"
     ]
    }
   ],
   "source": [
    "# Funtion to move files without spaces to new 'Without Spaces' directory         \n",
    "def checkSpaces(txtDir = rtnFilesDir, spacesDir = spaceFilesDir, absPath = os.getcwd()):\n",
    "# Checks that text file directory exists/is correct\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    spacesPath = absPath+'/'+spacesDir\n",
    "    if spacesDir not in os.listdir(absPath):\n",
    "        os.mkdir(spacesPath)\n",
    "        \n",
    "    with open(absPath+'/'+'Spaces.txt', 'a+', encoding='utf-8') as spaces: \n",
    "        spaces.write(\"Check Spaces\\n\" + date.today().strftime(\"%m/%d/%y\") +\n",
    "                  \" at \" + datetime.now().strftime(\"%H:%M:%S\") + \"\\n\\n\")\n",
    "        for entity in os.scandir(txtPath):\n",
    "            print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "            txtFile = open(txtPath+'/'+entity.name, 'r', encoding='utf-8')\n",
    "            text = txtFile.read()\n",
    "            split = text.split(' ')\n",
    "            if len(split) < len(text)/10 or len(text) < 100 or text == '':\n",
    "                txtFile.close()\n",
    "                spaces.write(entity.name+'\\n')\n",
    "                if entity.name not in os.listdir(spacesPath):\n",
    "                    os.rename(txtPath+'/'+entity.name, spacesPath+'/'+entity.name)\n",
    "                else:\n",
    "                    os.remove(txtPath+'/'+entity.name)\n",
    "            print(\"done\")\n",
    "        spaces.write('\\n\\n')\n",
    "checkSpaces(absPath = absolute) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on '00023.txt'. . . done\n",
      "Now on '0028_504_uchida_s.txt'. . . done\n",
      "Now on '0044_559_ishitani_y.txt'. . . done\n",
      "Now on '00708543.txt'. . . done\n",
      "Now on '00823977.txt'. . . done\n",
      "Now on '00863988.txt'. . . done\n",
      "Now on '009.txt'. . . done\n",
      "Now on '00969115.txt'. . . done\n",
      "Now on '00a.txt'. . . done\n",
      "Now on '01-Altamura_Esposito_Malerba--Transforming_paper_documents_into_XML_format_with_WISDOM__.txt'. . . done\n",
      "Now on '01046104.txt'. . . done\n",
      "Now on '01217605.txt'. . . done\n",
      "Now on '01237554.txt'. . . done\n",
      "Now on '01308672.txt'. . . done\n",
      "Now on '01673370.txt'. . . done\n",
      "Now on '01_AMI_Alcaniz.txt'. . . done\n",
      "Now on '02_AMI_Riva.txt'. . . done\n",
      "Now on '03_AMI_Gaggioli.txt'. . . done\n",
      "Now on '04100664.txt'. . . done\n",
      "Now on '04359339.txt'. . . done\n",
      "Now on '04407722.txt'. . . done\n",
      "Now on '04_AMI_istag.txt'. . . done\n",
      "Now on '05_AMI_Cortese.txt'. . . done\n",
      "Now on '06_AMI__Piva.txt'. . . done\n",
      "Now on '07_AMI_Kameas.txt'. . . done\n",
      "Now on '08_AMI_Kleiner.txt'. . . done\n",
      "Now on '09.txt'. . . done\n",
      "Now on '09_AMI_Schmidt.txt'. . . done\n",
      "Now on '0e.txt'. . . done\n",
      "Now on '1.txt'. . . done\n",
      "Now on '10.1.1.115.7110.txt'. . . done\n",
      "Now on '10.1.1.123.2158.txt'. . . done\n",
      "Now on '10.1.1.130.6691.txt'. . . done\n",
      "Now on '10.1.1.32.4689.txt'. . . done\n",
      "Now on '10.1.1.62.889.txt'. . . done\n",
      "Now on '10.1.1.65.7635.txt'. . . done\n",
      "Now on '10.1.1.72.8127.txt'. . . done\n",
      "Now on '10.1.1.91.9906.txt'. . . done\n",
      "Now on '10.txt'. . . done\n",
      "Now on '10_AMI_Simplicity.txt'. . . done\n",
      "Now on '11.txt'. . . done\n",
      "Now on '11_AMI_Cantoni.txt'. . . done\n",
      "Now on '12.txt'. . . done\n",
      "Now on '1203.txt'. . . done\n",
      "Now on '122808.txt'. . . done\n",
      "Now on '12628324.txt'. . . done\n",
      "Now on '12_AMI_Bettiol.txt'. . . done\n",
      "Now on '12_steps_paper.txt'. . . done\n",
      "Now on '1306267704-OptimalExperienceinWorkandLeisure.txt'. . . done\n",
      "Now on '1311.2524v5.txt'. . . done\n",
      "Now on '1311.2901v3.txt'. . . done\n",
      "Now on '13_AMI_Laso.txt'. . . done\n",
      "Now on '14.txt'. . . done\n",
      "Now on '1406.2661v1.txt'. . . done\n",
      "Now on '1409.1556.txt'. . . done\n",
      "Now on '1411416708528.txt'. . . done\n",
      "Now on '1412.2306v2.txt'. . . done\n",
      "Now on '142480.txt'. . . done\n",
      "Now on '142481.txt'. . . done\n",
      "Now on '142482.txt'. . . done\n",
      "Now on '142541.txt'. . . done\n",
      "Now on '148001.txt'. . . done\n",
      "Now on '1492645.txt'. . . done\n",
      "Now on '14_AMI_Cabrera.txt'. . . done\n",
      "Now on '15.txt'. . . done\n",
      "Now on '1504.08083.txt'. . . done\n",
      "Now on '1506.01497v3.txt'. . . done\n",
      "Now on '1506.02025.txt'. . . done\n",
      "Now on '1512.03385v1.txt'. . . done\n",
      "Now on '156469.txt'. . . done\n",
      "Now on '15_AMI_Morganti.txt'. . . done\n",
      "Now on '16.txt'. . . done\n",
      "Now on '1606.03507.txt'. . . done\n",
      "Now on '1699_ftp.pdf;jsessionid=B4F0526473A4AAE435BDEACE7DE27074.f04t03.txt'. . . done\n",
      "Now on '16_58_10_733_Lead_Magazine_feature.txt'. . . done\n",
      "Now on '17.txt'. . . done\n",
      "Now on '1708.02924.txt'. . . done\n",
      "Now on '173-pap0297-bateman.txt'. . . done\n",
      "Now on '18.txt'. . . done\n",
      "Now on '19.txt'. . . done\n",
      "Now on '1900008.1900088.txt'. . . done\n",
      "Now on '1st Stop Checklist- Business_Secretarial_Consulting Service.txt'. . . done\n",
      "Now on '20.txt'. . . done\n",
      "Now on '2002-breuel-spie.txt'. . . done\n",
      "Now on '20030125.txt'. . . done\n",
      "Now on '2003_CSE_approach_to_decision_support_systems - Smith and Geddes.txt'. . . done\n",
      "Now on '2006ICAD-WalkerNanceLindsay.txt'. . . done\n",
      "Now on '2008 Neuroprosthetics.txt'. . . done\n",
      "Now on '2009BookChapter.txt'. . . done\n",
      "Now on '2009HFES-JeonWalker-cameraready.txt'. . . done\n",
      "Now on '2009_trouva.txt'. . . done\n",
      "Now on '2014 1099 Report #1 - Keefer.txt'. . . done\n",
      "Now on '2014 1099 Report #2 - Keefer.txt'. . . done\n",
      "Now on '2015.10.12_Millenson_Berenson.txt'. . . done\n",
      "Now on '2016HealthcareBenchmarks_Care_Coordination_preview.txt'. . . done\n",
      "Now on '21.txt'. . . done\n",
      "Now on '213.txt'. . . done\n",
      "Now on '21851407.txt'. . . done\n",
      "Now on '22.txt'. . . done\n",
      "Now on '23.txt'. . . done\n",
      "Now on '2393216.2393281.txt'. . . done\n",
      "Now on '24.txt'. . . done\n",
      "Now on '25.txt'. . . done\n",
      "Now on '2553062.2553065.txt'. . . done\n",
      "Now on '2666795.2666817.txt'. . . done\n",
      "Now on '280-p276-winberg.txt'. . . done\n",
      "Now on '2891868.txt'. . . done\n",
      "Now on '3.txt'. . . done\n",
      "Now on '30 Years of Minix.txt'. . . done\n",
      "Now on '3178_18_1.txt'. . . done\n",
      "Now on '33163615.txt'. . . done\n",
      "Now on '33451B Medical Device Manufacturing in the US Industry Report.txt'. . . done\n",
      "Now on '34-the-power-of-full-engagement.txt'. . . done\n",
      "Now on '37_ExampleBased_JMSP_SpecialIsuue.txt'. . . done\n",
      "Now on '3_Embedded_Care_Coordination_Models_To_Manage_Diverse.txt'. . . done\n",
      "Now on '3_Toms.txt'. . . done\n",
      "Now on '4.txt'. . . done\n",
      "Now on '419.full.txt'. . . done\n",
      "Now on '420-1094-2-PB.txt'. . . done\n",
      "Now on '42345 Medical Supplies Wholesaling in the US Industry Report.txt'. . . done\n",
      "Now on '427-1101-2-PB.txt'. . . done\n",
      "Now on '4824-imagenet-classification-with-deep-convolutional-neural-networks.txt'. . . done\n",
      "Now on '5.txt'. . . done\n",
      "Now on '50fb36cd-2708-4308-b75e-2d6e970efc3f.txt'. . . done\n",
      "Now on '51121C Business Analytics & Enterprise Software Publishing in the US Industry Report.txt'. . . done\n",
      "Now on '52411B Health & Medical Insurance in the US Industry Report.txt'. . . done\n",
      "Now on '54151 IT Consulting in the US Industry Report.txt'. . . done\n",
      "Now on '56160540.txt'. . . done\n",
      "Now on '5845-15287-1-PB.txt'. . . done\n",
      "Now on '5CsofAgileManagement.txt'. . . done\n",
      "Now on '6.txt'. . . done\n",
      "Now on '62151 Diagnostic & Medical Laboratories in the US Industry Report.txt'. . . done\n",
      "Now on '62211 Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '62231 Specialty Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '7 Rules for Job Interview Questions That Result in Great Hires.txt'. . . done\n",
      "Now on '7 Ways To Improve Patient Satisfaction, Experience, And Customer Service,.txt'. . . done\n",
      "Now on '7-1-13_SR_GivingUSA.txt'. . . done\n",
      "Now on '7-Things-Health-Insurance-Customers-Not-Telling-You.txt'. . . done\n",
      "Now on '7.txt'. . . done\n",
      "Now on '7007809.txt'. . . done\n",
      "Now on '8.txt'. . . done\n",
      "Now on '831-828-1-PB.txt'. . . done\n",
      "Now on '8815fe56-9e95-414b-b5e3-1f8363e5471d.txt'. . . done\n",
      "Now on '9.txt'. . . done\n",
      "Now on 'A  cognitive affective model of organizational communication for designing IT.txt'. . . done\n",
      "Now on 'A Bible for the Disability Field.txt'. . . done\n",
      "Now on 'A Comparisoon of Binarization Methods for Historical Archive Documents.txt'. . . done\n",
      "Now on 'A light-weight text image processing method for handheld embedded cameras.txt'. . . done\n",
      "Now on 'A Mathematical Theory of Communication.txt'. . . done\n",
      "Now on 'A Model for Types and Levels of Human Interaction with Automation.txt'. . . done\n",
      "Now on 'A Parser for Real-Time Speech Synthesis of Conversational Texts.txt'. . . done\n",
      "Now on 'A Physically Based Approach to 2-D Shape Blending.txt'. . . done\n",
      "Now on 'A Review of 326 Children with Developmental and Physical Disabilities, Consecutively Taught at the Movement Development Clinic.txt'. . . done\n",
      "Now on 'A Review of Quasi-Linear Pilot Models.txt'. . . done\n",
      "Now on 'A Study of Design Requirements for Mobile Learning Environments.txt'. . . done\n",
      "Now on 'A Target Tracking Algorithm of Passive Sensor Normal Truncated Model.txt'. . . done\n",
      "Now on 'a12.txt'. . . done\n",
      "Now on 'a2-jeong.txt'. . . done\n",
      "Now on 'a2.txt'. . . done\n",
      "Now on 'AACOpenNewWorld.txt'. . . done\n",
      "Now on 'aar.txt'. . . done\n",
      "Now on 'Lafferty_pcfg-notes.txt'. . . done\n",
      "Now on 'LearningExecutableSemanticParsers.txt'. . . done\n",
      "Now on 'p123-zhang.txt'. . . done\n",
      "Now on 'Partial Parsing Finite-State Cascades.txt'. . . done\n",
      "Now on 'QueryEffectiveness.txt'. . . done\n",
      "Now on 'Text Clustering Algorithms.txt'. . . done\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.txt'. . . done\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.txt'. . . done\n"
     ]
    }
   ],
   "source": [
    "# Function to remove stopwords\n",
    "# NLTK or SpaCy\n",
    "# Inverted File: gram:[doc1, doc3] or gram:[[doc1,freq], [doc3,freq]]\n",
    "def rmvStopWords(txtDir = rtnFilesDir, swDir = swFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    swPath = absPath+'/'+swDir\n",
    "    if swDir not in os.listdir(absPath):\n",
    "        os.mkdir(swPath)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(swPath+'/'+entity.name, 'w+', encoding='utf-8') as swFile:\n",
    "                doc = nlp(txtFile.read())\n",
    "                noStopWords = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.text.isnumeric()]\n",
    "                swFile.write(\" \".join(noStopWords))\n",
    "                swFile.truncate()\n",
    "        print(\"done\")\n",
    "\n",
    "rmvStopWords(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on '00023.txt'. . . done\n",
      "Now on '0028_504_uchida_s.txt'. . . done\n",
      "Now on '0044_559_ishitani_y.txt'. . . done\n",
      "Now on '00708543.txt'. . . done\n",
      "Now on '00823977.txt'. . . done\n",
      "Now on '00863988.txt'. . . done\n",
      "Now on '009.txt'. . . done\n",
      "Now on '00969115.txt'. . . done\n",
      "Now on '00a.txt'. . . done\n",
      "Now on '01-Altamura_Esposito_Malerba--Transforming_paper_documents_into_XML_format_with_WISDOM__.txt'. . . done\n",
      "Now on '01046104.txt'. . . done\n",
      "Now on '01217605.txt'. . . done\n",
      "Now on '01237554.txt'. . . done\n",
      "Now on '01308672.txt'. . . done\n",
      "Now on '01673370.txt'. . . done\n",
      "Now on '01_AMI_Alcaniz.txt'. . . done\n",
      "Now on '02_AMI_Riva.txt'. . . done\n",
      "Now on '03_AMI_Gaggioli.txt'. . . done\n",
      "Now on '04100664.txt'. . . done\n",
      "Now on '04359339.txt'. . . done\n",
      "Now on '04407722.txt'. . . done\n",
      "Now on '04_AMI_istag.txt'. . . done\n",
      "Now on '05_AMI_Cortese.txt'. . . done\n",
      "Now on '06_AMI__Piva.txt'. . . done\n",
      "Now on '07_AMI_Kameas.txt'. . . done\n",
      "Now on '08_AMI_Kleiner.txt'. . . done\n",
      "Now on '09.txt'. . . done\n",
      "Now on '09_AMI_Schmidt.txt'. . . done\n",
      "Now on '0e.txt'. . . done\n",
      "Now on '1.txt'. . . done\n",
      "Now on '10.1.1.115.7110.txt'. . . done\n",
      "Now on '10.1.1.123.2158.txt'. . . done\n",
      "Now on '10.1.1.130.6691.txt'. . . done\n",
      "Now on '10.1.1.32.4689.txt'. . . done\n",
      "Now on '10.1.1.62.889.txt'. . . done\n",
      "Now on '10.1.1.65.7635.txt'. . . done\n",
      "Now on '10.1.1.72.8127.txt'. . . done\n",
      "Now on '10.1.1.91.9906.txt'. . . done\n",
      "Now on '10.txt'. . . done\n",
      "Now on '10_AMI_Simplicity.txt'. . . done\n",
      "Now on '11.txt'. . . done\n",
      "Now on '11_AMI_Cantoni.txt'. . . done\n",
      "Now on '12.txt'. . . done\n",
      "Now on '1203.txt'. . . done\n",
      "Now on '122808.txt'. . . done\n",
      "Now on '12628324.txt'. . . done\n",
      "Now on '12_AMI_Bettiol.txt'. . . done\n",
      "Now on '12_steps_paper.txt'. . . done\n",
      "Now on '1306267704-OptimalExperienceinWorkandLeisure.txt'. . . done\n",
      "Now on '1311.2524v5.txt'. . . done\n",
      "Now on '1311.2901v3.txt'. . . done\n",
      "Now on '13_AMI_Laso.txt'. . . done\n",
      "Now on '14.txt'. . . done\n",
      "Now on '1406.2661v1.txt'. . . done\n",
      "Now on '1409.1556.txt'. . . done\n",
      "Now on '1411416708528.txt'. . . done\n",
      "Now on '1412.2306v2.txt'. . . done\n",
      "Now on '142480.txt'. . . done\n",
      "Now on '142481.txt'. . . done\n",
      "Now on '142482.txt'. . . done\n",
      "Now on '142541.txt'. . . done\n",
      "Now on '148001.txt'. . . done\n",
      "Now on '1492645.txt'. . . done\n",
      "Now on '14_AMI_Cabrera.txt'. . . done\n",
      "Now on '15.txt'. . . done\n",
      "Now on '1504.08083.txt'. . . done\n",
      "Now on '1506.01497v3.txt'. . . done\n",
      "Now on '1506.02025.txt'. . . done\n",
      "Now on '1512.03385v1.txt'. . . done\n",
      "Now on '156469.txt'. . . done\n",
      "Now on '15_AMI_Morganti.txt'. . . done\n",
      "Now on '16.txt'. . . done\n",
      "Now on '1606.03507.txt'. . . done\n",
      "Now on '1699_ftp.pdf;jsessionid=B4F0526473A4AAE435BDEACE7DE27074.f04t03.txt'. . . done\n",
      "Now on '16_58_10_733_Lead_Magazine_feature.txt'. . . done\n",
      "Now on '17.txt'. . . done\n",
      "Now on '1708.02924.txt'. . . done\n",
      "Now on '173-pap0297-bateman.txt'. . . done\n",
      "Now on '18.txt'. . . done\n",
      "Now on '19.txt'. . . done\n",
      "Now on '1900008.1900088.txt'. . . done\n",
      "Now on '1st Stop Checklist- Business_Secretarial_Consulting Service.txt'. . . done\n",
      "Now on '20.txt'. . . done\n",
      "Now on '2002-breuel-spie.txt'. . . done\n",
      "Now on '20030125.txt'. . . done\n",
      "Now on '2003_CSE_approach_to_decision_support_systems - Smith and Geddes.txt'. . . done\n",
      "Now on '2006ICAD-WalkerNanceLindsay.txt'. . . done\n",
      "Now on '2008 Neuroprosthetics.txt'. . . done\n",
      "Now on '2009BookChapter.txt'. . . done\n",
      "Now on '2009HFES-JeonWalker-cameraready.txt'. . . done\n",
      "Now on '2009_trouva.txt'. . . done\n",
      "Now on '2014 1099 Report #1 - Keefer.txt'. . . done\n",
      "Now on '2014 1099 Report #2 - Keefer.txt'. . . done\n",
      "Now on '2015.10.12_Millenson_Berenson.txt'. . . done\n",
      "Now on '2016HealthcareBenchmarks_Care_Coordination_preview.txt'. . . done\n",
      "Now on '21.txt'. . . done\n",
      "Now on '213.txt'. . . done\n",
      "Now on '21851407.txt'. . . done\n",
      "Now on '22.txt'. . . done\n",
      "Now on '23.txt'. . . done\n",
      "Now on '2393216.2393281.txt'. . . done\n",
      "Now on '24.txt'. . . done\n",
      "Now on '25.txt'. . . done\n",
      "Now on '2553062.2553065.txt'. . . done\n",
      "Now on '2666795.2666817.txt'. . . done\n",
      "Now on '280-p276-winberg.txt'. . . done\n",
      "Now on '2891868.txt'. . . done\n",
      "Now on '3.txt'. . . done\n",
      "Now on '30 Years of Minix.txt'. . . done\n",
      "Now on '3178_18_1.txt'. . . done\n",
      "Now on '33163615.txt'. . . done\n",
      "Now on '33451B Medical Device Manufacturing in the US Industry Report.txt'. . . done\n",
      "Now on '34-the-power-of-full-engagement.txt'. . . done\n",
      "Now on '37_ExampleBased_JMSP_SpecialIsuue.txt'. . . done\n",
      "Now on '3_Embedded_Care_Coordination_Models_To_Manage_Diverse.txt'. . . done\n",
      "Now on '3_Toms.txt'. . . done\n",
      "Now on '4.txt'. . . done\n",
      "Now on '419.full.txt'. . . done\n",
      "Now on '420-1094-2-PB.txt'. . . done\n",
      "Now on '42345 Medical Supplies Wholesaling in the US Industry Report.txt'. . . done\n",
      "Now on '427-1101-2-PB.txt'. . . done\n",
      "Now on '4824-imagenet-classification-with-deep-convolutional-neural-networks.txt'. . . done\n",
      "Now on '5.txt'. . . done\n",
      "Now on '50fb36cd-2708-4308-b75e-2d6e970efc3f.txt'. . . done\n",
      "Now on '51121C Business Analytics & Enterprise Software Publishing in the US Industry Report.txt'. . . done\n",
      "Now on '52411B Health & Medical Insurance in the US Industry Report.txt'. . . done\n",
      "Now on '54151 IT Consulting in the US Industry Report.txt'. . . done\n",
      "Now on '56160540.txt'. . . done\n",
      "Now on '5845-15287-1-PB.txt'. . . done\n",
      "Now on '5CsofAgileManagement.txt'. . . done\n",
      "Now on '6.txt'. . . done\n",
      "Now on '62151 Diagnostic & Medical Laboratories in the US Industry Report.txt'. . . done\n",
      "Now on '62211 Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '62231 Specialty Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '7 Rules for Job Interview Questions That Result in Great Hires.txt'. . . done\n",
      "Now on '7 Ways To Improve Patient Satisfaction, Experience, And Customer Service,.txt'. . . done\n",
      "Now on '7-1-13_SR_GivingUSA.txt'. . . done\n",
      "Now on '7-Things-Health-Insurance-Customers-Not-Telling-You.txt'. . . done\n",
      "Now on '7.txt'. . . done\n",
      "Now on '7007809.txt'. . . done\n",
      "Now on '8.txt'. . . done\n",
      "Now on '831-828-1-PB.txt'. . . done\n",
      "Now on '8815fe56-9e95-414b-b5e3-1f8363e5471d.txt'. . . done\n",
      "Now on '9.txt'. . . done\n",
      "Now on 'A  cognitive affective model of organizational communication for designing IT.txt'. . . done\n",
      "Now on 'A Bible for the Disability Field.txt'. . . done\n",
      "Now on 'A Comparisoon of Binarization Methods for Historical Archive Documents.txt'. . . done\n",
      "Now on 'A light-weight text image processing method for handheld embedded cameras.txt'. . . done\n",
      "Now on 'A Mathematical Theory of Communication.txt'. . . done\n",
      "Now on 'A Model for Types and Levels of Human Interaction with Automation.txt'. . . done\n",
      "Now on 'A Parser for Real-Time Speech Synthesis of Conversational Texts.txt'. . . done\n",
      "Now on 'A Physically Based Approach to 2-D Shape Blending.txt'. . . done\n",
      "Now on 'A Review of 326 Children with Developmental and Physical Disabilities, Consecutively Taught at the Movement Development Clinic.txt'. . . done\n",
      "Now on 'A Review of Quasi-Linear Pilot Models.txt'. . . done\n",
      "Now on 'A Study of Design Requirements for Mobile Learning Environments.txt'. . . done\n",
      "Now on 'A Target Tracking Algorithm of Passive Sensor Normal Truncated Model.txt'. . . done\n",
      "Now on 'a12.txt'. . . done\n",
      "Now on 'a2-jeong.txt'. . . done\n",
      "Now on 'a2.txt'. . . done\n",
      "Now on 'AACOpenNewWorld.txt'. . . done\n",
      "Now on 'aar.txt'. . . done\n",
      "Now on 'Lafferty_pcfg-notes.txt'. . . done\n",
      "Now on 'LearningExecutableSemanticParsers.txt'. . . done\n",
      "Now on 'p123-zhang.txt'. . . done\n",
      "Now on 'Partial Parsing Finite-State Cascades.txt'. . . done\n",
      "Now on 'QueryEffectiveness.txt'. . . done\n",
      "Now on 'Text Clustering Algorithms.txt'. . . done\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.txt'. . . done\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.txt'. . . done\n"
     ]
    }
   ],
   "source": [
    "# Remove non-english words\n",
    "def rmvNonEng(txtDir = swFilesDir, engDir = engFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    engPath = absPath+'/'+engDir\n",
    "    if engDir not in os.listdir(absPath):\n",
    "        os.mkdir(engPath)\n",
    "    with open(absPath+'/'+'words_dictionary.json') as json_file:\n",
    "        words = json.load(json_file)\n",
    "        \n",
    "    lets = []\n",
    "    alph = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for let in alph:\n",
    "        lets.append(let)\n",
    "        for char in alph:\n",
    "            lets.append(let+char)\n",
    "        \n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(engPath+'/'+entity.name, 'w+', encoding='utf-8') as engFile:\n",
    "                text = txtFile.read().split(' ')\n",
    "                engChars = [word for word in text if word in words and word not in lets]\n",
    "                engFile.write(\" \".join(engChars))\n",
    "                engFile.truncate()\n",
    "        print(\"done\")\n",
    "rmvNonEng(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on '00023.txt'. . . done\n",
      "Now on '0028_504_uchida_s.txt'. . . done\n",
      "Now on '0044_559_ishitani_y.txt'. . . done\n",
      "Now on '00708543.txt'. . . done\n",
      "Now on '00823977.txt'. . . done\n",
      "Now on '00863988.txt'. . . done\n",
      "Now on '009.txt'. . . done\n",
      "Now on '00969115.txt'. . . done\n",
      "Now on '00a.txt'. . . done\n",
      "Now on '01-Altamura_Esposito_Malerba--Transforming_paper_documents_into_XML_format_with_WISDOM__.txt'. . . done\n",
      "Now on '01046104.txt'. . . done\n",
      "Now on '01217605.txt'. . . done\n",
      "Now on '01237554.txt'. . . done\n",
      "Now on '01308672.txt'. . . done\n",
      "Now on '01673370.txt'. . . done\n",
      "Now on '01_AMI_Alcaniz.txt'. . . done\n",
      "Now on '02_AMI_Riva.txt'. . . done\n",
      "Now on '03_AMI_Gaggioli.txt'. . . done\n",
      "Now on '04100664.txt'. . . done\n",
      "Now on '04359339.txt'. . . done\n",
      "Now on '04407722.txt'. . . done\n",
      "Now on '04_AMI_istag.txt'. . . done\n",
      "Now on '05_AMI_Cortese.txt'. . . done\n",
      "Now on '06_AMI__Piva.txt'. . . done\n",
      "Now on '07_AMI_Kameas.txt'. . . done\n",
      "Now on '08_AMI_Kleiner.txt'. . . done\n",
      "Now on '09.txt'. . . done\n",
      "Now on '09_AMI_Schmidt.txt'. . . done\n",
      "Now on '0e.txt'. . . done\n",
      "Now on '1.txt'. . . done\n",
      "Now on '10.1.1.115.7110.txt'. . . done\n",
      "Now on '10.1.1.123.2158.txt'. . . done\n",
      "Now on '10.1.1.130.6691.txt'. . . done\n",
      "Now on '10.1.1.32.4689.txt'. . . done\n",
      "Now on '10.1.1.62.889.txt'. . . done\n",
      "Now on '10.1.1.65.7635.txt'. . . done\n",
      "Now on '10.1.1.72.8127.txt'. . . done\n",
      "Now on '10.1.1.91.9906.txt'. . . done\n",
      "Now on '10.txt'. . . done\n",
      "Now on '10_AMI_Simplicity.txt'. . . done\n",
      "Now on '11.txt'. . . done\n",
      "Now on '11_AMI_Cantoni.txt'. . . done\n",
      "Now on '12.txt'. . . done\n",
      "Now on '1203.txt'. . . done\n",
      "Now on '122808.txt'. . . done\n",
      "Now on '12628324.txt'. . . done\n",
      "Now on '12_AMI_Bettiol.txt'. . . done\n",
      "Now on '12_steps_paper.txt'. . . done\n",
      "Now on '1306267704-OptimalExperienceinWorkandLeisure.txt'. . . done\n",
      "Now on '1311.2524v5.txt'. . . done\n",
      "Now on '1311.2901v3.txt'. . . done\n",
      "Now on '13_AMI_Laso.txt'. . . done\n",
      "Now on '14.txt'. . . done\n",
      "Now on '1406.2661v1.txt'. . . done\n",
      "Now on '1409.1556.txt'. . . done\n",
      "Now on '1411416708528.txt'. . . done\n",
      "Now on '1412.2306v2.txt'. . . done\n",
      "Now on '142480.txt'. . . done\n",
      "Now on '142481.txt'. . . done\n",
      "Now on '142482.txt'. . . done\n",
      "Now on '142541.txt'. . . done\n",
      "Now on '148001.txt'. . . done\n",
      "Now on '1492645.txt'. . . done\n",
      "Now on '14_AMI_Cabrera.txt'. . . done\n",
      "Now on '15.txt'. . . done\n",
      "Now on '1504.08083.txt'. . . done\n",
      "Now on '1506.01497v3.txt'. . . done\n",
      "Now on '1506.02025.txt'. . . done\n",
      "Now on '1512.03385v1.txt'. . . done\n",
      "Now on '156469.txt'. . . done\n",
      "Now on '15_AMI_Morganti.txt'. . . done\n",
      "Now on '16.txt'. . . done\n",
      "Now on '1606.03507.txt'. . . done\n",
      "Now on '1699_ftp.pdf;jsessionid=B4F0526473A4AAE435BDEACE7DE27074.f04t03.txt'. . . done\n",
      "Now on '16_58_10_733_Lead_Magazine_feature.txt'. . . done\n",
      "Now on '17.txt'. . . done\n",
      "Now on '1708.02924.txt'. . . done\n",
      "Now on '173-pap0297-bateman.txt'. . . done\n",
      "Now on '18.txt'. . . done\n",
      "Now on '19.txt'. . . done\n",
      "Now on '1900008.1900088.txt'. . . done\n",
      "Now on '1st Stop Checklist- Business_Secretarial_Consulting Service.txt'. . . done\n",
      "Now on '20.txt'. . . done\n",
      "Now on '2002-breuel-spie.txt'. . . done\n",
      "Now on '20030125.txt'. . . done\n",
      "Now on '2003_CSE_approach_to_decision_support_systems - Smith and Geddes.txt'. . . done\n",
      "Now on '2006ICAD-WalkerNanceLindsay.txt'. . . done\n",
      "Now on '2008 Neuroprosthetics.txt'. . . done\n",
      "Now on '2009BookChapter.txt'. . . done\n",
      "Now on '2009HFES-JeonWalker-cameraready.txt'. . . done\n",
      "Now on '2009_trouva.txt'. . . done\n",
      "Now on '2014 1099 Report #1 - Keefer.txt'. . . done\n",
      "Now on '2014 1099 Report #2 - Keefer.txt'. . . done\n",
      "Now on '2015.10.12_Millenson_Berenson.txt'. . . done\n",
      "Now on '2016HealthcareBenchmarks_Care_Coordination_preview.txt'. . . done\n",
      "Now on '21.txt'. . . done\n",
      "Now on '213.txt'. . . done\n",
      "Now on '21851407.txt'. . . done\n",
      "Now on '22.txt'. . . done\n",
      "Now on '23.txt'. . . done\n",
      "Now on '2393216.2393281.txt'. . . done\n",
      "Now on '24.txt'. . . done\n",
      "Now on '25.txt'. . . done\n",
      "Now on '2553062.2553065.txt'. . . done\n",
      "Now on '2666795.2666817.txt'. . . done\n",
      "Now on '280-p276-winberg.txt'. . . done\n",
      "Now on '2891868.txt'. . . done\n",
      "Now on '3.txt'. . . done\n",
      "Now on '30 Years of Minix.txt'. . . done\n",
      "Now on '3178_18_1.txt'. . . done\n",
      "Now on '33163615.txt'. . . done\n",
      "Now on '33451B Medical Device Manufacturing in the US Industry Report.txt'. . . done\n",
      "Now on '34-the-power-of-full-engagement.txt'. . . done\n",
      "Now on '37_ExampleBased_JMSP_SpecialIsuue.txt'. . . done\n",
      "Now on '3_Embedded_Care_Coordination_Models_To_Manage_Diverse.txt'. . . done\n",
      "Now on '3_Toms.txt'. . . done\n",
      "Now on '4.txt'. . . done\n",
      "Now on '419.full.txt'. . . done\n",
      "Now on '420-1094-2-PB.txt'. . . done\n",
      "Now on '42345 Medical Supplies Wholesaling in the US Industry Report.txt'. . . done\n",
      "Now on '427-1101-2-PB.txt'. . . done\n",
      "Now on '4824-imagenet-classification-with-deep-convolutional-neural-networks.txt'. . . done\n",
      "Now on '5.txt'. . . done\n",
      "Now on '50fb36cd-2708-4308-b75e-2d6e970efc3f.txt'. . . done\n",
      "Now on '51121C Business Analytics & Enterprise Software Publishing in the US Industry Report.txt'. . . done\n",
      "Now on '52411B Health & Medical Insurance in the US Industry Report.txt'. . . done\n",
      "Now on '54151 IT Consulting in the US Industry Report.txt'. . . done\n",
      "Now on '56160540.txt'. . . done\n",
      "Now on '5845-15287-1-PB.txt'. . . done\n",
      "Now on '5CsofAgileManagement.txt'. . . done\n",
      "Now on '6.txt'. . . done\n",
      "Now on '62151 Diagnostic & Medical Laboratories in the US Industry Report.txt'. . . done\n",
      "Now on '62211 Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '62231 Specialty Hospitals in the US Industry Report.txt'. . . done\n",
      "Now on '7 Rules for Job Interview Questions That Result in Great Hires.txt'. . . done\n",
      "Now on '7 Ways To Improve Patient Satisfaction, Experience, And Customer Service,.txt'. . . done\n",
      "Now on '7-1-13_SR_GivingUSA.txt'. . . done\n",
      "Now on '7-Things-Health-Insurance-Customers-Not-Telling-You.txt'. . . done\n",
      "Now on '7.txt'. . . done\n",
      "Now on '7007809.txt'. . . done\n",
      "Now on '8.txt'. . . done\n",
      "Now on '831-828-1-PB.txt'. . . done\n",
      "Now on '8815fe56-9e95-414b-b5e3-1f8363e5471d.txt'. . . done\n",
      "Now on '9.txt'. . . done\n",
      "Now on 'A  cognitive affective model of organizational communication for designing IT.txt'. . . done\n",
      "Now on 'A Bible for the Disability Field.txt'. . . done\n",
      "Now on 'A Comparisoon of Binarization Methods for Historical Archive Documents.txt'. . . done\n",
      "Now on 'A light-weight text image processing method for handheld embedded cameras.txt'. . . done\n",
      "Now on 'A Mathematical Theory of Communication.txt'. . . done\n",
      "Now on 'A Model for Types and Levels of Human Interaction with Automation.txt'. . . done\n",
      "Now on 'A Parser for Real-Time Speech Synthesis of Conversational Texts.txt'. . . done\n",
      "Now on 'A Physically Based Approach to 2-D Shape Blending.txt'. . . done\n",
      "Now on 'A Review of 326 Children with Developmental and Physical Disabilities, Consecutively Taught at the Movement Development Clinic.txt'. . . done\n",
      "Now on 'A Review of Quasi-Linear Pilot Models.txt'. . . done\n",
      "Now on 'A Study of Design Requirements for Mobile Learning Environments.txt'. . . done\n",
      "Now on 'A Target Tracking Algorithm of Passive Sensor Normal Truncated Model.txt'. . . done\n",
      "Now on 'a12.txt'. . . done\n",
      "Now on 'a2-jeong.txt'. . . done\n",
      "Now on 'a2.txt'. . . done\n",
      "Now on 'AACOpenNewWorld.txt'. . . done\n",
      "Now on 'aar.txt'. . . done\n",
      "Now on 'Lafferty_pcfg-notes.txt'. . . done\n",
      "Now on 'LearningExecutableSemanticParsers.txt'. . . done\n",
      "Now on 'p123-zhang.txt'. . . done\n",
      "Now on 'Partial Parsing Finite-State Cascades.txt'. . . done\n",
      "Now on 'QueryEffectiveness.txt'. . . done\n",
      "Now on 'Text Clustering Algorithms.txt'. . . done\n",
      "Now on 'Text-Analytics-and-Natural-Language-Processing--.txt'. . . done\n",
      "Now on 'Workshop on Robust Methods in Analysis of Natural Language Data.txt'. . . done\n"
     ]
    }
   ],
   "source": [
    "# Stem words in all documents\n",
    "def stem(txtDir = engFilesDir, stemDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    stemPath = absPath+'/'+stemDir\n",
    "    if stemDir not in os.listdir(absPath):\n",
    "        os.mkdir(stemPath)\n",
    "        \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    for entity in os.scandir(txtPath):\n",
    "        print(\"Now on '\"+entity.name+\"'. . . \", end='')\n",
    "        with open(txtPath+'/'+entity.name, 'r+', encoding='utf-8') as txtFile:\n",
    "            with open(stemPath+'/'+entity.name, 'w+', encoding='utf-8') as stemFile:\n",
    "                text = txtFile.read().split(' ')\n",
    "                stemmed = [stemmer.stem(word) for word in text]\n",
    "                stemFile.write(\" \".join(stemmed))\n",
    "                stemFile.truncate()\n",
    "        print(\"done\")\n",
    "stem(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create JSON with file_name : doc_id\n",
    "def update_doc_index(jsonDoc = jsonDocIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    jsonPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc in os.listdir(absPath):\n",
    "        with open(jsonPath, 'r') as jsonFile:\n",
    "            docIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        docIndex = {}\n",
    "    for fileName in os.listdir(txtPath):\n",
    "        if fileName not in docIndex.values():\n",
    "            docIndex[len(docIndex)+1] = fileName\n",
    "    with open(jsonPath, 'w') as jsonFile:\n",
    "        json.dump(docIndex, jsonFile, indent=4)\n",
    "\n",
    "update_doc_index(absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_inv_index(ngrams, jsonDoc = jsonDocIndex, jsonInv = jsonInvIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv in os.listdir(absPath):\n",
    "        with open(jsonInvPath, 'r') as jsonFile:\n",
    "            invIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        invIndex = {}\n",
    "#     Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "        \n",
    "    for ngram in ngrams:\n",
    "        for docID in docIndex:\n",
    "            if docID not in invIndex:\n",
    "                invIndex[docID] = {}\n",
    "            if ngram not in invIndex[docID]:\n",
    "                invIndex[docID][ngram] = {}\n",
    "                with open(txtPath+'/'+docIndex[docID], 'r', encoding='utf-8') as txtFile:\n",
    "                    text = txtFile.read().split(' ')\n",
    "                    while len(text) > ngram-1:\n",
    "                        term = \" \".join(text[:ngram])\n",
    "                        if term in invIndex[docID][ngram]:\n",
    "                            invIndex[docID][ngram][term] += 1\n",
    "                        else:\n",
    "                            invIndex[docID][ngram][term] = 1\n",
    "                        text.pop(0)\n",
    "    with open(jsonInvPath, 'w') as jsonFile:\n",
    "        json.dump(invIndex, jsonFile, indent=4)\n",
    "\n",
    "update_inv_index(list(range(2,4)), absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON with {doc_id1 : {\"gram1\":freq, \"gram2\":freq}, doc_id2 : {\"gram1\":freq}}\n",
    "def update_gram_index(minFreq = 1, jsonInv = jsonInvIndex, jsonGram = jsonGramIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "#     Makes sure all files and directories exist (jsonInv, txtDir)\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonInvPath + '\" does not exist')\n",
    "        return\n",
    "#     jsonGram is created if it does not exist\n",
    "    jsonGramPath = absPath+'/'+jsonGram\n",
    "    if jsonGram in os.listdir(absPath):\n",
    "        with open(jsonGramPath, 'r') as jsonFile:\n",
    "            gramIndex = json.load(jsonFile)\n",
    "    else:\n",
    "        gramIndex = {}\n",
    "#     Loads Inverted File index\n",
    "    with open(jsonInvPath, 'r') as jsonFile:\n",
    "        invIndex = json.load(jsonFile)\n",
    "        \n",
    "    for docID, ngrams in invIndex.items():\n",
    "        for terms in ngrams.values():\n",
    "            for term, freq in terms.items():\n",
    "                if freq >= minFreq:\n",
    "                    if term not in gramIndex:\n",
    "                        gramIndex[term] = {}\n",
    "                    gramIndex[term][docID] = freq\n",
    "#     Writes gramIndex to gram JSON file\n",
    "    with open(jsonGramPath, 'w') as jsonFile:\n",
    "        json.dump(gramIndex, jsonFile, indent=4)\n",
    "\n",
    "update_gram_index(absPath = absolute)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print grams found in 3 or more documents\n",
    "def getGrams(numDocs = 3, jsonDoc = jsonDocIndex, jsonGram = jsonGramIndex, txtDir = stemFilesDir, absPath = os.getcwd()):\n",
    "#     Makes sure all files and directories exist (textDir, jsonDoc, txtDir)\n",
    "    txtPath = absPath+'/'+txtDir\n",
    "    if txtDir not in os.listdir(absPath):\n",
    "        print('The specified directory \"' + txtPath + '\" does not exist')\n",
    "        return\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonGramPath = absPath+'/'+jsonGram\n",
    "    if jsonGram not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonGramPath + '\" does not exist')\n",
    "        return\n",
    "#    Loads document index\n",
    "    with open(jsonDocPath, 'r') as jsonFile:\n",
    "        docIndex = json.load(jsonFile)\n",
    "#    Loads gram index \n",
    "    with open(jsonGramPath, 'r') as jsonFile:\n",
    "        gramIndex = json.load(jsonFile)\n",
    "    \n",
    "    for gram, docs in gramIndex.items():\n",
    "        if len(docs) >= numDocs:\n",
    "            print(\"\\\"\"+gram+\"\\\" found in \"+str(len(docs))+\" documents\")\n",
    "            for docID in docs:\n",
    "                print(\"\\t\"+ str(docIndex[docID]))\n",
    "                \n",
    "getGrams(10, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create vectors for all n-grams (with freq>1?) between two docs, multiply them,\n",
    "#     then divide by the product Euclidian norms\n",
    "# Print out similar phrases\n",
    "def freqMatrices(ngrams, jsonDoc = jsonDocIndex, jsonInv = jsonInvIndex, jsonTFM = jsonTFMatrix, absPath = os.getcwd()):\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonInvPath = absPath+'/'+jsonInv\n",
    "    if jsonInv not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonInvPath + '\" does not exist')\n",
    "        return\n",
    "    \n",
    "    with open(jsonDocPath, 'r') as jsonDoc:\n",
    "        docIndex = json.load(jsonDoc)\n",
    "    with open(jsonInvPath, 'r') as jsonInv:\n",
    "        invIndex = json.load(jsonInv)\n",
    "        \n",
    "#     jsonTFM is created if it does not exist\n",
    "    jsonTFMPath = absPath+'/'+jsonTFM\n",
    "    if jsonTFM in os.listdir(absPath):\n",
    "        with open(jsonTFMPath, 'r') as jsonFile:\n",
    "            tfMatrix = json.load(jsonFile)\n",
    "    else:\n",
    "        tfMatrix = {}     \n",
    "        \n",
    "    for doc1 in invIndex:\n",
    "        doc1Terms = []\n",
    "        if doc1 not in tfMatrix:\n",
    "            tfMatrix[doc1] = {}\n",
    "        for ngram, terms in invIndex[doc1].items():\n",
    "            if int(ngram) in ngrams:\n",
    "                for term in terms:\n",
    "                    doc1Terms.append(term)\n",
    "        for doc2 in invIndex:\n",
    "            if doc2 >= doc1 and doc2 not in tfMatrix[doc1]:\n",
    "                print(docIndex[doc1]+\" and \"+docIndex[doc2])\n",
    "                if doc2 not in tfMatrix[doc1]:\n",
    "                    tfMatrix[doc1][doc2] = {}\n",
    "                    tfMatrix[doc1][doc2][\"all\"] = {}\n",
    "                    tfMatrix[doc1][doc2][\"like\"] = {}\n",
    "                allTerms = doc1Terms.copy()\n",
    "                likeTerms = []\n",
    "                for ngram, terms in invIndex[doc2].items():\n",
    "                    if int(ngram) in ngrams:\n",
    "                        for term in terms:\n",
    "                            if term not in allTerms:\n",
    "                                allTerms.append(term)\n",
    "                            else:\n",
    "                                likeTerms.append(term)\n",
    "                allTerms.sort()\n",
    "                likeTerms.sort()\n",
    "                doc1all = {}\n",
    "                doc2all = {}\n",
    "                doc1like = {}\n",
    "                doc2like = {}\n",
    "#                 ngramFreq = {}\n",
    "                for gramLen in ngrams:\n",
    "#                     ngramFreq[gramLen] = 0\n",
    "                    doc1all[gramLen] = []\n",
    "                    doc2all[gramLen] = []\n",
    "                    doc1like[gramLen] = []\n",
    "                    doc2like[gramLen] = []\n",
    "    \n",
    "                for term in allTerms:\n",
    "                    length = len(term.split())\n",
    "#                     ngramFreq[length] += 1\n",
    "#                     weight = ngrams[length]\n",
    "                    ngram = str(len(term.split()))\n",
    "                    doc1all[length].append(invIndex[doc1][ngram].get(term, 0))\n",
    "                    doc2all[length].append(invIndex[doc2][ngram].get(term, 0))\n",
    "                for term in likeTerms:\n",
    "                    length = len(term.split())\n",
    "#                     ngramFreq[length] += 1\n",
    "#                     weight = ngrams[length]\n",
    "                    ngram = str(len(term.split()))\n",
    "                    doc1like[length].append(invIndex[doc1][ngram].get(term))\n",
    "                    doc2like[length].append(invIndex[doc2][ngram].get(term))\n",
    "#                     doc1weighted.append(invIndex[doc1][ngram].get(term)*weight)\n",
    "#                     doc2weighted.append(invIndex[doc2][ngram].get(term)*weight)\n",
    "#                 tfMatrix[doc1][doc2][\"ngrams\"] = ngramFreq\n",
    "                tfMatrix[doc1][doc2][\"all\"][doc1] = doc1all\n",
    "                tfMatrix[doc1][doc2][\"all\"][doc2] = doc2all\n",
    "                tfMatrix[doc1][doc2][\"like\"][doc1] = doc1like\n",
    "                tfMatrix[doc1][doc2][\"like\"][doc2] = doc2like\n",
    "#                 cosSimilarity = calcCosSim(doc1freq, doc2freq)\n",
    "#                 weightedCosSim = calcCosSim(doc1weighted, doc2weighted)\n",
    "#                 print(\"Like terms (doc1, doc2):\")\n",
    "#                 for term in likeTerms:\n",
    "#                     print(\"\\t\"+term+\" (\"+str(invIndex[doc1][str(len(term.split()))][term])+\", \"+str(invIndex[doc2][str(len(term.split()))][term])+\")\")\n",
    "                numLike = len(likeTerms)\n",
    "                print(\"Doc1 has \"+str(len(doc1Terms)-numLike)+\" unique terms\")\n",
    "                print(\"Doc2 has \"+str(len(allTerms)-len(doc1Terms))+\" unique terms\")\n",
    "                for gramLength in ngrams:\n",
    "                    print(\"{}-grams: {}\".format(gramLength, len(doc1like[gramLength])))\n",
    "                print(\"Total like terms: \"+str(numLike))\n",
    "                print(\"Total unlike terms: \"+str(len(allTerms)-numLike), end=\"\\n\\n\")\n",
    "#                 print(\"Cosine Similarity: \"+ str(cosSimilarity))\n",
    "#                 print(\"Weighted Cosine Similarity: \"+ str(weightedCosSim), end='\\n\\n')\n",
    "#     Writes tfMatrix to term-freq matrix JSON file\n",
    "                with open(jsonTFMPath, 'w') as jsonFile:\n",
    "                    json.dump(tfMatrix, jsonFile, indent=4)\n",
    "grams = [2, 3]\n",
    "freqMatrices(grams, absPath = absolute)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def calcCosSim(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        print(\"Lists do not have the same number of elements\")\n",
    "        return -1\n",
    "    sumProd = sum(n1*n2 for n1, n2 in zip(list1, list2))\n",
    "    sumL1 = sum(n**2 for n in list1)\n",
    "    sumL2 = sum(n**2 for n in list2)\n",
    "    \n",
    "    return sumProd/(math.sqrt(sumL1)*math.sqrt(sumL2))\n",
    "\n",
    "def cossim(gramWeights, jsonDoc = jsonDocIndex, jsonTFM = jsonTFMatrix, absPath = os.getcwd()):\n",
    "    jsonDocPath = absPath+'/'+jsonDoc\n",
    "    if jsonDoc not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonDocPath + '\" does not exist')\n",
    "        return\n",
    "    jsonTFMPath = absPath+'/'+jsonTFM\n",
    "    if jsonTFM not in os.listdir(absPath):\n",
    "        print('FILE NOT FOUND: The specified file \"' + jsonTFMPath + '\" does not exist')\n",
    "        return\n",
    "    \n",
    "    with open(jsonDocPath, 'r') as jsonDoc:\n",
    "        docIndex = json.load(jsonDoc)\n",
    "    with open(jsonTFMPath, 'r') as jsonInv:\n",
    "        tfMatrix = json.load(jsonInv)\n",
    "    \n",
    "    for doc1 in tfMatrix:\n",
    "        for doc2, values in tfMatrix[doc1].items():\n",
    "            print(docIndex[doc1]+\"({}) and \".format(doc1)+docIndex[doc2]+\"({})\".format(doc2))\n",
    "            doc1freq = []\n",
    "            doc2freq = []\n",
    "            doc1weighted = []\n",
    "            doc2weighted = []\n",
    "            for gramLen, weight in gramWeights.items():\n",
    "                gramLen = str(gramLen)\n",
    "                doc1freq += values[\"like\"][doc1][gramLen]\n",
    "                doc2freq += values[\"like\"][doc2][gramLen]\n",
    "                doc1weighted += [ val*weight for val in values[\"like\"][doc1][gramLen] ]\n",
    "                doc2weighted += [ val*weight for val in values[\"like\"][doc2][gramLen] ]\n",
    "                \n",
    "            print(\"Cosine Similarity: \", calcCosSim(doc1freq, doc2freq))\n",
    "            print(\"Weighted: \", calcCosSim(doc1weighted, doc2weighted), end=\"\\n\\n\")\n",
    "            \n",
    "            \n",
    "gramWeight = {2:1, 3:5}\n",
    "cossim(gramWeight, absPath = absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram parser-based inverted file \n",
    "# (TF-DIF to remove trigrams common to most or all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithm based on trigram inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigram parser-based info to inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement clustering on bigram inverted file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
